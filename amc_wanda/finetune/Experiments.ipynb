{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9ef650",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model-Used\" data-toc-modified-id=\"Model-Used-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model Used</a></span></li><li><span><a href=\"#Paths\" data-toc-modified-id=\"Paths-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Paths</a></span></li><li><span><a href=\"#Utilities\" data-toc-modified-id=\"Utilities-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Utilities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Eval-PPL-on-wikitext\" data-toc-modified-id=\"Eval-PPL-on-wikitext-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Eval PPL on wikitext</a></span></li><li><span><a href=\"#Pruning-Utilities\" data-toc-modified-id=\"Pruning-Utilities-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Pruning Utilities</a></span></li></ul></li><li><span><a href=\"#Prompts\" data-toc-modified-id=\"Prompts-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Prompts</a></span></li><li><span><a href=\"#Generation-Code\" data-toc-modified-id=\"Generation-Code-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Generation Code</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-Generation\" data-toc-modified-id=\"Text-Generation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Text Generation</a></span></li><li><span><a href=\"#Throughput\" data-toc-modified-id=\"Throughput-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Throughput</a></span></li></ul></li><li><span><a href=\"#Define-Model\" data-toc-modified-id=\"Define-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Define Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Base\" data-toc-modified-id=\"Base-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Base</a></span></li><li><span><a href=\"#Chat\" data-toc-modified-id=\"Chat-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Chat</a></span></li></ul></li><li><span><a href=\"#Performance-of-Orig-Model\" data-toc-modified-id=\"Performance-of-Orig-Model-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Performance of Orig Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-performance-zero-shot-+-some-prompts\" data-toc-modified-id=\"Find-performance-zero-shot-+-some-prompts-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Find performance zero shot + some prompts</a></span><ul class=\"toc-item\"><li><span><a href=\"#PPL-on-wikitext\" data-toc-modified-id=\"PPL-on-wikitext-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>PPL on wikitext</a></span></li><li><span><a href=\"#Chat\" data-toc-modified-id=\"Chat-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Chat</a></span></li><li><span><a href=\"#Zero-Shot\" data-toc-modified-id=\"Zero-Shot-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Zero Shot</a></span></li><li><span><a href=\"#Chat\" data-toc-modified-id=\"Chat-7.1.4\"><span class=\"toc-item-num\">7.1.4&nbsp;&nbsp;</span>Chat</a></span></li></ul></li></ul></li><li><span><a href=\"#Compare-benchmarks\" data-toc-modified-id=\"Compare-benchmarks-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Compare benchmarks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prompts\" data-toc-modified-id=\"Prompts-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Prompts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prompts\" data-toc-modified-id=\"Prompts-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>Prompts</a></span></li><li><span><a href=\"#Throughput\" data-toc-modified-id=\"Throughput-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>Throughput</a></span></li><li><span><a href=\"#Memory-Size\" data-toc-modified-id=\"Memory-Size-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;</span>Memory Size</a></span></li></ul></li></ul></li><li><span><a href=\"#Given-pruning-dict,-prune-the-model\" data-toc-modified-id=\"Given-pruning-dict,-prune-the-model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Given pruning dict, prune the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prune-the-model-using-masks-and-save-checkpoints-in-save_dir\" data-toc-modified-id=\"Prune-the-model-using-masks-and-save-checkpoints-in-save_dir-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Prune the model using masks and save checkpoints in save_dir</a></span></li></ul></li><li><span><a href=\"#Pruned-Model-Performance\" data-toc-modified-id=\"Pruned-Model-Performance-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Pruned Model Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-performance-zero-shot-+-some-prompts\" data-toc-modified-id=\"Find-performance-zero-shot-+-some-prompts-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Find performance zero shot + some prompts</a></span><ul class=\"toc-item\"><li><span><a href=\"#PPL-on-wikitext\" data-toc-modified-id=\"PPL-on-wikitext-10.1.1\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>PPL on wikitext</a></span></li><li><span><a href=\"#Zero-Shot\" data-toc-modified-id=\"Zero-Shot-10.1.2\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;</span>Zero Shot</a></span></li></ul></li><li><span><a href=\"#See-how-is-generation?\" data-toc-modified-id=\"See-how-is-generation?-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>See how is generation?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prompts\" data-toc-modified-id=\"Prompts-10.2.1\"><span class=\"toc-item-num\">10.2.1&nbsp;&nbsp;</span>Prompts</a></span></li><li><span><a href=\"#Memory-Size\" data-toc-modified-id=\"Memory-Size-10.2.2\"><span class=\"toc-item-num\">10.2.2&nbsp;&nbsp;</span>Memory Size</a></span></li><li><span><a href=\"#Throughput\" data-toc-modified-id=\"Throughput-10.2.3\"><span class=\"toc-item-num\">10.2.3&nbsp;&nbsp;</span>Throughput</a></span></li></ul></li></ul></li><li><span><a href=\"#Create-real-pruned-model\" data-toc-modified-id=\"Create-real-pruned-model-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Create real pruned model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-checkpoint-saved-in-save_dir\" data-toc-modified-id=\"Remove-checkpoint-saved-in-save_dir-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Remove checkpoint saved in save_dir</a></span></li></ul></li><li><span><a href=\"#Finetune-on-Alpaca\" data-toc-modified-id=\"Finetune-on-Alpaca-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Finetune on Alpaca</a></span></li><li><span><a href=\"#Experiment-Runner\" data-toc-modified-id=\"Experiment-Runner-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Experiment Runner</a></span><ul class=\"toc-item\"><li><span><a href=\"#COmpare-to-LLMPruner\" data-toc-modified-id=\"COmpare-to-LLMPruner-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>COmpare to LLMPruner</a></span></li><li><span><a href=\"#Wanda-0.5\" data-toc-modified-id=\"Wanda-0.5-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Wanda 0.5</a></span></li><li><span><a href=\"#Experiment-#1--->-PreserveRatio=0.95,-lbound=0.8,-rbound=1.0\" data-toc-modified-id=\"Experiment-#1--->-PreserveRatio=0.95,-lbound=0.8,-rbound=1.0-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>Experiment #1 --&gt; PreserveRatio=0.95, lbound=0.8, rbound=1.0</a></span></li><li><span><a href=\"#Experiment-#2--->-PreserveRatio=0.95,-lbound=0.8,-rbound=1.0-(changes-reward-fn)\" data-toc-modified-id=\"Experiment-#2--->-PreserveRatio=0.95,-lbound=0.8,-rbound=1.0-(changes-reward-fn)-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;</span>Experiment #2 --&gt; PreserveRatio=0.95, lbound=0.8, rbound=1.0 (changes reward fn)</a></span></li><li><span><a href=\"#Experiment-#3--->-PreserveRatio=0.95,-lbound=0.8,-rbound=0.95-(changes-reward-fn)\" data-toc-modified-id=\"Experiment-#3--->-PreserveRatio=0.95,-lbound=0.8,-rbound=0.95-(changes-reward-fn)-13.5\"><span class=\"toc-item-num\">13.5&nbsp;&nbsp;</span>Experiment #3 --&gt; PreserveRatio=0.95, lbound=0.8, rbound=0.95 (changes reward fn)</a></span></li><li><span><a href=\"#Experiment-#4--->-PreserveRatio=0.95,-lbound=0.8,-rbound=0.9-(changes-reward-fn)\" data-toc-modified-id=\"Experiment-#4--->-PreserveRatio=0.95,-lbound=0.8,-rbound=0.9-(changes-reward-fn)-13.6\"><span class=\"toc-item-num\">13.6&nbsp;&nbsp;</span>Experiment #4 --&gt; PreserveRatio=0.95, lbound=0.8, rbound=0.9 (changes reward fn)</a></span></li><li><span><a href=\"#Experiment-#5--->-PreserveRatio=0.95,-lbound=0.7,-rbound=0.9-(changes-reward-fn)\" data-toc-modified-id=\"Experiment-#5--->-PreserveRatio=0.95,-lbound=0.7,-rbound=0.9-(changes-reward-fn)-13.7\"><span class=\"toc-item-num\">13.7&nbsp;&nbsp;</span>Experiment #5 --&gt; PreserveRatio=0.95, lbound=0.7, rbound=0.9 (changes reward fn)</a></span></li><li><span><a href=\"#Experiment-#6--->-PreserveRatio=0.7,-lbound=0.65,-rbound=1.0-(changes-reward-fn)\" data-toc-modified-id=\"Experiment-#6--->-PreserveRatio=0.7,-lbound=0.65,-rbound=1.0-(changes-reward-fn)-13.8\"><span class=\"toc-item-num\">13.8&nbsp;&nbsp;</span>Experiment #6 --&gt; PreserveRatio=0.7, lbound=0.65, rbound=1.0 (changes reward fn)</a></span></li></ul></li><li><span><a href=\"#Chat-Experiments\" data-toc-modified-id=\"Chat-Experiments-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Chat Experiments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Experiment-#1--->-preserve_ratio=-0.95,-lbound-=-0.8,-rbound-=-1.0\" data-toc-modified-id=\"Experiment-#1--->-preserve_ratio=-0.95,-lbound-=-0.8,-rbound-=-1.0-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>Experiment #1 --&gt; preserve_ratio= 0.95, lbound = 0.8, rbound = 1.0</a></span></li><li><span><a href=\"#Experiment-#2-->-preserve_ratio=-0.95,-lbound-=-0.7,-rbound-=-0.9\" data-toc-modified-id=\"Experiment-#2-->-preserve_ratio=-0.95,-lbound-=-0.7,-rbound-=-0.9-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>Experiment #2--&gt; preserve_ratio= 0.95, lbound = 0.7, rbound = 0.9</a></span></li><li><span><a href=\"#Experiment-#3--->--preserve_ratio=-0.7,-lbound-=-0.65,-rbound-=-1.0\" data-toc-modified-id=\"Experiment-#3--->--preserve_ratio=-0.7,-lbound-=-0.65,-rbound-=-1.0-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>Experiment #3 --&gt;  preserve_ratio= 0.7, lbound = 0.65, rbound = 1.0</a></span></li></ul></li><li><span><a href=\"#Throughput-Experiments\" data-toc-modified-id=\"Throughput-Experiments-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Throughput Experiments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Experiment-:-6\" data-toc-modified-id=\"Experiment-:-6-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Experiment : 6</a></span></li><li><span><a href=\"#Experiment-5\" data-toc-modified-id=\"Experiment-5-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span>Experiment 5</a></span></li><li><span><a href=\"#Experiment-4:\" data-toc-modified-id=\"Experiment-4:-15.3\"><span class=\"toc-item-num\">15.3&nbsp;&nbsp;</span>Experiment 4:</a></span></li><li><span><a href=\"#Experiment-3\" data-toc-modified-id=\"Experiment-3-15.4\"><span class=\"toc-item-num\">15.4&nbsp;&nbsp;</span>Experiment 3</a></span></li><li><span><a href=\"#Experiment-2\" data-toc-modified-id=\"Experiment-2-15.5\"><span class=\"toc-item-num\">15.5&nbsp;&nbsp;</span>Experiment 2</a></span></li><li><span><a href=\"#Experiment-1\" data-toc-modified-id=\"Experiment-1-15.6\"><span class=\"toc-item-num\">15.6&nbsp;&nbsp;</span>Experiment 1</a></span></li><li><span><a href=\"#Experiment-1-Chat\" data-toc-modified-id=\"Experiment-1-Chat-15.7\"><span class=\"toc-item-num\">15.7&nbsp;&nbsp;</span>Experiment 1 Chat</a></span></li><li><span><a href=\"#Experiment-2-Chat\" data-toc-modified-id=\"Experiment-2-Chat-15.8\"><span class=\"toc-item-num\">15.8&nbsp;&nbsp;</span>Experiment 2 Chat</a></span></li><li><span><a href=\"#Experiment-3-Chat\" data-toc-modified-id=\"Experiment-3-Chat-15.9\"><span class=\"toc-item-num\">15.9&nbsp;&nbsp;</span>Experiment 3 Chat</a></span></li></ul></li><li><span><a href=\"#Experiment-(Extreme-Pruning)\" data-toc-modified-id=\"Experiment-(Extreme-Pruning)-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Experiment (Extreme Pruning)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddacdd",
   "metadata": {},
   "source": [
    "# Model Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95f335f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:28.659121Z",
     "start_time": "2024-06-12T06:20:28.648218Z"
    }
   },
   "outputs": [],
   "source": [
    "model_used = \"base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1d798",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430e33e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:29.221539Z",
     "start_time": "2024-06-12T06:20:29.218501Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0044f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:29.280741Z",
     "start_time": "2024-06-12T06:20:29.274196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_template = True if model_used=='chat' else False\n",
    "use_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122cebd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:29.333001Z",
     "start_time": "2024-06-12T06:20:29.330225Z"
    }
   },
   "outputs": [],
   "source": [
    "hf_llama_path = \"/data/home/milinbhade/LLaMa/hf_llama/\"\n",
    "hf_llama_chat_path = \"/data/home/milinbhade/LLaMa/hf_llama_chat/\"\n",
    "\n",
    "alpacha_dataset_path = \"/data/home/milinbhade/LLaMa/pretrain_data/alpaca/alpaca-cleaned-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffc0e2",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e24e0a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:31.935607Z",
     "start_time": "2024-06-12T06:20:29.433323Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import fnmatch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import json\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba58004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.002451Z",
     "start_time": "2024-06-12T06:20:31.938514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6655d385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.065815Z",
     "start_time": "2024-06-12T06:20:32.004327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35d0a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.265653Z",
     "start_time": "2024-06-12T06:20:32.068214Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbc136",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c854fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.275582Z",
     "start_time": "2024-06-12T06:20:32.267836Z"
    },
    "code_folding": [
     0,
     1,
     5,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "# Load and process wikitext2 dataset\n",
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    # Load train and test datasets\n",
    "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    valdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    # Encode datasets\n",
    "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
    "    valenc = tokenizer(\"\\n\\n\".join(valdata['text']), return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "\n",
    "    # Generate samples from training set\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, valenc, testenc\n",
    "\n",
    "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
    "    if 'wikitext2' in name:\n",
    "        return get_wikitext2(nsamples, seed, seqlen, tokenizer)\n",
    "    if \"c4\" in name:\n",
    "        return get_c4(nsamples, seed, seqlen, tokenizer)\n",
    "    if \"bookcorpus\" in name:\n",
    "        return get_book_corpus(nsamples, seed, seqlen, tokenizer)\n",
    "    if \"ptb\" in name:\n",
    "        return get_ptb(nsamples, seed, seqlen, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6338db",
   "metadata": {},
   "source": [
    "## Eval PPL on wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07075e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.293223Z",
     "start_time": "2024-06-12T06:20:32.277546Z"
    },
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "# Function to evaluate perplexity (ppl) on a specified model and tokenizer\n",
    "def eval_ppl(model, tokenizer, device=torch.device(\"cuda:0\")):\n",
    "    # Set dataset\n",
    "    dataset = \"wikitext2\"\n",
    "\n",
    "    # Print status\n",
    "    print(f\"evaluating on {dataset}\")\n",
    "\n",
    "    # Get the test loader\n",
    "    _, _, testloader = get_loaders(\n",
    "        dataset, seed=0, seqlen=model.seqlen, tokenizer=tokenizer \n",
    "    )\n",
    "\n",
    "    # Evaluate ppl in no grad context to avoid updating the model\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)\n",
    "    return ppl_test \n",
    "\n",
    "# Function to evaluate perplexity (ppl) specifically on the wikitext dataset\n",
    "def eval_ppl_wikitext_train(model, trainloader, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    # testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    # nsamples = testenc.numel() // model.seqlen\n",
    "    nsamples = len(trainloader)\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in range(0,nsamples,bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"sample {i}\")\n",
    "\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        # inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = trainloader[i][0].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return ppl.item()\n",
    "\n",
    "# Function to evaluate perplexity (ppl) specifically on the wikitext dataset\n",
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    # nsamples = testenc.numel() // model.seqlen\n",
    "    nsamples = 25\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in tqdm(range(0,nsamples,bs), desc =\"WikiText Validation: \"):\n",
    "#         if i % 5 == 0:\n",
    "#             print(f\"sample {i}\")\n",
    "\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        print(inputs.shape)\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"PPL: \", ppl.item())\n",
    "\n",
    "    return ppl.item()\n",
    "\n",
    "def eval_zero_shot(model_name, model, tokenizer, saved_dir, task_list=[\"piqa\",\"boolq\",\"rte\",\"hellaswag\",\"winogrande\",\"arc_challenge\",\"arc_easy\",\"openbookqa\"], \n",
    "        num_fewshot=0, use_accelerate=False, add_special_tokens=False):\n",
    "    from lm_eval import tasks, evaluator \n",
    "    def pattern_match(patterns, source_list):\n",
    "        task_names = set()\n",
    "        for pattern in patterns:\n",
    "            for matching in fnmatch.filter(source_list, pattern):\n",
    "                task_names.add(matching)\n",
    "        return list(task_names)\n",
    "    \n",
    "    print(\"Model passed to evaluation: \", model)\n",
    "    task_names = pattern_match(task_list, tasks.get_task_dict([\"boolq\",\"rte\",\"hellaswag\",\"winogrande\",\"arc_challenge\",\"arc_easy\",\"openbookqa\"]).keys())\n",
    "    model_args = f\"pretrained={saved_dir}\"\n",
    "    limit = 100\n",
    "    if \"70b\" in model_name or \"65b\" in model_name:\n",
    "        limit = 2000\n",
    "    if use_accelerate:\n",
    "        model_args = f\"pretrained={model_path}, cache_dir={model_path},use_accelerate=True\"\n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"huggingface\",\n",
    "        model_args=model_args,    ## This is responsible for taking the pruned model\n",
    "        tasks=task_names,\n",
    "        num_fewshot=num_fewshot,\n",
    "        # pretrained_model=model,\n",
    "        batch_size=None,\n",
    "        device=None,\n",
    "        limit=limit,\n",
    "    )\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5a03c",
   "metadata": {},
   "source": [
    "## Pruning Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0141b850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.315800Z",
     "start_time": "2024-06-12T06:20:32.295331Z"
    },
    "code_folding": [
     0,
     2,
     6,
     99,
     122,
     145,
     173
    ]
   },
   "outputs": [],
   "source": [
    "def find_pruneable_heads_and_indices(\n",
    "    heads, n_heads, head_size, already_pruned_heads = set()\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the heads and their indices taking `already_pruned_heads` into account.\n",
    "\n",
    "    Args:\n",
    "        heads (`List[int]`): List of the indices of heads to prune.\n",
    "        n_heads (`int`): The number of heads in the model.\n",
    "        head_size (`int`): The size of each head.\n",
    "        already_pruned_heads (`Set[int]`): A set of already pruned heads.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[Set[int], torch.LongTensor]`: A tuple with the indices of heads to prune taking `already_pruned_heads`\n",
    "        into account and the indices of rows/columns to keep in the layer weight.\n",
    "    \"\"\"\n",
    "    mask = torch.ones(n_heads, head_size)\n",
    "    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n",
    "    for head in heads:\n",
    "        # Compute how many pruned heads are before the head and move the index accordingly\n",
    "        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n",
    "        mask[head] = 0\n",
    "    mask = mask.view(-1).contiguous().eq(1)\n",
    "    index: torch.LongTensor = torch.arange(len(mask))[mask].long()\n",
    "    return heads, index\n",
    "\n",
    "\n",
    "\n",
    "def prune_linear_by_mask(layer: nn.Linear, index: torch.LongTensor, dim: int = 0) -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Masking based Pruning of linear layer, just make entries zeros\n",
    "    Not change size of the matrices\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (`torch.nn.Linear`): The layer to prune.\n",
    "        index (`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (`int`, *optional*, defaults to 0): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        `torch.nn.Linear`: The pruned layer as a new layer with `requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    mask = torch.zeros_like(layer.weight)\n",
    "    if dim == 0:\n",
    "        mask[index] = 1\n",
    "    else:\n",
    "        mask[:, index] = 1\n",
    "\n",
    "    layer.weight.data *= mask\n",
    "    layer.weight.requires_grad = True\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        bias_mask = torch.zeros_like(layer.bias)\n",
    "        if dim == 0:\n",
    "            bias_mask[index] = 1\n",
    "        else:\n",
    "            bias_mask = 1\n",
    "        layer.bias.data *= bias_mask\n",
    "        layer.bias.requires_grad = True\n",
    "\n",
    "    return layer\n",
    "\n",
    "def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0) -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Prune a linear layer to keep only entries in index.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (`torch.nn.Linear`): The layer to prune.\n",
    "        index (`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (`int`, *optional*, defaults to 0): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        `torch.nn.Linear`: The pruned layer as a new layer with `requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    assert max(index) < layer.weight.size(dim), \"Index out of bounds\"\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if layer.bias is not None:\n",
    "        if dim == 1:\n",
    "            b = layer.bias.clone().detach()\n",
    "        else:\n",
    "            b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())\n",
    "    new_size[dim] = len(index)\n",
    "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    if layer.bias is not None:\n",
    "        new_layer.bias.requires_grad = False\n",
    "        new_layer.bias.copy_(b.contiguous())\n",
    "        new_layer.bias.requires_grad = True\n",
    "    return new_layer\n",
    "\n",
    "\n",
    "def get_heads_to_prune(model, importance_func, threshold):\n",
    "    import numpy as np\n",
    "    # Initialize dictionary to hold layers and heads to prune\n",
    "    heads_to_prune = {}\n",
    "\n",
    "    # Loop over each layer in the model\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        # Calculate the importance of the weights of each head in the layer using the provided function\n",
    "        weights = layer.self_attn.q_proj.weight + layer.self_attn.k_proj.weight + layer.self_attn.v_proj.weight\n",
    "        head_importances = importance_func(weights.view(layer.self_attn.num_heads, -1), target=torch.zeros_like(weights.view(layer.self_attn.num_heads, -1)), reduction='none').sum(dim=-1)\n",
    "        head_importances = head_importances.detach().cpu().numpy()\n",
    "        \n",
    "        d_prime = int(np.ceil(layer.self_attn.num_heads * threshold))\n",
    "        print(d_prime)\n",
    "        sorted_idx = np.argsort(head_importances)\n",
    "        preserve_idx = sorted_idx[d_prime:]\n",
    "        prune_indices = sorted_idx[:d_prime]\n",
    "\n",
    "        # Add the layer and the heads to prune to the dictionary\n",
    "        heads_to_prune[i] = prune_indices\n",
    "\n",
    "    return heads_to_prune\n",
    "\n",
    "def get_heads_to_prune_layer(model, importance_func, threshold, layer_index):\n",
    "    import numpy as np\n",
    "    # Initialize dictionary to hold layers and heads to prune\n",
    "    heads_to_prune = {}\n",
    "\n",
    "    layers = model.model.layers\n",
    "    layer = layers[layer_index]\n",
    "    # Calculate the importance of the weights of each head in the layer using the provided function\n",
    "    weights = layer.self_attn.q_proj.weight + layer.self_attn.k_proj.weight + layer.self_attn.v_proj.weight\n",
    "    head_importances = importance_func(weights.view(layer.self_attn.num_heads, -1), target=torch.zeros_like(weights.view(layer.self_attn.num_heads, -1)), reduction='none').sum(dim=-1)\n",
    "    head_importances = head_importances.detach().cpu().numpy()\n",
    "    \n",
    "    d_prime = int(np.ceil(layer.self_attn.num_heads * threshold))\n",
    "    print(d_prime)\n",
    "    sorted_idx = np.argsort(head_importances)\n",
    "    preserve_idx = sorted_idx[d_prime:]\n",
    "    prune_indices = sorted_idx[:d_prime]\n",
    "\n",
    "    # Add the layer and the heads to prune to the dictionary\n",
    "\n",
    "    return prune_indices\n",
    "\n",
    "\n",
    "def get_inter_indices_to_keep(model, layer, importance_func, threshold):\n",
    "    # Type : LlamaMLP\n",
    "    if isinstance(layer, int):\n",
    "        inter_layer = list(model.modules())[layer]\n",
    "    else:\n",
    "        inter_layer = layer\n",
    "    print(type(inter_layer))\n",
    "    assert isinstance(inter_layer, LlamaMLP)\n",
    "    inter_dim = inter_layer.intermediate_size\n",
    "    d_prime = int(np.ceil(inter_dim *(1 - threshold)))\n",
    "    print(d_prime)\n",
    "                  \n",
    "    \n",
    "    gate_weights = inter_layer.gate_proj.weight.data.cpu().numpy() \n",
    "    up_weights = inter_layer.up_proj.weight.data.cpu().numpy() \n",
    "    down_weights= inter_layer.down_proj.weight.data.cpu().numpy()\n",
    "    down_weights = np.transpose(down_weights)\n",
    "    \n",
    "    ## Add importance function here \n",
    "    \n",
    "    importance = torch.from_numpy(np.abs(gate_weights) + np.abs(up_weights) + np.abs(down_weights)).sum(dim=1)\n",
    "    print(len(importance))\n",
    "    sorted_idx = np.argsort(-importance)\n",
    "    preserve_idx = sorted_idx[:d_prime]\n",
    "    print(preserve_idx)\n",
    "    prune_idx = sorted_idx[d_prime:]\n",
    "    return preserve_idx, importance\n",
    "\n",
    "def find_layers(module, layers=[nn.Linear], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find the layers of a certain type in a module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): PyTorch module.\n",
    "        layers (list): List of layer types to find.\n",
    "        name (str): Name of the module.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layers of the given type(s) within the module.\n",
    "    \"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(\n",
    "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
    "        ))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae894f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269e710f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.321873Z",
     "start_time": "2024-06-12T06:20:32.317916Z"
    }
   },
   "outputs": [],
   "source": [
    "long_input = \"\"\"Provide a concise summary of the below passage.\n",
    "\n",
    "Hannah Arendt was one of the seminal political thinkers of the twentieth century. The power and originality of her thinking was evident in works such as The Origins of Totalitarianism, The Human Condition, On Revolution and The Life of the Mind. In these works and in numerous essays she grappled with the most crucial political events of her time, trying to grasp their meaning and historical import, and showing how they affected our categories of moral and political judgment. What was required, in her view, was a new framework that could enable us to come to terms with the twin horrors of the twentieth century, Nazism and Stalinism. She provided such framework in her book on totalitarianism, and went on to develop a new set of philosophical categories that could illuminate the human condition and provide a fresh perspective on the nature of political life.\n",
    "\n",
    "Although some of her works now belong to the classics of the Western tradition of political thought, she has always remained difficult to classify. Her political philosophy cannot be characterized in terms of the traditional categories of conservatism, liberalism, and socialism. Nor can her thinking be assimilated to the recent revival of communitarian political thought, to be found, for example, in the writings of A. MacIntyre, M. Sandel, C. Taylor and M. Walzer. Her name has been invoked by a number of critics of the liberal tradition, on the grounds that she presented a vision of politics that stood in opposition some key liberal principles. There are many strands of Arendt’s thought that could justify such a claim, in particular, her critique of representative democracy, her stress on civic engagement and political deliberation, her separation of morality from politics, and her praise of the revolutionary tradition. However, it would be a mistake to view Arendt as an anti-liberal thinker. Arendt was in fact a stern defender of constitutionalism and the rule of law, an advocate of fundamental human rights (among which she included not only the right to life, liberty, and freedom of expression, but also the right to action and to opinion), and a critic of all forms of political community based on traditional ties and customs, as well as those based on religious, ethnic, or racial identity.\n",
    "\n",
    "Arendt’s political thought cannot, in this sense, be identified either with the liberal tradition or with the claims advanced by a number of its critics. Arendt did not conceive of politics as a means for the satisfaction of individual preferences, nor as a way to integrate individuals around a shared conception of the good. Her conception of politics is based instead on the idea of active citizenship, that is, on the value and importance of civic engagement and collective deliberation about all matters affecting the political community. If there is a tradition of thought with which Arendt can be identified, it is the classical tradition of civic republicanism originating in Aristotle and embodied in the writings of Machiavelli, Montesquieu, Jefferson, and Tocqueville. According to this tradition politics finds its authentic expression whenever citizens gather together in a public space to deliberate and decide about matters of collective concern. Political activity is valued not because it may lead to agreement or to a shared conception of the good, but because it enables each citizen to exercise his or her powers of agency, to develop the capacities for judgment and to attain by concerted action some measure of political efficacy.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd13b4",
   "metadata": {},
   "source": [
    "# Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfeb5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.329432Z",
     "start_time": "2024-06-12T06:20:32.323827Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "inputs_chat = [\n",
    "  \"Think of some family rules to promote a healthy family relationship\",\n",
    "  \"In the series A Song of Ice and Fire, who is the founder of House Karstark?\",\n",
    "  \"which weighs more, cold or hot water?\",\n",
    "  \"Write a short paragraph about why you should not have both a pet cat and a pet bird.\",\n",
    "  \"Is beauty objective or subjective?\",\n",
    "  \"What is SVM?\",\n",
    "  \"What is the current capital of Japan?\",\n",
    "  \"Name 10 colors\",\n",
    "  \"How should I invest my money?\",\n",
    "  \"What are some ways to improve the value of your home?\",\n",
    "  \"What does fasting mean?\",\n",
    "  \"What is cloud computing in simple terms?\",\n",
    "  \"What is the meaning of life?\",\n",
    "  \"What is Linux?\",\n",
    "  \"Why do people like gardening?\",\n",
    "  \"What makes for a good photograph?\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs_base = [\n",
    "    \"Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them\",\n",
    "    \"Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster\",\n",
    "    \"The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future\",\n",
    "    \n",
    "    \"Artificial intelligence is transforming the healthcare industry by enabling\",\n",
    "    \"Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can\",\n",
    "    \"The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that\",\n",
    "    \n",
    "    \"Self-driving cars rely heavily on sensor data and advanced algorithms to navigate\",\n",
    "    \"One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can\",\n",
    "    \"In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that\",\n",
    "    \n",
    "    \"Reinforcement learning algorithms learn optimal behaviors through\",\n",
    "    \"Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could\",\n",
    "    \"The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that\",\n",
    "    \n",
    "    \"Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by\",\n",
    "    \"Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to\",\n",
    "    \n",
    "    \"The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and\",\n",
    "    \"Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f304a",
   "metadata": {},
   "source": [
    "# Generation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d4c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T19:25:35.236887Z",
     "start_time": "2024-05-17T19:25:35.230980Z"
    }
   },
   "source": [
    "## Text Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fe6980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:32.337963Z",
     "start_time": "2024-06-12T06:20:32.332252Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"\n",
    "<s>[INST]<<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "[/INST]\n",
    "\"\"\".format(\n",
    "    system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "    instruction=\"{instruction}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define parameters to generate text\n",
    "def gen_text(prompts, pipeline, tokenizer, use_template=False, **kwargs):\n",
    "    if use_template:\n",
    "        full_prompts = [\n",
    "            PROMPT_FOR_GENERATION_FORMAT.format(instruction=prompt)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        full_prompts = prompts\n",
    "\n",
    "    if \"batch_size\" not in kwargs:\n",
    "        kwargs[\"batch_size\"] = 1\n",
    "    \n",
    "    # the default max length is pretty small (20), which would cut the generated output in the middle, so it's necessary to increase the threshold to the complete response\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 512\n",
    "\n",
    "    # configure other text generation arguments, see common configurable args here: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,  # Hugging Face sets pad_token_id to eos_token_id by default; setting here to not see redundant message\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    outputs = pipeline(full_prompts, **kwargs)\n",
    "    outputs = [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# results = gen_text([\"What is a large language model?\"])\n",
    "# print(results[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693a24b",
   "metadata": {},
   "source": [
    "## Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52bb8b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:34.761660Z",
     "start_time": "2024-06-12T06:20:34.758441Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_num_tokens(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    return inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ed36b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:34.960292Z",
     "start_time": "2024-06-12T06:20:34.954661Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_gen_text_throughput(prompt, pipeline, tokenizer, use_template=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Return tuple ( number of tokens / sec, num tokens, output ) of the generated tokens\n",
    "    \"\"\"\n",
    "    if use_template:\n",
    "        full_prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=prompt)\n",
    "    else:\n",
    "        full_prompt = prompt\n",
    "\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 512\n",
    "\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "            \"return_tensors\": True,  # make the pipeline return token ids instead of decoded text to get the number of generated tokens\n",
    "        }\n",
    "    )\n",
    "\n",
    "    num_input_tokens = get_num_tokens(full_prompt)\n",
    "\n",
    "    # measure the time it takes for text generation\n",
    "    start = time.time()\n",
    "    outputs = pipeline(full_prompt, **kwargs)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    # get the number of generated tokens\n",
    "    n_tokens = len(outputs[0][\"generated_token_ids\"])\n",
    "\n",
    "    # show the generated text in logging\n",
    "    result = tokenizer.batch_decode(\n",
    "        outputs[0][\"generated_token_ids\"][num_input_tokens:], skip_special_tokens=True\n",
    "    )\n",
    "    result = \" \".join(result)\n",
    "\n",
    "    return (n_tokens / duration, n_tokens, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec085d",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed523483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:44:25.256866Z",
     "start_time": "2024-05-18T19:44:24.871160Z"
    }
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac6edb07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:59:07.806551Z",
     "start_time": "2024-05-21T13:58:43.341344Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22db8b5f5d574cc9a5aab6d3b77f5dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "# model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d087f",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b532d2cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:33:13.980562Z",
     "start_time": "2024-05-21T18:33:04.462547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-22 00:03:05.801938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 00:03:07.073230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = \"meta-llama/Llama-2-7b-chat-hf\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(hf_llama_chat_path, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_llama_chat_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;43;03m#     revision=revision,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Required tokenizer setting for batch inference\u001b[39;00m\n\u001b[1;32m     18\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/__init__.py:905\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 905\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    916\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:279\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    281\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/modeling_utils.py:3375\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3370\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3371\u001b[0m )\n\u001b[1;32m   3373\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3374\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3375\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1106\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:927\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 927\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    928\u001b[0m )\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:927\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 927\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    928\u001b[0m )\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:702\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mLLAMA_ATTENTION_CLASSES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaAttention.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias)\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:296\u001b[0m, in \u001b[0;36mLlamaAttention._init_rope\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_rope\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaRotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m         scaling_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:109\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.__init__\u001b[0;34m(self, dim, max_position_embeddings, base, device, scaling_factor)\u001b[0m\n\u001b[1;32m    107\u001b[0m freqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mouter(t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_freq)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Different from paper, but it uses a different permutation in order to obtain the same calculation\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cos_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb\u001b[38;5;241m.\u001b[39mcos()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mget_default_dtype()), persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sin_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb\u001b[38;5;241m.\u001b[39msin()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mget_default_dtype()), persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "# model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_chat_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_chat_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dd154eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:59:16.089180Z",
     "start_time": "2024-05-21T13:59:16.085720Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1849982d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:58:18.430950Z",
     "start_time": "2024-05-21T13:58:18.413762Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02e6056d",
   "metadata": {},
   "source": [
    "# Performance of Orig Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45d94613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:27:13.962077Z",
     "start_time": "2024-05-21T18:27:13.955749Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba5ec4",
   "metadata": {},
   "source": [
    "## Find performance zero shot + some prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba853e",
   "metadata": {},
   "source": [
    "### PPL on wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb44208f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T17:54:02.833573Z",
     "start_time": "2024-05-18T17:51:26.492557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /data/home/milinbhade/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Apr 23 16:29:25 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:14<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  5.695328712463379\n"
     ]
    }
   ],
   "source": [
    "ppl_test = eval_ppl(pipeline.model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d2750",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbe5bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:28:50.650858Z",
     "start_time": "2024-05-21T18:27:33.335341Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:21,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:21,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:02<00:20,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:03<00:19,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:04<00:18,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:05<00:17,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:06<00:15,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:07<00:14,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:07<00:13,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:08<00:12,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:09<00:11,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:10<00:11,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:11<00:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:12<00:09,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:13<00:08,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:14<00:08,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:15<00:07,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:15<00:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:16<00:05,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:17<00:04,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:18<00:03,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:19<00:02,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:20<00:01,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:21<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:21<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  7.347228527069092\n"
     ]
    }
   ],
   "source": [
    "ppl_test = eval_ppl(pipeline.model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffd676",
   "metadata": {},
   "source": [
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "437fab7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:09:35.689643Z",
     "start_time": "2024-05-18T20:09:35.684885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/home/milinbhade/Milin/AMC/bertamc_v4/finetune'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c78d66a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:33:21.656951Z",
     "start_time": "2024-05-21T18:33:21.652583Z"
    }
   },
   "outputs": [],
   "source": [
    "m = \"huggingface\"\n",
    "accelerate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f68bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:15:15.659050Z",
     "start_time": "2024-05-18T20:15:15.655823Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad0060e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:34:29.369979Z",
     "start_time": "2024-05-21T18:34:23.106700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /data/home/milinbhade/LLaMa/hf_llama/\n",
      "Model passed to evaluation:  None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m hf_llama_path\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(save_dir))\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43meval_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_shot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccelerate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*****\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 133\u001b[0m, in \u001b[0;36meval_zero_shot\u001b[0;34m(model_name, model, tokenizer, saved_dir, task_list, num_fewshot, use_accelerate, add_special_tokens)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(task_names)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel passed to evaluation: \u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[0;32m--> 133\u001b[0m task_names \u001b[38;5;241m=\u001b[39m pattern_match(task_list, \u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_task_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboolq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrte\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhellaswag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwinogrande\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marc_challenge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marc_easy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenbookqa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    134\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/tasks/__init__.py:418\u001b[0m, in \u001b[0;36mget_task_dict\u001b[0;34m(task_name_list, task_manager)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string_task_name_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         task_manager \u001b[38;5;241m=\u001b[39m \u001b[43mTaskManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     task_name_from_string_dict \u001b[38;5;241m=\u001b[39m task_manager\u001b[38;5;241m.\u001b[39mload_task_or_group(\n\u001b[1;32m    421\u001b[0m         string_task_name_list\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_element \u001b[38;5;129;01min\u001b[39;00m others_task_name_list:\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/tasks/__init__.py:23\u001b[0m, in \u001b[0;36mTaskManager.__init__\u001b[0;34m(self, verbosity, include_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39meval_logger\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39msetLevel(\u001b[38;5;28mgetattr\u001b[39m(logging, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mverbosity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_index\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_group_map \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/tasks/__init__.py:45\u001b[0m, in \u001b[0;36mTaskManager.initialize_tasks\u001b[0;34m(self, include_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m task_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_dir \u001b[38;5;129;01min\u001b[39;00m all_paths:\n\u001b[0;32m---> 45\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_task_and_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     task_index \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtasks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtask_index}\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m task_index\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/tasks/__init__.py:303\u001b[0m, in \u001b[0;36mTaskManager._get_task_and_group\u001b[0;34m(self, task_dir)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    302\u001b[0m     yaml_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, f)\n\u001b[0;32m--> 303\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_yaml_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_is_python_task(config):\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# This is a python class config\u001b[39;00m\n\u001b[1;32m    306\u001b[0m         tasks_and_groups[config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython_task\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: yaml_path,\n\u001b[1;32m    309\u001b[0m         }\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/utils.py:349\u001b[0m, in \u001b[0;36mload_yaml_config\u001b[0;34m(yaml_path, yaml_config, yaml_dir, mode)\u001b[0m\n\u001b[1;32m    346\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(yaml_dir, path)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     included_yaml_config \u001b[38;5;241m=\u001b[39m \u001b[43mload_yaml_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     final_yaml_config\u001b[38;5;241m.\u001b[39mupdate(included_yaml_config)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# If failed to load, ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/utils.py:349\u001b[0m, in \u001b[0;36mload_yaml_config\u001b[0;34m(yaml_path, yaml_config, yaml_dir, mode)\u001b[0m\n\u001b[1;32m    346\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(yaml_dir, path)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     included_yaml_config \u001b[38;5;241m=\u001b[39m \u001b[43mload_yaml_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     final_yaml_config\u001b[38;5;241m.\u001b[39mupdate(included_yaml_config)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# If failed to load, ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/LLaMa/lm-evaluation-harness/lm_eval/utils.py:324\u001b[0m, in \u001b[0;36mload_yaml_config\u001b[0;34m(yaml_path, yaml_config, yaml_dir, mode)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m yaml_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(yaml_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 324\u001b[0m         yaml_config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m yaml_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     yaml_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(yaml_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/__init__.py:105\u001b[0m, in \u001b[0;36mfull_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_load\u001b[39m(stream):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    unsafe on untrusted input.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFullLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/constructor.py:49\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_document(node)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/composer.py:36\u001b[0m, in \u001b[0;36mComposer.get_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n\u001b[0;32m---> 36\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/composer.py:55\u001b[0m, in \u001b[0;36mComposer.compose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Compose the root node.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/composer.py:127\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m anchor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors[anchor] \u001b[38;5;241m=\u001b[39m node\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMappingEndEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m#key_event = self.peek_event()\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/parser.py:98\u001b[0m, in \u001b[0;36mParser.check_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate:\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/parser.py:428\u001b[0m, in \u001b[0;36mParser.parse_block_mapping_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_block_mapping_key\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeyToken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    429\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_token(KeyToken, ValueToken, BlockEndToken):\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/scanner.py:116\u001b[0m, in \u001b[0;36mScanner.check_token\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mchoices):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Check if the next token is one of the given types.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_more_tokens():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_more_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/scanner.py:255\u001b[0m, in \u001b[0;36mScanner.fetch_more_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# It must be a plain scalar then.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_plain():\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# No? It's an error. Let's produce a nice error message.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ScannerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile scanning for the next token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound character \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m that cannot start any token\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ch,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mark())\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/scanner.py:679\u001b[0m, in \u001b[0;36mScanner.fetch_plain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_simple_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m# Scan and add SCALAR. May change `allow_simple_key`.\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/yaml/scanner.py:1295\u001b[0m, in \u001b[0;36mScanner.scan_plain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1290\u001b[0m     ch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeek(length)\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x85\u001b[39;00m\u001b[38;5;130;01m\\u2028\u001b[39;00m\u001b[38;5;130;01m\\u2029\u001b[39;00m\u001b[38;5;124m'\u001b[39m    \\\n\u001b[1;32m   1292\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m (ch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeek(length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x85\u001b[39;00m\u001b[38;5;130;01m\\u2028\u001b[39;00m\u001b[38;5;130;01m\\u2029\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1294\u001b[0m                           \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,[]\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_level \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\\\n\u001b[0;32m-> 1295\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_level\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,?[]\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m     length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task_list = [\"boolq\", \"rte\",\"hellaswag\",\"winogrande\", \"arc_easy\",\"arc_challenge\", \"openbookqa\"]\n",
    "num_shot = 0\n",
    "save_dir = hf_llama_path\n",
    "print(\"Loading checkpoint from {}\".format(save_dir))\n",
    " \n",
    "results = eval_zero_shot(m, None, tokenizer, save_dir, task_list, num_shot, accelerate)\n",
    "print(\"\\n\\n\")\n",
    "print(\"*****\"*30)\n",
    "print(\"zero_shot evaluation results\")\n",
    "print(results['results'])\n",
    "print(\"\\n\\n\")\n",
    "print(\"*****\"*30)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70a6d0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:31:42.944518Z",
     "start_time": "2024-05-18T20:31:42.879965Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('LLama2_orig_metrics.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5db676ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:53.743196Z",
     "start_time": "2024-05-18T20:29:53.737237Z"
    },
    "code_folding": [
     11
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     boolq     |    0.76   | 0.04292346959909281  |      None      |          None         |\n",
      "|    arc_easy   |    0.72   | 0.04512608598542128  |      0.75      |  0.04351941398892446  |\n",
      "| arc_challenge |    0.37   |  0.048523658709391   |      0.42      |  0.049604496374885836 |\n",
      "|   hellaswag   |    0.54   | 0.05009082659620333  |      0.69      |  0.04648231987117316  |\n",
      "|   winogrande  |    0.7    | 0.046056618647183814 |      None      |          None         |\n",
      "|      rte      |    0.65   | 0.04793724854411019  |      None      |          None         |\n",
      "|   openbookqa  |    0.28   | 0.045126085985421276 |      0.45      |  0.049999999999999996 |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "\n",
    "# Create a PrettyTable object\n",
    "table = PrettyTable()\n",
    "\n",
    "# Add columns\n",
    "table.field_names = [\"Alias\", \"Acc, None\", \"Acc StdErr, None\", \"Acc Norm, None\", \"Acc Norm StdErr, None\"]\n",
    "\n",
    "# Add rows\n",
    "for key, value in results['results'].items():\n",
    "    row = [value.get('alias'), value.get('acc,none'), value.get('acc_stderr,none'), value.get('acc_norm,none'), value.get('acc_norm_stderr,none')]\n",
    "    table.add_row(row)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912462fd",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cacf1632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:34:35.917758Z",
     "start_time": "2024-05-21T18:34:35.914234Z"
    }
   },
   "outputs": [],
   "source": [
    "m = \"huggingface\"\n",
    "accelerate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd40863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:34:36.497488Z",
     "start_time": "2024-05-21T18:34:36.494316Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = hf_llama_chat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a680bc9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:45:30.216152Z",
     "start_time": "2024-05-21T18:34:43.989415Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /data/home/milinbhade/LLaMa/hf_llama_chat/\n",
      "Model passed to evaluation:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22:00:04:52,280 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:00:04:52,282 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-22:00:04:59,344 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:00:04:59,345 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-22:00:05:40,595 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-22:00:05:40,596 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/LLaMa/hf_llama_chat/'}\n",
      "2024-05-22:00:05:40,611 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-22:00:05:40,612 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8909b94c6006431d9c44c72e96ed075e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-05-22:00:06:30,571 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:00:06:30,573 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-22:00:06:54,539 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:00:06:54,542 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-22:00:06:59,437 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-22:00:06:59,441 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-22:00:06:59,444 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-22:00:06:59,447 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-22:00:06:59,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-22:00:06:59,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-22:00:06:59,456 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-22:00:06:59,463 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1464.28it/s]\n",
      "2024-05-22:00:06:59,575 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1654.23it/s]\n",
      "2024-05-22:00:06:59,662 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 49566.34it/s]\n",
      "2024-05-22:00:06:59,675 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1386.12it/s]\n",
      "2024-05-22:00:06:59,756 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 779.60it/s]\n",
      "2024-05-22:00:06:59,898 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1524.59it/s]\n",
      "2024-05-22:00:06:59,978 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 775.96it/s]\n",
      "2024-05-22:00:07:00,120 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [08:15<00:00,  4.44it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "zero_shot evaluation results\n",
      "{'rte': {'acc,none': 0.71, 'acc_stderr,none': 0.04560480215720684, 'alias': 'rte'}, 'hellaswag': {'acc,none': 0.53, 'acc_stderr,none': 0.050161355804659205, 'acc_norm,none': 0.73, 'acc_norm_stderr,none': 0.044619604333847394, 'alias': 'hellaswag'}, 'winogrande': {'acc,none': 0.72, 'acc_stderr,none': 0.04512608598542127, 'alias': 'winogrande'}, 'boolq': {'acc,none': 0.76, 'acc_stderr,none': 0.04292346959909283, 'alias': 'boolq'}, 'arc_easy': {'acc,none': 0.7, 'acc_stderr,none': 0.046056618647183814, 'acc_norm,none': 0.71, 'acc_norm_stderr,none': 0.045604802157206845, 'alias': 'arc_easy'}, 'openbookqa': {'acc,none': 0.34, 'acc_stderr,none': 0.04760952285695235, 'acc_norm,none': 0.4, 'acc_norm_stderr,none': 0.04923659639173309, 'alias': 'openbookqa'}, 'arc_challenge': {'acc,none': 0.41, 'acc_stderr,none': 0.04943110704237102, 'acc_norm,none': 0.41, 'acc_norm_stderr,none': 0.049431107042371025, 'alias': 'arc_challenge'}}\n",
      "\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "task_list = [\"boolq\", \"rte\",\"hellaswag\",\"winogrande\", \"arc_easy\",\"arc_challenge\", \"openbookqa\"]\n",
    "num_shot = 0\n",
    "\n",
    "print(\"Loading checkpoint from {}\".format(save_dir))\n",
    " \n",
    "results = eval_zero_shot(m, None, tokenizer, save_dir, task_list, num_shot, accelerate)\n",
    "print(\"\\n\\n\")\n",
    "print(\"*****\"*30)\n",
    "print(\"zero_shot evaluation results\")\n",
    "print(results['results'])\n",
    "print(\"\\n\\n\")\n",
    "print(\"*****\"*30)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0939bb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:45:46.828307Z",
     "start_time": "2024-05-21T18:45:46.741880Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('LLama2_orig_chat_metrics.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d47339a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:45:48.335406Z",
     "start_time": "2024-05-21T18:45:48.317351Z"
    },
    "code_folding": [
     11
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|      rte      |    0.71   | 0.04560480215720684  |      None      |          None         |\n",
      "|   hellaswag   |    0.53   | 0.050161355804659205 |      0.73      |  0.044619604333847394 |\n",
      "|   winogrande  |    0.72   | 0.04512608598542127  |      None      |          None         |\n",
      "|     boolq     |    0.76   | 0.04292346959909283  |      None      |          None         |\n",
      "|    arc_easy   |    0.7    | 0.046056618647183814 |      0.71      |  0.045604802157206845 |\n",
      "|   openbookqa  |    0.34   | 0.04760952285695235  |      0.4       |  0.04923659639173309  |\n",
      "| arc_challenge |    0.41   | 0.04943110704237102  |      0.41      |  0.049431107042371025 |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "\n",
    "# Create a PrettyTable object\n",
    "table = PrettyTable()\n",
    "\n",
    "# Add columns\n",
    "table.field_names = [\"Alias\", \"Acc, None\", \"Acc StdErr, None\", \"Acc Norm, None\", \"Acc Norm StdErr, None\"]\n",
    "\n",
    "# Add rows\n",
    "for key, value in results['results'].items():\n",
    "    row = [value.get('alias'), value.get('acc,none'), value.get('acc_stderr,none'), value.get('acc_norm,none'), value.get('acc_norm_stderr,none')]\n",
    "    table.add_row(row)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a81814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0830162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c8321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550b3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccd96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e468436",
   "metadata": {},
   "source": [
    "# Compare benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e621e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T04:01:40.390926Z",
     "start_time": "2024-05-21T04:01:40.378718Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pr=0.95, lbound=0.8, rbound=1.0\n",
    "\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "|   hellaswag   |    0.53   | 0.050161355804659205 |      0.68      |  0.04688261722621504  |\n",
    "|     boolq     |    0.74   | 0.04408440022768078  |      None      |          None         |\n",
    "| arc_challenge |    0.42   | 0.04960449637488584  |      0.44      |  0.04988876515698589  |\n",
    "|      rte      |    0.7    | 0.046056618647183814 |      None      |          None         |\n",
    "|   winogrande  |    0.71   | 0.04560480215720684  |      None      |          None         |\n",
    "|   openbookqa  |    0.3    | 0.046056618647183814 |      0.44      |  0.04988876515698589  |\n",
    "|    arc_easy   |    0.71   | 0.04560480215720684  |      0.75      |  0.04351941398892446  |\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "\n",
    "\n",
    "pr=0.95, lbound=0.8, rbound=1.0 (Reward Changed)\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "|   winogrande  |    0.69   | 0.04648231987117316  |      None      |          None         |\n",
    "|    arc_easy   |    0.7    | 0.046056618647183814 |      0.76      |  0.04292346959909284  |\n",
    "| arc_challenge |    0.42   | 0.04960449637488584  |      0.43      |  0.049756985195624284 |\n",
    "|      rte      |    0.72   | 0.045126085985421276 |      None      |          None         |\n",
    "|     boolq     |    0.74   | 0.04408440022768078  |      None      |          None         |\n",
    "|   hellaswag   |    0.53   | 0.050161355804659205 |      0.68      |  0.04688261722621504  |\n",
    "|   openbookqa  |    0.31   | 0.04648231987117316  |      0.44      |  0.04988876515698589  |\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "\n",
    "pr=0.95, lbound=0.8, rbound=0.95 (Reward Changed)\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "|    arc_easy   |    0.6    | 0.04923659639173309  |      0.67      |  0.04725815626252607  |\n",
    "|   hellaswag   |    0.43   | 0.049756985195624284 |      0.65      |  0.04793724854411019  |\n",
    "|     boolq     |    0.7    | 0.046056618647183814 |      None      |          None         |\n",
    "| arc_challenge |    0.32   | 0.04688261722621504  |      0.34      |  0.04760952285695236  |\n",
    "|   winogrande  |    0.61   | 0.04902071300001975  |      None      |          None         |\n",
    "|   openbookqa  |    0.23   | 0.04229525846816506  |      0.41      |  0.04943110704237102  |\n",
    "|      rte      |    0.56   | 0.049888765156985884 |      None      |          None         |\n",
    "+---------------+-----------+----------------------+----------------+-----------------------+\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa984d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T10:56:01.247461Z",
     "start_time": "2024-05-18T10:56:01.244205Z"
    }
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57df07d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T17:54:52.428574Z",
     "start_time": "2024-05-18T17:54:24.615822Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large amount of text data to generate human-like text. LLMs are typically used for tasks such as natural language processing (NLP), machine translation, and text generation.\n",
      "What is the difference between a large language model and a small language model?\n",
      "A large language model is a type of artificial intelligence (AI) model that is trained on a large amount of text data to generate human-like text. LLMs are typically used for tasks such as natural language processing (NLP), machine translation, and text generation. A small language model is a type of artificial intelligence (AI) model that is trained on a small amount of text data to generate human-like text. SLMs are typically used for tasks such as natural language processing (NLP), machine translation, and text generation.\n",
      "What are the benefits of using a large language model?\n",
      "The benefits of using a large language model include:\n",
      "1. Improved accuracy: LLMs are trained on a large amount of text data, which allows them to generate more accurate text than smaller models.\n",
      "2. Increased efficiency: LLMs are typically faster than smaller models, which can save time and resources.\n",
      "3. Greater flexibility: LLMs can be used for a variety of tasks, such as natural language processing (NLP), machine translation, and text generation.\n",
      "4. Improved scalability: LLMs can be easily scaled up to handle larger amounts of text data.\n",
      "5. Greater versatility: LLMs can be used for a variety of tasks, such as natural language processing (NLP), machine translation, and text generation.\n",
      "What are the challenges of using a large language model?\n",
      "The challenges of using a large language model include:\n",
      "1. Increased complexity: LLMs are typically more complex than smaller models, which can make them more difficult to use.\n",
      "2. Increased cost: LLMs are typically more expensive to train and maintain than smaller models.\n",
      "3. Increased time: LLMs are typically slower than smaller models, which can increase the time required to complete tasks.\n",
      "4. Increased resources: LLMs require more resources, such as memory and processing power, than smaller models.\n",
      "5. Increased risk: LLMs are typically more pr\n"
     ]
    }
   ],
   "source": [
    "# Use args such as temperature and max_new_tokens to control text generation\n",
    "results = gen_text([\"What is a large language model?\"], temperature=0.5, max_new_tokens=512, use_template=use_template)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a29f9f",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d859cb01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T18:36:58.963674Z",
     "start_time": "2024-05-18T18:36:58.958391Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_output(output_text):\n",
    "    # Remove repeated sentences\n",
    "    unique_sentences = set()\n",
    "    cleaned_output = []\n",
    "    \n",
    "    for sentence in output_text.split('.'):\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence not in unique_sentences:\n",
    "            cleaned_output.append(sentence)\n",
    "            unique_sentences.add(sentence)\n",
    "    \n",
    "    # Remove garbage symbols\n",
    "#     cleaned_output = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in cleaned_output]\n",
    "    \n",
    "    # Exclude the last sentence if it does not end with a period\n",
    "    if output_text[-1] != '.':\n",
    "        cleaned_output = cleaned_output[:-1]\n",
    "    \n",
    "    return \". \".join(cleaned_output) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baffe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d81e97a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T18:52:59.715690Z",
     "start_time": "2024-05-18T18:46:04.645548Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "Gardening is a great way to get exercise and fresh air.\n",
      "Gardening can be a great way to connect with nature and the environment.\n",
      "Gardening can be a great way to save money on food.\n",
      "Gardening can be a great way to reduce stress and anxiety.\n",
      "Gardening can be a great way to connect with friends and family.\n",
      "Gardening can be a great way to learn about plants and the environment.\n",
      "Gardening can be a great way to get involved in the community.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to learn about different religions.\n",
      "Gardening can be a great way to learn about different languages.\n",
      "Gardening can be a great way to learn about different countries.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to learn about different religions.\n",
      "Gardening can be a great way to learn about different languages.\n",
      "Gardening can be a great way to learn about different countries.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to learn about different religions.\n",
      "Gardening can be a great way to learn about different languages.\n",
      "Gardening can be a great way to learn about different countries.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to learn about different religions.\n",
      "Gardening can be a great way to learn about different languages.\n",
      "Gardening can be a great way to learn about different countries.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to learn about different religions.\n",
      "Gardening can be a great way to learn about different languages.\n",
      "Gardening can be a great way to learn about different countries.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to learn about different religions.\n",
      "Gardening can be a great way to learn about different languages.\n",
      "Gardening can be a great way to learn about different countries.\n",
      "Gardening can be a great way to learn about different cultures.\n",
      "Gardening can be a great way to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. Gardening is a great way to get exercise and fresh air. Gardening can be a great way to connect with nature and the environment. Gardening can be a great way to save money on food. Gardening can be a great way to reduce stress and anxiety. Gardening can be a great way to connect with friends and family. Gardening can be a great way to learn about plants and the environment. Gardening can be a great way to get involved in the community. Gardening can be a great way to learn about different cultures. Gardening can be a great way to learn about different religions. Gardening can be a great way to learn about different languages. Gardening can be a great way to learn about different countries.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of community and connection.\n",
      "Family traditions can also be a source of pride and identity. They can be a way to celebrate cultural heritage or to honor the values and beliefs that are important to the family. For example, some families may have a tradition of volunteering together or of attending religious services.\n",
      "Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to connect with loved ones who may be far away or who have passed away.\n",
      "Family traditions can also be a source of fun and enjoyment. They can provide opportunities for laughter, storytelling, and shared experiences. They can also be a way to create new memories and to strengthen relationships.\n",
      "In conclusion, family traditions are an important part of family life. They provide a sense of continuity, belonging, pride, identity, comfort, security, and fun. They can be a source of strength and support, and they can help to create a sense of community and connection.\n",
      "Family traditions are an important part of family life. They provide a sense of continuity, belonging, pride, identity, comfort, security, and fun. They can be a source of strength and support, and they can help to create a sense of community and connection.\n",
      "Family traditions can be a source of pride and identity. They can be a way to celebrate cultural heritage or to honor the values and beliefs that are important to the family. For example, some families may have a tradition of volunteering together or of attending religious services.\n",
      "Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to connect with loved ones who may be far away or who have passed away.\n",
      "Family traditions can also be a source of fun and enjoyment. They can provide opportunities for laughter, storytelling, and shared experiences. They can also be a way to create new memories and to strengthen relationships.\n",
      "In conclusion, family traditions are an important part of family life. They provide a sense of continuity, belonging, pride, identity, comfort, security, and fun. They can be a source of strength and support, and they can help to create a sense of community and connection.\n",
      "Family traditions are an\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of community and connection. Family traditions can also be a source of pride and identity. They can be a way to celebrate cultural heritage or to honor the values and beliefs that are important to the family. For example, some families may have a tradition of volunteering together or of attending religious services. Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to connect with loved ones who may be far away or who have passed away. Family traditions can also be a source of fun and enjoyment. They can provide opportunities for laughter, storytelling, and shared experiences. They can also be a way to create new memories and to strengthen relationships. In conclusion, family traditions are an important part of family life. They provide a sense of continuity, belonging, pride, identity, comfort, security, and fun. They can be a source of strength and support, and they can help to create a sense of community and connection. Family traditions are an important part of family life. Family traditions can be a source of pride and identity.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will continue to evolve and adapt to the changing needs and preferences of society, creating new trends and styles that reflect the values and aspirations of the people who wear them.\n",
      "Fashion is a form of self-expression that adapts to the times.\n",
      "Fashion is a form of self-expression that adapts to the times. It is a way for people to express their individuality and identity through the clothes they wear. Fashion has evolved over time, reflecting changes in society and culture. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion has always been a way for people to express themselves.\n",
      "Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices.\n",
      "In the future, fashion will continue to evolve and adapt to the changing needs and preferences of society, creating new trends and styles that reflect the values and aspirations of the people who wear them.\n",
      "Fashion is a form of self-expression that adapts to the times. It is a way for people to express their individuality and identity through the clothes they wear. Fashion has evolved over time, reflecting changes in society and culture. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion has always been a way for people to express themselves. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future, fashion will continue to evolve and adapt to the changing needs and preferences of society, creating new trends and styles that reflect the values and aspirations of the people who wear them.\n",
      "Fashion is a form of self-expression that adapts to the times. It is a way for people to express their individuality and identity through the clothes they wear. Fashion has evolved over time, reflecting changes in society and culture. From the elaborate garments\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will continue to evolve and adapt to the changing needs and preferences of society, creating new trends and styles that reflect the values and aspirations of the people who wear them. Fashion is a form of self-expression that adapts to the times. It is a way for people to express their individuality and identity through the clothes they wear. Fashion has evolved over time, reflecting changes in society and culture. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion has always been a way for people to express themselves. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future, fashion will continue to evolve and adapt to the changing needs and preferences of society, creating new trends and styles that reflect the values and aspirations of the people who wear them.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m healthcare providers to improve patient care, reduce costs, and increase efficiency.\n",
      "Artificial intelligence is transforming the healthcare industry by enabling healthcare providers to improve patient care, reduce costs, and increase efficiency.\n",
      "Artificial intelligence is transforming the healthcare industry by enabling healthcare providers to improve patient care, reduce costs, and increase efficiency. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\n",
      "AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\n",
      "AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\n",
      "AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\n",
      "AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\n",
      "AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\n",
      "AI\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mhealthcare providers to improve patient care, reduce costs, and increase efficiency. Artificial intelligence is transforming the healthcare industry by enabling healthcare providers to improve patient care, reduce costs, and increase efficiency. AI is being used to improve patient outcomes, reduce the risk of medical errors, and improve the efficiency of healthcare operations.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m help investors make more informed decisions about when to buy and sell stocks.\n",
      "One of the most popular machine learning algorithms for stock market prediction is the support vector machine (SVM). SVMs are used to classify data points into different categories, and they have been shown to be effective at predicting stock market trends.\n",
      "Another popular machine learning algorithm for stock market prediction is the neural network. Neural networks are a type of artificial intelligence that mimic the way the human brain works. They are able to learn from data and make predictions based on that data.\n",
      "There are many different machine learning algorithms that can be used for stock market prediction. The most important thing is to find an algorithm that works well with the data you have available.\n",
      "Machine learning is a branch of artificial intelligence that deals with the design and development of algorithms that can learn from and make predictions on data. Machine learning algorithms are used in a variety of applications, including stock market prediction.\n",
      "There are many different types of machine learning algorithms, but the most common ones used for stock market prediction are linear regression, support vector machines, and neural networks. Each of these algorithms has its own strengths and weaknesses, and the choice of which one to use depends on the specific problem that needs to be solved.\n",
      "Linear regression is a simple algorithm that can be used to predict stock prices. It works by fitting a line to the data and then using that line to make predictions. Linear regression is easy to understand and implement, but it is not very accurate.\n",
      "Support vector machines are more complex than linear regression, but they are more accurate. Support vector machines work by finding the line that best separates the data into two groups. This line is called the decision boundary, and it is used to make predictions. Support vector machines are often used in applications where there is a lot of noise in the data, such as stock market prediction.\n",
      "Neural networks are the most complex type of machine learning algorithm, but they are also the most accurate. Neural networks work by mimicking the way the human brain works. They are made up of many interconnected nodes, and each node is responsible for processing a small piece of the data. Neural networks are very good at pattern recognition, and they can be used to make predictions on complex data sets.\n",
      "Machine learning is a branch of artificial intelligence that deals with the design and development of algorithms that can learn from and make predictions on data. Machine learning algorithms are used\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mhelp investors make more informed decisions about when to buy and sell stocks. One of the most popular machine learning algorithms for stock market prediction is the support vector machine (SVM). SVMs are used to classify data points into different categories, and they have been shown to be effective at predicting stock market trends. Another popular machine learning algorithm for stock market prediction is the neural network. Neural networks are a type of artificial intelligence that mimic the way the human brain works. They are able to learn from data and make predictions based on that data. There are many different machine learning algorithms that can be used for stock market prediction. The most important thing is to find an algorithm that works well with the data you have available. Machine learning is a branch of artificial intelligence that deals with the design and development of algorithms that can learn from and make predictions on data. Machine learning algorithms are used in a variety of applications, including stock market prediction. There are many different types of machine learning algorithms, but the most common ones used for stock market prediction are linear regression, support vector machines, and neural networks. Each of these algorithms has its own strengths and weaknesses, and the choice of which one to use depends on the specific problem that needs to be solved. Linear regression is a simple algorithm that can be used to predict stock prices. It works by fitting a line to the data and then using that line to make predictions. Linear regression is easy to understand and implement, but it is not very accurate. Support vector machines are more complex than linear regression, but they are more accurate. Support vector machines work by finding the line that best separates the data into two groups. This line is called the decision boundary, and it is used to make predictions. Support vector machines are often used in applications where there is a lot of noise in the data, such as stock market prediction. Neural networks are the most complex type of machine learning algorithm, but they are also the most accurate. Neural networks work by mimicking the way the human brain works. They are made up of many interconnected nodes, and each node is responsible for processing a small piece of the data. Neural networks are very good at pattern recognition, and they can be used to make predictions on complex data sets.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will become even more sophisticated and capable of performing a wider range of tasks.\n",
      "In conclusion, the future of NLP is bright and full of potential. As the technology continues to evolve, we can expect to see it being used in a wider range of applications, from healthcare to finance, and from education to entertainment. With the integration of machine learning and natural language processing, we can expect to see even more sophisticated and intelligent NLP systems that can understand and respond to user queries with unprecedented accuracy.\n",
      "Natural Language Processing (NLP) is a field of computer science that deals with the interaction between computers and human languages. It involves the development of algorithms and techniques that enable computers to understand, interpret, and generate human language. NLP has a wide range of applications, including speech recognition, machine translation, sentiment analysis, and text classification.\n",
      "NLP is a rapidly growing field, and there are many exciting developments in the field. Here are some of the most exciting developments in NLP:\n",
      "1. Deep Learning: Deep learning is a type of machine learning that involves the use of neural networks to learn complex patterns in data. Deep learning algorithms have been shown to be effective in a wide range of NLP tasks, including speech recognition, machine translation, and sentiment analysis.\n",
      "2. Natural Language Understanding: Natural language understanding is the ability of a computer to understand the meaning of a sentence or a paragraph. This involves the ability to understand the context, the relationships between words, and the intent of the speaker. NLP algorithms are being developed that can understand the meaning of a sentence or a paragraph with high accuracy.\n",
      "3. Machine Translation: Machine translation is the ability of a computer to translate text from one language to another. This involves the use of NLP algorithms to analyze the source text and generate a translation that is accurate and natural-sounding.\n",
      "4. Sentiment Analysis: Sentiment analysis is the ability of a computer to analyze the sentiment of a text. This involves the use of NLP algorithms to analyze the text and determine the emotional tone of the text.\n",
      "5. Text Classification: Text classification is the ability of a computer to classify a text into a specific category. This involves the use of NLP algorithms to analyze the text and determine the category to which it belongs.\n",
      "6. Chatbots: Chatbots are computer programs that can interact with humans in natural\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will become even more sophisticated and capable of performing a wider range of tasks. In conclusion, the future of NLP is bright and full of potential. As the technology continues to evolve, we can expect to see it being used in a wider range of applications, from healthcare to finance, and from education to entertainment. With the integration of machine learning and natural language processing, we can expect to see even more sophisticated and intelligent NLP systems that can understand and respond to user queries with unprecedented accuracy. Natural Language Processing (NLP) is a field of computer science that deals with the interaction between computers and human languages. It involves the development of algorithms and techniques that enable computers to understand, interpret, and generate human language. NLP has a wide range of applications, including speech recognition, machine translation, sentiment analysis, and text classification. NLP is a rapidly growing field, and there are many exciting developments in the field. Here are some of the most exciting developments in NLP:\n",
      "1. Deep Learning: Deep learning is a type of machine learning that involves the use of neural networks to learn complex patterns in data. Deep learning algorithms have been shown to be effective in a wide range of NLP tasks, including speech recognition, machine translation, and sentiment analysis. 2. Natural Language Understanding: Natural language understanding is the ability of a computer to understand the meaning of a sentence or a paragraph. This involves the ability to understand the context, the relationships between words, and the intent of the speaker. NLP algorithms are being developed that can understand the meaning of a sentence or a paragraph with high accuracy. 3. Machine Translation: Machine translation is the ability of a computer to translate text from one language to another. This involves the use of NLP algorithms to analyze the source text and generate a translation that is accurate and natural-sounding. 4. Sentiment Analysis: Sentiment analysis is the ability of a computer to analyze the sentiment of a text. This involves the use of NLP algorithms to analyze the text and determine the emotional tone of the text. 5. Text Classification: Text classification is the ability of a computer to classify a text into a specific category. This involves the use of NLP algorithms to analyze the text and determine the category to which it belongs. 6.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m the road. The sensors are used to detect obstacles and other vehicles, while the algorithms are used to make decisions about how to avoid them.\n",
      "The sensors used in self-driving cars include cameras, radar, and lidar. Cameras are used to detect objects in the car’s field of view, while radar and lidar are used to detect objects in the car’s surroundings.\n",
      "The algorithms used in self-driving cars are complex and constantly evolving. They are used to make decisions about how to avoid obstacles, such as other vehicles, pedestrians, and cyclists.\n",
      "The algorithms are also used to make decisions about how to navigate the road, such as when to change lanes or when to turn.\n",
      "Self-driving cars rely heavily on sensor data and advanced algorithms to navigate the road. The sensors are used to detect obstacles and other vehicles, while the algorithms are used to make decisions about how to avoid them.\n",
      "The sensors used in self-driving cars include cameras, radar, and lidar. The algorithms used in self-driving cars are complex and constantly evolving. They are used to make decisions about how to avoid obstacles, such as other vehicles, pedestrians, and cyclists.\n",
      "The sensors used in self-driving cars include cameras, radar, and lidar.\n",
      "The algorithms used in self-driving cars are complex and constantly evolving.\n",
      "The sensors used in self-driving cars include cameras, radar, and lidar. The algorithms used in self-driving cars are complex and constantly evolving. They are used to make decisions about how to avoid obstacles, such as other vehicles, pedestrians, and cyclists.\n",
      "The sensors used in self-driving cars include cameras, radar, and lidar. The algorithms used in self-driving cars are complex and constantly evolving. They are used to make decisions about how to avoid obstacles, such as other vehicles, pedestrians, and cyclists. The sensors used in self-driving cars include cameras, radar, and lidar. The algorithms used in self-driving cars are complex and constantly evolving. They are used to make decisions about how to avoid obstacles, such\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91mthe road. The sensors are used to detect obstacles and other vehicles, while the algorithms are used to make decisions about how to avoid them. The sensors used in self-driving cars include cameras, radar, and lidar. Cameras are used to detect objects in the car’s field of view, while radar and lidar are used to detect objects in the car’s surroundings. The algorithms used in self-driving cars are complex and constantly evolving. They are used to make decisions about how to avoid obstacles, such as other vehicles, pedestrians, and cyclists. The algorithms are also used to make decisions about how to navigate the road, such as when to change lanes or when to turn. Self-driving cars rely heavily on sensor data and advanced algorithms to navigate the road.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m help ensure AI systems are fair and equitable.\n",
      "AI systems can be used to improve access to healthcare. For example, AI systems can be used to diagnose diseases, predict patient outcomes, and recommend treatment plans. AI systems can also be used to improve access to healthcare by providing personalized healthcare recommendations and reminders.\n",
      "AI systems can be used to improve access to education. For example, AI systems can be used to provide personalized learning recommendations, identify students who need additional support, and provide feedback on student performance. AI systems can also be used to improve access to education by providing personalized learning recommendations and reminders.\n",
      "AI systems can be used to improve access to transportation. For example, AI systems can be used to provide personalized transportation recommendations, identify traffic patterns, and provide real-time traffic updates. AI systems can also be used to improve access to transportation by providing personalized transportation recommendations and reminders.\n",
      "AI systems can be used to improve access to housing. For example, AI systems can be used to provide personalized housing recommendations, identify housing options, and provide real-time housing updates. AI systems can also be used to improve access to housing by providing personalized housing recommendations and reminders.\n",
      "AI systems can be used to improve access to employment. For example, AI systems can be used to provide personalized job recommendations, identify job opportunities, and provide real-time job updates. AI systems can also be used to improve access to employment by providing personalized job recommendations and reminders.\n",
      "AI systems can be used to improve access to food. For example, AI systems can be used to provide personalized food recommendations, identify food options, and provide real-time food updates. AI systems can also be used to improve access to food by providing personalized food recommendations and reminders.\n",
      "AI systems can be used to improve access to water. For example, AI systems can be used to provide personalized water recommendations, identify water options, and provide real-time water updates. AI systems can also be used to improve access to water by providing personalized water recommendations and reminders.\n",
      "AI systems can be used to improve access to energy. For example, AI systems can be used to provide personalized energy recommendations, identify energy options, and provide real-time\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mhelp ensure AI systems are fair and equitable. AI systems can be used to improve access to healthcare. For example, AI systems can be used to diagnose diseases, predict patient outcomes, and recommend treatment plans. AI systems can also be used to improve access to healthcare by providing personalized healthcare recommendations and reminders. AI systems can be used to improve access to education. For example, AI systems can be used to provide personalized learning recommendations, identify students who need additional support, and provide feedback on student performance. AI systems can also be used to improve access to education by providing personalized learning recommendations and reminders. AI systems can be used to improve access to transportation. For example, AI systems can be used to provide personalized transportation recommendations, identify traffic patterns, and provide real-time traffic updates. AI systems can also be used to improve access to transportation by providing personalized transportation recommendations and reminders. AI systems can be used to improve access to housing. For example, AI systems can be used to provide personalized housing recommendations, identify housing options, and provide real-time housing updates. AI systems can also be used to improve access to housing by providing personalized housing recommendations and reminders. AI systems can be used to improve access to employment. For example, AI systems can be used to provide personalized job recommendations, identify job opportunities, and provide real-time job updates. AI systems can also be used to improve access to employment by providing personalized job recommendations and reminders. AI systems can be used to improve access to food. For example, AI systems can be used to provide personalized food recommendations, identify food options, and provide real-time food updates. AI systems can also be used to improve access to food by providing personalized food recommendations and reminders. AI systems can be used to improve access to water. For example, AI systems can be used to provide personalized water recommendations, identify water options, and provide real-time water updates. AI systems can also be used to improve access to water by providing personalized water recommendations and reminders. AI systems can be used to improve access to energy.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can perform more complex tasks, such as autonomous driving and natural language processing.\n",
      "In conclusion, the future of AI is bright, and the possibilities are endless. As we continue to develop new technologies and algorithms, we will be able to create even more powerful and intelligent systems that can help us solve some of the world’s most pressing problems. Whether it’s in the field of healthcare, transportation, or education, AI has the potential to transform our lives in ways we never thought possible.\n",
      "Previous articleThe Future of AI in Healthcare: How Artificial Intelligence is Revolutionizing the Healthcare Industry\n",
      "Next articleThe Future of AI in Education: How Artificial Intelligence is Revolutionizing the Education Industry\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan perform more complex tasks, such as autonomous driving and natural language processing. In conclusion, the future of AI is bright, and the possibilities are endless. As we continue to develop new technologies and algorithms, we will be able to create even more powerful and intelligent systems that can help us solve some of the world’s most pressing problems. Whether it’s in the field of healthcare, transportation, or education, AI has the potential to transform our lives in ways we never thought possible.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m trial and error.\n",
      "The most common reinforcement learning algorithms are:\n",
      "Q-learning: This algorithm uses a table to store the values of the Q-function for each state-action pair. The algorithm then updates the values of the Q-function based on the current state and action.\n",
      "Policy gradient: This algorithm uses a gradient descent algorithm to update the policy parameters. The policy gradient is the gradient of the expected reward with respect to the policy parameters.\n",
      "Deep Q-network: This algorithm uses a deep neural network to learn the Q-function. The network is trained using a reinforcement learning algorithm such as Q-learning or policy gradient.\n",
      "Deep reinforcement learning algorithms learn optimal behaviors through trial and error.\n",
      "The most common deep reinforcement learning algorithms are:\n",
      "Deep Q-network: This algorithm uses a deep neural network to learn the Q-function. The network is trained using a reinforcement learning algorithm such as Q-learning or policy gradient.\n",
      "Deep policy gradient: This algorithm uses a deep neural network to learn the policy parameters. The network is trained using a policy gradient algorithm.\n",
      "Deep actor-critic: This algorithm uses a deep neural network to learn both the policy parameters and the Q-function. The network is trained using a reinforcement learning algorithm such as Q-learning or policy gradient.\n",
      "Deep reinforcement learning algorithms are more complex than supervised learning algorithms, but they can learn more complex behaviors.\n",
      "Deep reinforcement learning algorithms are more complex than supervised learning algorithms, but they can learn more complex behaviors. The most common deep reinforcement learning algorithms are:\n",
      "Deep Q-network: This algorithm uses a deep neural network to learn the Q-function. The network is trained using a reinforcement learning algorithm such as Q-learning or policy gradient. Deep Q-network is a popular algorithm for learning complex behaviors.\n",
      "Deep policy gradient: This algorithm uses a deep neural network to learn the policy parameters. The network is trained using a policy gradient algorithm. Deep policy gradient is a popular algorithm for learning complex behaviors.\n",
      "Deep actor-critic: This algorithm uses a deep neural network to learn both the policy parameters and the Q-function. The network is trained using a reinforcement learning algorithm such as Q-learning or policy gradient. Deep actor-critic is a popular algorithm for learning complex behaviors.\n",
      "Deep reinforcement learning algorithms are\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91mtrial and error. The most common reinforcement learning algorithms are:\n",
      "Q-learning: This algorithm uses a table to store the values of the Q-function for each state-action pair. The algorithm then updates the values of the Q-function based on the current state and action. Policy gradient: This algorithm uses a gradient descent algorithm to update the policy parameters. The policy gradient is the gradient of the expected reward with respect to the policy parameters. Deep Q-network: This algorithm uses a deep neural network to learn the Q-function. The network is trained using a reinforcement learning algorithm such as Q-learning or policy gradient. Deep reinforcement learning algorithms learn optimal behaviors through trial and error. The most common deep reinforcement learning algorithms are:\n",
      "Deep Q-network: This algorithm uses a deep neural network to learn the Q-function. Deep policy gradient: This algorithm uses a deep neural network to learn the policy parameters. The network is trained using a policy gradient algorithm. Deep actor-critic: This algorithm uses a deep neural network to learn both the policy parameters and the Q-function. Deep reinforcement learning algorithms are more complex than supervised learning algorithms, but they can learn more complex behaviors. Deep Q-network is a popular algorithm for learning complex behaviors. Deep policy gradient is a popular algorithm for learning complex behaviors. Deep actor-critic is a popular algorithm for learning complex behaviors.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m potentially break many of the cryptographic algorithms currently in use.\n",
      "However, there are also potential benefits to quantum computing for cryptography. For example, quantum algorithms could be used to create more secure cryptographic algorithms that are resistant to quantum attacks. Additionally, quantum computing could be used to create more efficient cryptographic protocols that require less computation and energy.\n",
      "Overall, the impact of quantum computing on cryptography is still uncertain. However, it is clear that quantum computing has the potential to revolutionize the field of cryptography and could have a significant impact on the security of our digital communications.\n",
      "The Impact of Quantum Computing on Cryptography\n",
      "Quantum computing is a rapidly developing field of computer science that has the potential to revolutionize the way we think about computing. One of the most exciting applications of quantum computing is its potential to revolutionize cryptography.\n",
      "Cryptography is the practice of securing communications and data by encrypting them so that only authorized parties can read them. It is a critical component of modern security systems, and it is essential for protecting sensitive information from unauthorized access.\n",
      "Quantum computing has the potential to revolutionize cryptography in several ways. First, it can enable the development of more secure cryptographic algorithms. Quantum computers are capable of performing complex calculations much faster than classical computers, and this speed advantage can be used to create more secure cryptographic algorithms.\n",
      "Second, quantum computing can enable the development of more efficient cryptographic protocols. Quantum computers are capable of performing complex calculations much faster than classical computers, and this speed advantage can be used to create more efficient cryptographic protocols.\n",
      "Third, quantum computing can enable the development of more secure cryptographic systems. Quantum computers are capable of performing complex calculations much faster than classical computers, and this speed advantage can be used to create more secure cryptographic systems.\n",
      "Overall, quantum computing has the potential to revolutionize cryptography in several ways. It can enable the development of more secure cryptographic algorithms, more efficient cryptographic protocols, and more secure cryptographic systems. As quantum computing continues to develop, it is likely that it will have a significant impact on the field of cryptography.\n",
      "The Impact of Quantum Computing on Cryptography: A Comprehensive Overview\n",
      "Quantum computing is a rapidly developing field of computer science that has the potential to revolutionize the way we think about computing. One of the most exciting applications\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mpotentially break many of the cryptographic algorithms currently in use. However, there are also potential benefits to quantum computing for cryptography. For example, quantum algorithms could be used to create more secure cryptographic algorithms that are resistant to quantum attacks. Additionally, quantum computing could be used to create more efficient cryptographic protocols that require less computation and energy. Overall, the impact of quantum computing on cryptography is still uncertain. However, it is clear that quantum computing has the potential to revolutionize the field of cryptography and could have a significant impact on the security of our digital communications. The Impact of Quantum Computing on Cryptography\n",
      "Quantum computing is a rapidly developing field of computer science that has the potential to revolutionize the way we think about computing. One of the most exciting applications of quantum computing is its potential to revolutionize cryptography. Cryptography is the practice of securing communications and data by encrypting them so that only authorized parties can read them. It is a critical component of modern security systems, and it is essential for protecting sensitive information from unauthorized access. Quantum computing has the potential to revolutionize cryptography in several ways. First, it can enable the development of more secure cryptographic algorithms. Quantum computers are capable of performing complex calculations much faster than classical computers, and this speed advantage can be used to create more secure cryptographic algorithms. Second, quantum computing can enable the development of more efficient cryptographic protocols. Quantum computers are capable of performing complex calculations much faster than classical computers, and this speed advantage can be used to create more efficient cryptographic protocols. Third, quantum computing can enable the development of more secure cryptographic systems. Quantum computers are capable of performing complex calculations much faster than classical computers, and this speed advantage can be used to create more secure cryptographic systems. Overall, quantum computing has the potential to revolutionize cryptography in several ways. It can enable the development of more secure cryptographic algorithms, more efficient cryptographic protocols, and more secure cryptographic systems. As quantum computing continues to develop, it is likely that it will have a significant impact on the field of cryptography. The Impact of Quantum Computing on Cryptography: A Comprehensive Overview\n",
      "Quantum computing is a rapidly developing field of computer science that has the potential to revolutionize the way we think about computing.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m can handle even larger and more complex datasets.\n",
      "The future of AI is bright, and the potential for its applications is vast. As we continue to develop new techniques and technologies, we can expect to see AI become even more integrated into our daily lives, transforming the way we work, communicate, and interact with the world around us.\n",
      "Previous articleThe Future of AI in Healthcare: How AI is Revolutionizing the Healthcare Industry\n",
      "Next articleThe Future of AI in Education: How AI is Transforming the Way We Learn\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mcan handle even larger and more complex datasets. The future of AI is bright, and the potential for its applications is vast. As we continue to develop new techniques and technologies, we can expect to see AI become even more integrated into our daily lives, transforming the way we work, communicate, and interact with the world around us.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m only training on a small dataset.\n",
      "\n",
      "\\subsection{Transfer Learning for Image Classification}\n",
      "\n",
      "Transfer learning has been widely used in image classification tasks.\n",
      "In this section, we review the most popular transfer learning methods for image classification.\n",
      "\n",
      "\\subsubsection{Feature Extraction}\n",
      "\n",
      "Feature extraction is the first step in transfer learning.\n",
      "In this step, the features of the source dataset are extracted and transferred to the target dataset.\n",
      "There are two main methods for feature extraction:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Feature Selection}: Feature selection is a method that selects the most important features from the source dataset.\n",
      "\\item \\textbf{Feature Extraction}: Feature extraction is a method that extracts the features from the source dataset.\n",
      "\\end{itemize}\n",
      "\n",
      "\\textbf{Feature Selection}: Feature selection is a method that selects the most important features from the source dataset.\n",
      "This method is used to reduce the dimensionality of the source dataset and to improve the performance of the target dataset.\n",
      "There are two main methods for feature selection:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Manual Feature Selection}: Manual feature selection is a method that selects the most important features by hand.\n",
      "\\item \\textbf{Automatic Feature Selection}: Automatic feature selection is a method that selects the most important features automatically.\n",
      "\\end{itemize}\n",
      "\n",
      "\\textbf{Feature Extraction}: Feature extraction is a method that extracts the features from the source dataset.\n",
      "This method is used to improve the performance of the target dataset.\n",
      "There are two main methods for feature extraction:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Manual Feature Extraction}: Manual feature extraction is a method that extracts the features by hand.\n",
      "\\item \\textbf{Automatic Feature Extraction}: Automatic feature extraction is a method that extracts the features automatically.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsubsection{Feature Aggregation}\n",
      "\n",
      "Feature aggregation is the second step in transfer learning.\n",
      "In this step, the features of the source dataset are aggregated and transferred to the target dataset.\n",
      "There are two main methods for feature aggregation:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Feature Aggregation}: Feature aggregation is a method that aggregates the features from the source dataset.\n",
      "\\item \\textbf{Feature Fusion\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91monly training on a small dataset. \\subsection{Transfer Learning for Image Classification}\n",
      "\n",
      "Transfer learning has been widely used in image classification tasks. In this section, we review the most popular transfer learning methods for image classification. \\subsubsection{Feature Extraction}\n",
      "\n",
      "Feature extraction is the first step in transfer learning. In this step, the features of the source dataset are extracted and transferred to the target dataset. There are two main methods for feature extraction:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Feature Selection}: Feature selection is a method that selects the most important features from the source dataset. \\item \\textbf{Feature Extraction}: Feature extraction is a method that extracts the features from the source dataset. \\end{itemize}\n",
      "\n",
      "\\textbf{Feature Selection}: Feature selection is a method that selects the most important features from the source dataset. This method is used to reduce the dimensionality of the source dataset and to improve the performance of the target dataset. There are two main methods for feature selection:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Manual Feature Selection}: Manual feature selection is a method that selects the most important features by hand. \\item \\textbf{Automatic Feature Selection}: Automatic feature selection is a method that selects the most important features automatically. \\end{itemize}\n",
      "\n",
      "\\textbf{Feature Extraction}: Feature extraction is a method that extracts the features from the source dataset. This method is used to improve the performance of the target dataset. There are two main methods for feature extraction:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Manual Feature Extraction}: Manual feature extraction is a method that extracts the features by hand. \\item \\textbf{Automatic Feature Extraction}: Automatic feature extraction is a method that extracts the features automatically. \\end{itemize}\n",
      "\n",
      "\\subsubsection{Feature Aggregation}\n",
      "\n",
      "Feature aggregation is the second step in transfer learning. In this step, the features of the source dataset are aggregated and transferred to the target dataset. There are two main methods for feature aggregation:\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Feature Aggregation}: Feature aggregation is a method that aggregates the features from the source dataset.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m increase trust and transparency in AI systems.\n",
      "One of the key challenges in XAI is the need to balance explainability with model performance. Explaining a model’s decision-making process can be computationally expensive, which can negatively impact model performance. Additionally, the explanations provided by XAI methods may not always be accurate or complete, which can lead to misinterpretations of the model’s behavior.\n",
      "To address these challenges, researchers are exploring new approaches to XAI that balance explainability and performance. One approach is to use interpretable machine learning models, which provide explanations that are more accurate and complete. Another approach is to use explainable deep learning models, which provide explanations that are more interpretable and explainable.\n",
      "In addition to improving the accuracy and completeness of explanations, researchers are also exploring new ways to visualize and present explanations to users. For example, researchers are developing visualization techniques that allow users to see how different features and variables influence the model’s decision-making process.\n",
      "Overall, the future of XAI is promising, with new approaches being developed to balance explainability and performance while providing more accurate and complete explanations. As AI systems become increasingly important in critical areas, the need for XAI will continue to grow, and researchers will continue to explore new ways to improve the accuracy and completeness of explanations.\n",
      "In conclusion, XAI is a rapidly growing field that is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to increase trust and transparency in AI systems.\n",
      "However, there are still challenges to be addressed, such as the need to balance explainability with model performance and the need to improve the accuracy and completeness of explanations.\n",
      "Researchers are exploring new approaches to XAI that balance explainability and performance, such as using interpretable machine learning models and explainable deep learning models.\n",
      "They are also exploring new ways to visualize and present explanations to users, such as developing visualization techniques that allow users to see how different features and variables influence the model’s decision-making process.\n",
      "As AI systems become increasingly important in critical areas, the need for XAI will continue to grow, and researchers will continue to explore new ways to improve the accuracy and completeness of explan\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mincrease trust and transparency in AI systems. One of the key challenges in XAI is the need to balance explainability with model performance. Explaining a model’s decision-making process can be computationally expensive, which can negatively impact model performance. Additionally, the explanations provided by XAI methods may not always be accurate or complete, which can lead to misinterpretations of the model’s behavior. To address these challenges, researchers are exploring new approaches to XAI that balance explainability and performance. One approach is to use interpretable machine learning models, which provide explanations that are more accurate and complete. Another approach is to use explainable deep learning models, which provide explanations that are more interpretable and explainable. In addition to improving the accuracy and completeness of explanations, researchers are also exploring new ways to visualize and present explanations to users. For example, researchers are developing visualization techniques that allow users to see how different features and variables influence the model’s decision-making process. Overall, the future of XAI is promising, with new approaches being developed to balance explainability and performance while providing more accurate and complete explanations. As AI systems become increasingly important in critical areas, the need for XAI will continue to grow, and researchers will continue to explore new ways to improve the accuracy and completeness of explanations. In conclusion, XAI is a rapidly growing field that is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to increase trust and transparency in AI systems. However, there are still challenges to be addressed, such as the need to balance explainability with model performance and the need to improve the accuracy and completeness of explanations. Researchers are exploring new approaches to XAI that balance explainability and performance, such as using interpretable machine learning models and explainable deep learning models. They are also exploring new ways to visualize and present explanations to users, such as developing visualization techniques that allow users to see how different features and variables influence the model’s decision-making process.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun disappears into the horizon.\n",
      "The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink.\n",
      "The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink.\n",
      "The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink. The sunset is a time of day when the sun is setting, and\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun disappears into the horizon. The sunset is a time of day when the sun is setting, and the sky is turning shades of orange and pink.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "Traveling can also help you develop a sense of independence and self-reliance. When you travel alone, you are responsible for your own safety, transportation, and accommodation. This can help you build confidence and resilience, which can be beneficial in both personal and professional life.\n",
      "Traveling can also help you develop a sense of adventure and curiosity. When you travel to new places, you are constantly exposed to new experiences and challenges. This can help you develop a sense of adventure and curiosity, which can be beneficial in both personal and professional life.\n",
      "Traveling can also help you develop a sense of empathy and understanding. When you travel to new countries, you are exposed to different cultures and traditions. This can help you develop a deeper understanding of the world and the people who live in it.\n",
      "Traveling can also help you develop a sense of self-awareness. When you travel alone, you are forced to confront your own fears, anxieties, and insecurities. This can help you develop a deeper understanding of yourself and your own strengths and weaknesses.\n",
      "Traveling can also help you develop a sense of responsibility. When you travel alone, you are responsible for your own safety, transportation, and accommodation. This can help you develop a sense of responsibility and accountability, which can be beneficial in both personal and professional life.\n",
      "Traveling can also help you develop a sense of independence and self-reliance. When you travel alone, you are responsible for your own safety, transportation, and accommodation. This can help you develop a sense of independence and self-reliance, which can be beneficial in both personal and professional life.\n",
      "Traveling can also help you develop a sense of adventure and curiosity. When you travel to new places, you are constantly exposed to new experiences and challenges. This can help you develop a sense of adventure and curiosity, which can be beneficial in both personal and professional life.\n",
      "Traveling can also help you develop a sense of empathy and understanding. When you travel to new countries, you are exposed to different cultures and traditions. This can help you develop a deeper understanding of the world and the people who live in it.\n",
      "Traveling can also help you develop a sense of self-awareness. When you travel alone, you are forced to confront your own\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. Traveling can also help you develop a sense of independence and self-reliance. When you travel alone, you are responsible for your own safety, transportation, and accommodation. This can help you build confidence and resilience, which can be beneficial in both personal and professional life. Traveling can also help you develop a sense of adventure and curiosity. When you travel to new places, you are constantly exposed to new experiences and challenges. This can help you develop a sense of adventure and curiosity, which can be beneficial in both personal and professional life. Traveling can also help you develop a sense of empathy and understanding. When you travel to new countries, you are exposed to different cultures and traditions. This can help you develop a deeper understanding of the world and the people who live in it. Traveling can also help you develop a sense of self-awareness. When you travel alone, you are forced to confront your own fears, anxieties, and insecurities. This can help you develop a deeper understanding of yourself and your own strengths and weaknesses. Traveling can also help you develop a sense of responsibility. This can help you develop a sense of responsibility and accountability, which can be beneficial in both personal and professional life. This can help you develop a sense of independence and self-reliance, which can be beneficial in both personal and professional life.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp in inputs_base: \n",
    "    results = gen_text([inp], temperature=0.3, use_template=use_template, batch_size=8)\n",
    "\n",
    "    print(\"***\" * 50)\n",
    "    print(\"Original: \")\n",
    "    print(\"Prompt:  \", end=\"\")\n",
    "    print(\"\\033[1m {} \\033[0m\".format(inp), end=\"\")  # \\033[1m and \\033[0m are the ANSI escape sequences for start and end of bold respectively\n",
    "    print(\"\\033[91m{}\\033[0m\".format(results[0]))\n",
    "    \n",
    "    print(\"---\" * 50)\n",
    "    print(\"Cleaned: \")\n",
    "    print(\"Prompt:  \", end=\"\")\n",
    "    print(\"\\033[1m {} \\033[0m\".format(inp), end=\"\")  # \\033[1m and \\033[0m are the ANSI escape sequences for start and end of bold respectively\n",
    "    print(\"\\033[91m{}\\033[0m\".format(clean_output(results[0])))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58efb9c",
   "metadata": {},
   "source": [
    "### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a53770f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:56:36.731576Z",
     "start_time": "2024-05-18T19:55:37.272341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 17.264213224155824 tokens/sec, 516 tokens (including full prompt)\n",
      "Long Context: 43.60678792669122 tokens/sec, 1288 tokens (including full prompt)\n"
     ]
    }
   ],
   "source": [
    "throughput, n_tokens, result = get_gen_text_throughput(\"What is ML?\", use_template=use_template)\n",
    "\n",
    "print(f\"Short Context: {throughput} tokens/sec, {n_tokens} tokens (including full prompt)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# When the context is long or the generated text is long, it takes longer to generate each token in average\n",
    "throughput, n_tokens, result = get_gen_text_throughput(long_input, max_new_tokens=512, use_template=use_template)\n",
    "\n",
    "print(f\"Long Context: {throughput} tokens/sec, {n_tokens} tokens (including full prompt)\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "963ebe89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:08:52.625365Z",
     "start_time": "2024-05-18T20:02:46.992128Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  25.719833135281185 tokens/sec, 558 tokens (including full prompt)\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "data = inputs_base\n",
    "for inp in inputs_base:\n",
    "    throughput, n_tokens, result = get_gen_text_throughput(inp, max_new_tokens=512, use_template=use_template)\n",
    "    t.append(throughput)\n",
    "print(f\"Average:  {sum(t)/len(t)} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be7e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T19:29:03.722188Z",
     "start_time": "2024-05-17T19:29:03.719243Z"
    }
   },
   "source": [
    "### Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba0ef442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:09:29.149218Z",
     "start_time": "2024-05-18T20:09:29.142243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.51458816\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/10e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df96994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca74f60a",
   "metadata": {},
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d43a1",
   "metadata": {},
   "source": [
    "# Given pruning dict, prune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Pruning dictionary layer 5 to 28\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dcfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b646e5d9",
   "metadata": {},
   "source": [
    "## Prune the model using masks and save checkpoints in save_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd8b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df51ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec5cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7938385d",
   "metadata": {},
   "source": [
    "# Pruned Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3e154",
   "metadata": {},
   "source": [
    "## Find performance zero shot + some prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b58590e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T10:47:08.887532Z",
     "start_time": "2024-05-18T10:47:08.884532Z"
    }
   },
   "source": [
    "### PPL on wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_test = eval_ppl(args, pipeline.model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b2840",
   "metadata": {},
   "source": [
    "### Zero Shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8aaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = [\"boolq\", \"rte\",\"hellaswag\",\"winogrande\", \"arc_easy\",\"arc_challenge\", \"openbookqa\"]\n",
    "num_shot = 0\n",
    "results = eval_zero_shot(args.model, orig_model, tokenizer, save_dir, task_list, num_shot, accelerate)\n",
    "print(\"\\n\\n\")\n",
    "print(\"*****\"*30)\n",
    "print(\"zero_shot evaluation results\")\n",
    "print(results['results'])\n",
    "print(\"\\n\\n\")\n",
    "print(\"*****\"*30)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3440088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d14af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "908af2c7",
   "metadata": {},
   "source": [
    "## See how is generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084702ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use args such as temperature and max_new_tokens to control text generation\n",
    "results = gen_text([\"What is a large language model?\"], temperature=0.5, max_new_tokens=100, use_template=True)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd5294",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gen_text(inputs, use_template=True, batch_size=8)\n",
    "\n",
    "for i, output in enumerate(results):\n",
    "    print(\"**\" * 30)\n",
    "    print(\"Prompt: {}\".format(inputs[i]))\n",
    "    print(\"-\" * 30)\n",
    "    print(output)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"**\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput, n_tokens, result = get_gen_text_throughput(\"What is ML?\", use_template=False)\n",
    "\n",
    "print(f\"{throughput} tokens/sec, {n_tokens} tokens (including full prompt)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# When the context is long or the generated text is long, it takes longer to generate each token in average\n",
    "throughput, n_tokens, result = get_gen_text_throughput(long_input, max_new_tokens=200, use_template=True)\n",
    "\n",
    "print(f\"{throughput} tokens/sec, {n_tokens} tokens (including full prompt)\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec161687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T19:29:03.722188Z",
     "start_time": "2024-05-17T19:29:03.719243Z"
    }
   },
   "source": [
    "### Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebea719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()/10e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea607a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef4503d",
   "metadata": {},
   "source": [
    "### Throughput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04095e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "\n",
    "for i in range(10):\n",
    "    throughput, n_tokens, result = get_gen_text_throughput(inputs, max_new_tokens=200, use_template=True)\n",
    "    t.append(throughput)\n",
    "print(f\"{sum(t)/len(t)} tokens/sec, {n_tokens} tokens (including full prompt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85eee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fdf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0b84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4194fa4",
   "metadata": {},
   "source": [
    "# Create real pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3488ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423472c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07f70a54",
   "metadata": {},
   "source": [
    "## Remove checkpoint saved in save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50078b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515a088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17e01cd",
   "metadata": {},
   "source": [
    "# Finetune on Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64c8666b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:32:08.927706Z",
     "start_time": "2024-05-18T20:32:08.906940Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b93a6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:33:41.366009Z",
     "start_time": "2024-05-18T20:33:41.358232Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"../templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7212c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:33:42.053169Z",
     "start_time": "2024-05-18T20:33:42.027344Z"
    },
    "code_folding": [
     0,
     31
    ]
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = hf_llama_path,  # the only required argument\n",
    "    data_path: str = alpacha_dataset_path,\n",
    "    output_dir: str = \"./logs/lora-alpaca\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    val_set_size: int = 0.1,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"val_set_size: {val_set_size}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"add_eos_token: {add_eos_token}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"wandb_project: {wandb_project}\\n\"\n",
    "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "            f\"wandb_watch: {wandb_watch}\\n\"\n",
    "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    prompter = Prompter(prompt_template_name)\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\n",
    "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    "    )\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "    tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=add_eos_token\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        print(\"If\")\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "        print(data)\n",
    "    else:\n",
    "        data = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "        print(\"Else\")\n",
    "        print(data)\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(\n",
    "            test_size=val_set_size, shuffle=True, seed=42\n",
    "        )\n",
    "        train_data = (\n",
    "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "        val_data = (\n",
    "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else None,\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d43fef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T20:33:46.134128Z",
     "start_time": "2024-05-18T20:33:42.564447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: /data/home/milinbhade/LLaMa/hf_llama/\n",
      "data_path: /data/home/milinbhade/LLaMa/pretrain_data/alpaca/alpaca-cleaned-dataset\n",
      "output_dir: ./logs/lora-alpaca\n",
      "batch_size: 128\n",
      "micro_batch_size: 4\n",
      "num_epochs: 3\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "val_set_size: 0.1\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19:02:03:43,918 INFO     [modeling.py:940] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfire\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/fire/core.py:143\u001b[0m, in \u001b[0;36mFire\u001b[0;34m(component, command, name, serialize)\u001b[0m\n\u001b[1;32m    140\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_globals)\n\u001b[1;32m    141\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_locals)\n\u001b[0;32m--> 143\u001b[0m component_trace \u001b[38;5;241m=\u001b[39m \u001b[43m_Fire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_flag_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_trace\u001b[38;5;241m.\u001b[39mHasError():\n\u001b[1;32m    146\u001b[0m   _DisplayError(component_trace)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/fire/core.py:477\u001b[0m, in \u001b[0;36m_Fire\u001b[0;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[1;32m    474\u001b[0m is_class \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39misclass(component)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m   component, remaining_args \u001b[38;5;241m=\u001b[39m \u001b[43m_CallAndUpdateTrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m      \u001b[49m\u001b[43mremaining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroutine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m   handled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FireError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/fire/core.py:693\u001b[0m, in \u001b[0;36m_CallAndUpdateTrace\u001b[0;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[1;32m    691\u001b[0m   component \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(fn(\u001b[38;5;241m*\u001b[39mvarargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m   component \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvarargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m treatment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    696\u001b[0m   action \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mINSTANTIATED_CLASS\n",
      "Cell \u001b[0;32mIn[53], line 85\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(base_model, data_path, output_dir, batch_size, micro_batch_size, num_epochs, learning_rate, cutoff_len, val_set_size, lora_r, lora_alpha, lora_dropout, lora_target_modules, train_on_inputs, add_eos_token, group_by_length, wandb_project, wandb_run_name, wandb_watch, wandb_log_model, resume_from_checkpoint, prompt_template_name)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(wandb_log_model) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     83\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_LOG_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m wandb_log_model\n\u001b[0;32m---> 85\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model)\n\u001b[1;32m     94\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# unk. we want this to be different from the eos token\u001b[39;00m\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3455\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:86\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m            Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m            quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m            in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m            `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m            https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m            for more details.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m            \"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.37.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    "
     ]
    }
   ],
   "source": [
    "fire.Fire(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b8e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f6910fe",
   "metadata": {},
   "source": [
    "# Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13359b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:52.889462Z",
     "start_time": "2024-06-12T06:20:40.240787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 11:50:40.915310: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-12 11:50:40.961160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 11:50:41.904547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e348536fad346119f1fdb9324a0b271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35897a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:52.928743Z",
     "start_time": "2024-06-12T06:20:52.897388Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_output(output_text):\n",
    "    # Remove repeated sentences\n",
    "    unique_sentences = set()\n",
    "    cleaned_output = []\n",
    "    \n",
    "    for sentence in output_text.split('.'):\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence not in unique_sentences:\n",
    "            cleaned_output.append(sentence)\n",
    "            unique_sentences.add(sentence)\n",
    "    \n",
    "    # Remove garbage symbols\n",
    "#     cleaned_output = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in cleaned_output]\n",
    "    \n",
    "    # Exclude the last sentence if it does not end with a period\n",
    "    if output_text[-1] != '.':\n",
    "        cleaned_output = cleaned_output[:-1]\n",
    "    \n",
    "    return \". \".join(cleaned_output) + \".\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaMLP, LlamaConfig\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=None, output_dir=None, ignored_layers=None, \n",
    "                 use_template=False):\n",
    "        self.pipeline = pipeline\n",
    "        self.model = self.pipeline.model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pruning_dict = pruning_dict\n",
    "        self.save_dir = save_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.ignored_layers = ignored_layers\n",
    "        \n",
    "        self.use_template = use_template\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "    \n",
    "    def get_head_inter_pruning_dict(self):\n",
    "        ## load pruning dict\n",
    "        if isinstance(self.pruning_dict,str):\n",
    "            pruning_dict_path = \"../pruning_dicts/\"  # replace with your directory\n",
    "            file_path = os.path.join(pruning_dict_path, self.pruning_dict)\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            self.pruning_dict = data\n",
    "        else:\n",
    "            if isinstance(self.pruning_dict, dict):\n",
    "                self.pruning_dict = pruning_dict\n",
    "            else:\n",
    "                raise ValueError(\"Pruning dict not valid\")\n",
    "        for i, ind in self.pruning_dict.items():\n",
    "            print(i, \"==> \",len(ind))\n",
    "    def prune_model(self, real):\n",
    "        modules = list(self.model.modules())\n",
    "        for idx, index in self.pruning_dict.items():\n",
    "            idx = int(idx)\n",
    "#             print(\"Pruning idx: {}, Type: {}\".format(idx, type(modules[idx])))\n",
    "            op = modules[idx]\n",
    "            if isinstance(op, LlamaAttention):\n",
    "                if real:\n",
    "                    op.q_proj = prune_linear_layer(op.q_proj, torch.LongTensor(index)).half()\n",
    "                    op.k_proj = prune_linear_layer(op.k_proj, torch.LongTensor(index)).half()\n",
    "                    op.v_proj = prune_linear_layer(op.v_proj, torch.LongTensor(index)).half()\n",
    "                    op.o_proj = prune_linear_layer(op.o_proj, torch.LongTensor(index), dim=1).half()\n",
    "                    \n",
    "                    op.num_heads = len(index)//op.head_dim\n",
    "                    op.num_key_value_heads = op.num_heads\n",
    "                    op.hidden_size = len(index)\n",
    "                else:\n",
    "                    op.q_proj = prune_linear_by_mask(op.q_proj, torch.LongTensor(index)).half()\n",
    "                    op.k_proj = prune_linear_by_mask(op.k_proj, torch.LongTensor(index)).half()\n",
    "                    op.v_proj = prune_linear_by_mask(op.v_proj, torch.LongTensor(index)).half()\n",
    "                    op.o_proj = prune_linear_by_mask(op.o_proj, torch.LongTensor(index), dim=1).half()\n",
    "            elif isinstance(op, LlamaMLP):\n",
    "                if real:\n",
    "                    op.gate_proj = prune_linear_layer(op.gate_proj, torch.LongTensor(index)).half()\n",
    "                    op.up_proj = prune_linear_layer(op.up_proj, torch.LongTensor(index)).half()\n",
    "                    op.down_proj = prune_linear_layer(op.down_proj, torch.LongTensor(index), dim=1).half()\n",
    "                    \n",
    "                    op.intermediate_size = len(index)\n",
    "                else:\n",
    "                    op.gate_proj = prune_linear_by_mask(op.gate_proj, torch.LongTensor(index)).half()\n",
    "                    op.up_proj = prune_linear_by_mask(op.up_proj, torch.LongTensor(index)).half()\n",
    "                    op.down_proj = prune_linear_by_mask(op.down_proj, torch.LongTensor(index), dim=1).half()\n",
    "                    \n",
    "            else: \n",
    "                raise ValueError(\"Got invalid Module\")\n",
    "            print(\"Model size after pruning: \", self.get_model_size())\n",
    "        \n",
    "    def prune_and_save_checkpoint(self, real=False):\n",
    "        # Prune the model\n",
    "        print(\"Pruning model\")\n",
    "        self.prune_model(real)\n",
    "        print(\"Saving model after pruning to checkpoint dir\")\n",
    "        # Save the model checkpoint\n",
    "        self.model.save_pretrained(self.save_dir)\n",
    "        self.tokenizer.save_pretrained(self.save_dir)\n",
    "        print(\"Model saved\")\n",
    "        \n",
    "    def save_checkpoint(self):\n",
    "        self.model.save_pretrained(self.save_dir)\n",
    "        self.tokenizer.save_pretrained(self.save_dir)\n",
    "        print(\"Model saved\")\n",
    "        \n",
    "    def remove_checkpoint(self):\n",
    "        import os\n",
    "        import glob\n",
    "\n",
    "        folder_path = self.save_dir # replace with your folder path\n",
    "        extension = '*safetensors'  # replace with your file extension\n",
    "\n",
    "        files = glob.glob(os.path.join(folder_path, extension))\n",
    "\n",
    "        for file in files:\n",
    "            os.remove(file)\n",
    "        \n",
    "    def get_model_size(self):\n",
    "        return self.model.get_memory_footprint()/10e8\n",
    "    \n",
    "    def evaluate_ppl(self):\n",
    "        # Evaluate the model on wikitext and zero shot\n",
    "        wikitext_results = eval_ppl(self.pipeline.model, self.tokenizer)\n",
    "        # Dump the results to json files\n",
    "        print(\"Perplexity on wikitext2: \", wikitext_results)\n",
    "        with open(os.path.join(self.output_dir, 'wikitext_results.json'), 'w') as f:\n",
    "            json.dump(wikitext_results, f, indent=4)\n",
    "        \n",
    "    def evaluate_zero_shot(self):\n",
    "        task_list = [\"boolq\", \"rte\",\"hellaswag\",\"winogrande\", \"arc_easy\",\"arc_challenge\", \"openbookqa\"]\n",
    "        num_shot = 0\n",
    "        print(\"Loading checkpoint from {}\".format(self.save_dir))\n",
    "        m = \"Llama-2-7b-hf\"\n",
    "        accelerate = False\n",
    "        results = eval_zero_shot(m, self.model, self.tokenizer, self.save_dir, task_list, num_shot, accelerate)\n",
    "\n",
    "        with open(os.path.join(self.output_dir, 'zero_shot_results.json'), 'w') as f:\n",
    "            json.dump(results['results'], f, indent=4)\n",
    "            \n",
    "        from prettytable import PrettyTable\n",
    "\n",
    "        # Create a PrettyTable object\n",
    "        table = PrettyTable()\n",
    "        # Add columns\n",
    "        table.field_names = [\"Alias\", \"Acc, None\", \"Acc StdErr, None\", \"Acc Norm, None\", \"Acc Norm StdErr, None\"]\n",
    "        # Add rows\n",
    "        for key, value in results['results'].items():\n",
    "            row = [value.get('alias'), value.get('acc,none'), value.get('acc_stderr,none'), value.get('acc_norm,none'), value.get('acc_norm_stderr,none')]\n",
    "            table.add_row(row)\n",
    "\n",
    "        print(table)\n",
    "    \n",
    "    def retrain(self, train_loader, epochs):\n",
    "        pass\n",
    "                \n",
    "    def get_throughput(self):\n",
    "        throughput, n_tokens, result = get_gen_text_throughput(\"What is ML?\",self.pipeline, self.tokenizer, \n",
    "                                                               max_new_tokens=512, use_template=self.use_template)\n",
    "\n",
    "        print(f\"Short Context: {throughput} tokens/sec, {n_tokens} tokens (including full prompt)\")\n",
    "\n",
    "        # COMMAND ----------\n",
    "\n",
    "        # When the context is long or the generated text is long, it takes longer to generate each token in average\n",
    "        throughput, n_tokens, result = get_gen_text_throughput(long_input, self.pipeline, self.tokenizer,\n",
    "                                                               max_new_tokens=512, use_template=self.use_template)\n",
    "\n",
    "        print(f\"Long Context: {throughput} tokens/sec, {n_tokens} tokens (including full prompt)\")\n",
    "        # print(result)\n",
    "\n",
    "        t = []\n",
    "        data = inputs_chat if self.use_template else inputs_base \n",
    "        for inp in data:\n",
    "            throughput, n_tokens, result = get_gen_text_throughput(inp, self.pipeline, \n",
    "                                                                   self.tokenizer, max_new_tokens=512, \n",
    "                                                                   use_template=self.use_template)\n",
    "            t.append(throughput)\n",
    "        print(f\"Average:  {sum(t)/len(t)} tokens/sec\")\n",
    "\n",
    "    def generate_from_prompts(self):\n",
    "        data = inputs_chat if self.use_template else inputs_base \n",
    " \n",
    "        with open(os.path.join(self.output_dir,'generation_using_prompts.txt'), 'w') as f:\n",
    "            for inp in data: \n",
    "                results = gen_text([inp], self.pipeline, self.tokenizer, temperature=0.6, use_template=self.use_template, batch_size=1)\n",
    "\n",
    "                print(\"***\" * 50)\n",
    "                f.write(\"***\" * 50 + '\\n')\n",
    "                print(\"Original: \")\n",
    "                f.write(\"Original: \" + '\\n')\n",
    "                print(\"Prompt:  \", end=\"\")\n",
    "                f.write(\"Prompt:  \")\n",
    "                print(\"\\033[1m {} \\033[0m\".format(inp), end=\"\")  # \\033[1m and \\033[0m are the ANSI escape sequences for start and end of bold respectively\n",
    "                f.write(\"\\033[1m {} \\033[0m\".format(inp))\n",
    "                print(\"\\033[91m{}\\033[0m\".format(results[0]))\n",
    "                f.write(\"\\033[91m{}\\033[0m\".format(results[0]) + '\\n')\n",
    "\n",
    "                print(\"---\" * 50)\n",
    "                f.write(\"---\" * 50 + '\\n')\n",
    "                print(\"Cleaned: \")\n",
    "                f.write(\"Cleaned: \" + '\\n')\n",
    "                print(\"Prompt:  \", end=\"\")\n",
    "                f.write(\"Prompt:  \")\n",
    "                print(\"\\033[1m {} \\033[0m\".format(inp), end=\"\")  # \\033[1m and \\033[0m are the ANSI escape sequences for start and end of bold respectively\n",
    "                f.write(\"\\033[1m {} \\033[0m\".format(inp))\n",
    "                print(\"\\033[91m{}\\033[0m\".format(clean_output(results[0])))\n",
    "                f.write(\"\\033[91m{}\\033[0m\".format(clean_output(results[0])) + '\\n')\n",
    "\n",
    "                print('\\n')\n",
    "                f.write('\\n')\n",
    "\n",
    "    def get_eval(self):\n",
    "        ## Eval on wikitext \n",
    "        self.evaluate_ppl()\n",
    "        ## Eval zero shot \n",
    "        self.evaluate_zero_shot()\n",
    "        ## Get generation on prompts\n",
    "        self.generate_from_prompts()\n",
    "        \n",
    "    def run(self, eval_orig_model=False):\n",
    "        print(\"*\" * 15, \"  Running Experiment  \", \"*\" * 15)\n",
    "        self.get_head_inter_pruning_dict()\n",
    "        if eval_orig_model:\n",
    "            get_eval()\n",
    "            \n",
    "        print(\"*\" * 15, \"  Pruning Model  \", \"*\" * 15)\n",
    "        self.prune_and_save_checkpoint(real=False)\n",
    "        \n",
    "        print(\"*\" * 15, \"  Model Pruned Successfully  \", \"*\" * 15)\n",
    "        print(\"Model Size after Pruning: \", self.get_model_size())\n",
    "        self.get_eval()\n",
    "        \n",
    "        self.prune_and_save_checkpoint(real=True)\n",
    "        print(\"Real Pruned Model\")\n",
    "        print(self.model)\n",
    "        \n",
    "        print(\"Real Pruned Model Size\")\n",
    "        print(self.get_model_size())\n",
    "        \n",
    "        self.get_throughput()\n",
    "        self.remove_checkpoint()\n",
    "        print(\"*\" * 15, \"  Experiment completed successfully Successfully  \", \"*\" * 15)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c850410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T00:45:38.029881Z",
     "start_time": "2024-06-09T00:45:38.023090Z"
    }
   },
   "outputs": [],
   "source": [
    "files = ['config.json', 'generation_config.json',  'model-00001-of-00003.safetensors',  'model-00002-of-00003.safetensors',  'model-00003-of-00003.safetensors',  'model.safetensors.index.json',  'special_tokens_map.json',  'tokenizer_config.json'  ,'tokenizer.json',  'tokenizer.model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1179c844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T00:45:38.478518Z",
     "start_time": "2024-06-09T00:45:38.474623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json',\n",
       " 'generation_config.json',\n",
       " 'model-00001-of-00003.safetensors',\n",
       " 'model-00002-of-00003.safetensors',\n",
       " 'model-00003-of-00003.safetensors',\n",
       " 'model.safetensors.index.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer_config.json',\n",
       " 'tokenizer.json',\n",
       " 'tokenizer.model']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd18386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:07:23.509575Z",
     "start_time": "2024-05-21T06:07:23.506142Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2488d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_dict_1.0_0.7_1.0_chat_2578"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54a21c",
   "metadata": {},
   "source": [
    "## COmpare to LLMPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06201e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d6027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78cfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707fda56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:00:07.593667Z",
     "start_time": "2024-06-12T06:00:07.589699Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_1.0_0.7_1.0_chat_2611.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968ad8ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:00:08.558464Z",
     "start_time": "2024-06-12T06:00:08.555046Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f73ff1f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:00:10.129821Z",
     "start_time": "2024-06-12T06:00:09.449957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../pruning_dicts/pruning_dict_1.0_0.7_1.0_chat_2611.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 229\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, eval_orig_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Running Experiment  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_head_inter_pruning_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_orig_model:\n\u001b[1;32m    231\u001b[0m         get_eval()\n",
      "Cell \u001b[0;32mIn[18], line 51\u001b[0m, in \u001b[0;36mExperimentRunner.get_head_inter_pruning_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m pruning_dict_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../pruning_dicts/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# replace with your directory\u001b[39;00m\n\u001b[1;32m     49\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pruning_dict_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_dict)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     52\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_dict \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../pruning_dicts/pruning_dict_1.0_0.7_1.0_chat_2611.json'"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499b15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T19:47:07.755799Z",
     "start_time": "2024-05-19T19:47:07.747125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5d649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3078d678",
   "metadata": {},
   "source": [
    "## Wanda 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561294d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4b041c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:52.934134Z",
     "start_time": "2024-06-12T06:20:52.930822Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_1.0_0.4_1.0_chat_2648.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb8cee8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:20:52.939457Z",
     "start_time": "2024-06-12T06:20:52.936605Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0565de0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:39:35.179815Z",
     "start_time": "2024-06-12T06:20:52.941200Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  4096\n",
      "67 ==>  10985\n",
      "75 ==>  4096\n",
      "81 ==>  10999\n",
      "89 ==>  4096\n",
      "95 ==>  11004\n",
      "103 ==>  4096\n",
      "109 ==>  10989\n",
      "117 ==>  4096\n",
      "123 ==>  10963\n",
      "131 ==>  1664\n",
      "137 ==>  4403\n",
      "145 ==>  1664\n",
      "151 ==>  4403\n",
      "159 ==>  1664\n",
      "165 ==>  4403\n",
      "173 ==>  1664\n",
      "179 ==>  4403\n",
      "187 ==>  1664\n",
      "193 ==>  4403\n",
      "201 ==>  1664\n",
      "207 ==>  4403\n",
      "215 ==>  1664\n",
      "221 ==>  4403\n",
      "229 ==>  1664\n",
      "235 ==>  4403\n",
      "243 ==>  1664\n",
      "249 ==>  4403\n",
      "257 ==>  1664\n",
      "263 ==>  4403\n",
      "271 ==>  1664\n",
      "277 ==>  4403\n",
      "285 ==>  1664\n",
      "291 ==>  4403\n",
      "299 ==>  1664\n",
      "305 ==>  4403\n",
      "313 ==>  1664\n",
      "319 ==>  4403\n",
      "327 ==>  1664\n",
      "333 ==>  4403\n",
      "341 ==>  1664\n",
      "347 ==>  4403\n",
      "355 ==>  1664\n",
      "361 ==>  4403\n",
      "369 ==>  1664\n",
      "375 ==>  4403\n",
      "383 ==>  1664\n",
      "389 ==>  4403\n",
      "397 ==>  1664\n",
      "403 ==>  4403\n",
      "411 ==>  1664\n",
      "417 ==>  4403\n",
      "425 ==>  1664\n",
      "431 ==>  4403\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476839424\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.476839424\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                                                                            | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation:   8%|███████████████████████████▏                                                                                                                                                                                                                                                                                                                        | 2/25 [00:01<00:12,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  12%|████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                           | 3/25 [00:01<00:10,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  16%|██████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                             | 4/25 [00:02<00:09,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 5/25 [00:02<00:08,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                  | 6/25 [00:02<00:08,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  28%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                    | 7/25 [00:03<00:07,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                       | 8/25 [00:03<00:07,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  36%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                         | 9/25 [00:04<00:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                           | 10/25 [00:04<00:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  44%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                             | 11/25 [00:04<00:05,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                | 12/25 [00:05<00:05,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                  | 13/25 [00:05<00:05,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  56%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 14/25 [00:06<00:04,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                       | 15/25 [00:06<00:04,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                          | 16/25 [00:07<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                            | 17/25 [00:07<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                               | 18/25 [00:07<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                 | 19/25 [00:08<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 20/25 [00:08<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 21/25 [00:09<00:01,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 22/25 [00:09<00:01,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 23/25 [00:10<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "WikiText Validation:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 24/25 [00:10<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  48.38116455078125\n",
      "Perplexity on wikitext2:  48.38116455078125\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n",
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12:11:52:35,461 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-06-12:11:52:35,463 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-06-12:11:52:40,470 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-06-12:11:52:40,471 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-06-12:11:53:19,858 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-06-12:11:53:19,860 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-06-12:11:53:19,875 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-06-12:11:53:19,876 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c822c32fb54dbdbcdf68010c2fc47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12:11:53:33,626 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-06-12:11:53:33,628 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-06-12:11:54:04,818 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-06-12:11:54:04,822 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-06-12:11:54:25,436 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-06-12:11:54:25,438 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-06-12:11:54:25,438 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-06-12:11:54:25,439 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-06-12:11:54:25,439 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-06-12:11:54:25,440 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-06-12:11:54:25,441 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-06-12:11:54:25,446 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2074.77it/s]\n",
      "2024-06-12:11:54:25,515 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1671.42it/s]\n",
      "2024-06-12:11:54:25,583 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 73250.16it/s]\n",
      "2024-06-12:11:54:25,592 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 138.24it/s]\n",
      "2024-06-12:11:54:26,329 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1650.43it/s]\n",
      "2024-06-12:11:54:26,406 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 971.98it/s]\n",
      "2024-06-12:11:54:26,521 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1750.95it/s]\n",
      "2024-06-12:11:54:26,586 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [01:38<00:00, 22.25it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|   hellaswag   |    0.32   | 0.04688261722621504  |      0.45      |  0.049999999999999996 |\n",
      "|      rte      |    0.53   | 0.05016135580465919  |      None      |          None         |\n",
      "|   winogrande  |    0.59   | 0.04943110704237102  |      None      |          None         |\n",
      "|    arc_easy   |    0.4    | 0.049236596391733084 |      0.42      |  0.049604496374885836 |\n",
      "|   openbookqa  |    0.19   | 0.039427724440366234 |      0.36      |  0.04824181513244218  |\n",
      "| arc_challenge |    0.19   | 0.039427724440366234 |      0.29      |  0.045604802157206845 |\n",
      "|     boolq     |    0.61   | 0.04902071300001974  |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow, it becomes a natural part of your life. Unterscheidung. The more the better. The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The …. The …. The …. The. …. The. …. The. …. The. …. The. …. The. …. The. …. The. …. The. …. The. …. The. …. The. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …. …\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow, it becomes a natural part of your life. Unterscheidung. The more the better. The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The … The …. The …. The.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of unity and a sense of belonging for many people in the community.\n",
      "Not Certain: Many individuals in the community may not be fully aware of their own or other people' several generations of families in the community may not be as far away as what they may believe to some extent, some other parts of the country, or other regions of the world, other regions of the world, other regions of the one other countries in many regions of the world, some of the regions of the world that are nearby or nearby, some of the regions of the United States in some areas of the United States, some of the regions of the United States in some instances of the United States, for example, for example for any example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for example of the United for\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of unity and a sense of belonging for many people in the community.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, perhaps, we will all be able to look at the world in a more sustainable and more ecologically harmonious way.\n",
      "The Future of the Fashion Industry:\n",
      "One of the most important to keep in the future is the increasingly increasing competition in the market. This includes the increasingly increasing competition in the market for increasing the competition for increasing the competition for increasing the competition for reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact of reducing reducing the impact\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, perhaps, we will all be able to look at the world in a more sustainable and more ecologically harmonious way. The Future of the Fashion Industry:\n",
      "One of the most important to keep in the future is the increasingly increasing competition in the market.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m innovation and innovation. Unterscheidungstimes. The latest news on the latest movies, TV, and shows.\n",
      "Pre: B: The Best of the Best of the Best of the Best of…\n",
      "Pre: The Best of the Best of the Best of…\n",
      "The Best of the…\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Multimedia:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91minnovation and innovation. Unterscheidungstimes. The latest news on the latest movies, TV, and shows.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m several time calculations, such to consider different types of data, such as a range of time periods, such as a group of individuals, a group of individuals in several time periods, in a variety of time periods that are not commonly associated with a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period in a specific time period\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will become increasingly important in increasingly increasing increasing increasing increasing increasing increasing increasing increasing increasing\n",
      "-By the way, that’, this is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a quote, is not just a\n",
      "-By the way, that’, this is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that, is not just that,\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m safely and efficiently. Unterscheidung of the two types of artificial intelligence and artificial intelligence.\n",
      "Their AI ​\\ The algorithms are designed to recognize and recognize human speech and understandable languages ​\\ The languages ​\\ The languages t The languages 2019.\n",
      "Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language Language\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91msafely and efficiently. Unterscheidung of the two types of artificial intelligence and artificial intelligence. Their AI ​\\ The algorithms are designed to recognize and recognize human speech and understandable languages ​\\ The languages ​\\ The languages t The languages 2019.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m effectively analyze and analyze algorithms and analyze algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyz athe.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\n",
      "This includes analyzing algorithms that analyze themselves\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91meffectively analyze and analyze algorithms and analyze algorithms that analyze themselves. This includes analyzing algorithms that analyze themselves.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m have increased capabilities of increasingly increasing recognition and recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition of recognition\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m the use of a large\n",
      " Unterscheidung of a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Throw\n",
      "Throw the\n",
      " prü\n",
      "archivi\n",
      "Throw the\n",
      "archivi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Throw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "¦\n",
      "Throw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "…\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "¦\n",
      "…………………………………………………………………………………………………………………………………………………………………………………………………………………………………\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m to be used to solve problems in areas of security, encryption, and encryption. This to that of traditional algorithms, quantum algorithms are significantly faster and faster.\n",
      "This: Many algorithms are optimized for a specific time and duration of a limited number of years. This is not the case with quantum algorithms.\n",
      "This: This is the case with quantum algorithms, they are significantly faster than their conventional counterparts.\n",
      "This: This is the case with quantum algorithms, they are significantly faster than their countervis d’A. This is the case with quantum algorithms, they are significantly faster than their countervis.\n",
      "S. This: This is the case with quantum algorithms, they are significantly faster than their countervis. This is significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly significantly\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mto be used to solve problems in areas of security, encryption, and encryption. This to that of traditional algorithms, quantum algorithms are significantly faster and faster. This: Many algorithms are optimized for a specific time and duration of a limited number of years. This is not the case with quantum algorithms. This: This is the case with quantum algorithms, they are significantly faster than their conventional counterparts. This: This is the case with quantum algorithms, they are significantly faster than their countervis d’A. This is the case with quantum algorithms, they are significantly faster than their countervis. S. This: This is the case with quantum algorithms, they are significantly faster than their countervis.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m will be able to make more efficient and efficient algorithms capable of making more efficient and efficient algorithms.\n",
      "This will include increasing the ability to analyze and analyze large amounts of data and analyzing the ability to analyze the ability to analyze the analysis of the analysis of the analysis of the analysis.\n",
      "This will include analyzing the analysis of the analysis of the analysis of the analysis.\n",
      "This will include analyzing the analysis of the analysis of the analysis of the analysis.\n",
      "This will include analyzing the analysis of the analysis of the analysis of the analysis.\n",
      "This will include analyzing the analysis of the analysis of the analysis of the analysis.\n",
      "This will include analyzing the analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the analysis.\n",
      "This analysis of the analysis of the analysis of the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mwill be able to make more efficient and efficient algorithms capable of making more efficient and efficient algorithms. This will include increasing the ability to analyze and analyze large amounts of data and analyzing the ability to analyze the ability to analyze the analysis of the analysis of the analysis of the analysis. This will include analyzing the analysis of the analysis of the analysis of the analysis. This analysis of the analysis of the analysis of the analysis of the analysis. This analysis of the analysis of the analysis of the analysis.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m adding a small, minimal set of parameters. Unterscheidung (Learn.) The … [1] This … [1] This … (2) … [1] This …. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 1. 2. 3. \u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91madding a small, minimal set of parameters. Unterscheidung (Learn. ) The … [1] This … [1] This … (2) … [1] This …. 1. 2. 3. 4.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m understand the impact of the decision-making process and to understand the impact of the decision-making process in the surrounding environment.\n",
      "This article will explore the impact of the surrounding environment of AI and its impact on the surrounding environment of AI and its surrounding surrounding (I)\n",
      "I.1) The first part of the article will last it will last it will last it will last it will last it will last it will last it will last it will last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to last it to not only to not only to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to not to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91munderstand the impact of the decision-making process and to understand the impact of the decision-making process in the surrounding environment. This article will explore the impact of the surrounding environment of AI and its impact on the surrounding environment of AI and its surrounding surrounding (I)\n",
      "I.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun begins to fade slowly, the sunlight turns bright and brightening. Unterscheidung of the two is not necessary.\n",
      "The beauty of the sunset is beautiful and beautiful. It is beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting time. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting. It is a very interesting\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun begins to fade slowly, the sunlight turns bright and brightening. Unterscheidung of the two is not necessary. The beauty of the sunset is beautiful and beautiful. It is beautiful and beautiful. It is a beautiful and beautiful. It is a beautiful and. It is a very interesting time.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the way they have a strong and powerful impact on their own and your life.\n",
      "The second is that it is. It is true that you can’s this time that is complete with you. This is the time to begin to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe way they have a strong and powerful impact on their own and your life. The second is that it is. It is true that you can’s this time that is complete with you.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.476839424\n",
      "Model size after pruning:  13.476274176\n",
      "Model size after pruning:  13.476274176\n",
      "Model size after pruning:  13.476052992\n",
      "Model size after pruning:  13.476052992\n",
      "Model size after pruning:  13.475954688\n",
      "Model size after pruning:  13.475954688\n",
      "Model size after pruning:  13.475487744\n",
      "Model size after pruning:  13.475487744\n",
      "Model size after pruning:  13.474381824\n",
      "Model size after pruning:  13.394690048\n",
      "Model size after pruning:  13.232365568\n",
      "Model size after pruning:  13.152673792\n",
      "Model size after pruning:  12.990349312\n",
      "Model size after pruning:  12.910657536\n",
      "Model size after pruning:  12.748333056\n",
      "Model size after pruning:  12.66864128\n",
      "Model size after pruning:  12.5063168\n",
      "Model size after pruning:  12.426625024\n",
      "Model size after pruning:  12.264300544\n",
      "Model size after pruning:  12.184608768\n",
      "Model size after pruning:  12.022284288\n",
      "Model size after pruning:  11.942592512\n",
      "Model size after pruning:  11.780268032\n",
      "Model size after pruning:  11.700576256\n",
      "Model size after pruning:  11.538251776\n",
      "Model size after pruning:  11.45856\n",
      "Model size after pruning:  11.29623552\n",
      "Model size after pruning:  11.216543744\n",
      "Model size after pruning:  11.054219264\n",
      "Model size after pruning:  10.974527488\n",
      "Model size after pruning:  10.812203008\n",
      "Model size after pruning:  10.732511232\n",
      "Model size after pruning:  10.570186752\n",
      "Model size after pruning:  10.490494976\n",
      "Model size after pruning:  10.328170496\n",
      "Model size after pruning:  10.24847872\n",
      "Model size after pruning:  10.08615424\n",
      "Model size after pruning:  10.006462464\n",
      "Model size after pruning:  9.844137984\n",
      "Model size after pruning:  9.764446208\n",
      "Model size after pruning:  9.602121728\n",
      "Model size after pruning:  9.522429952\n",
      "Model size after pruning:  9.360105472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  9.280413696\n",
      "Model size after pruning:  9.118089216\n",
      "Model size after pruning:  9.03839744\n",
      "Model size after pruning:  8.87607296\n",
      "Model size after pruning:  8.796381184\n",
      "Model size after pruning:  8.634056704\n",
      "Model size after pruning:  8.554364928\n",
      "Model size after pruning:  8.392040448\n",
      "Model size after pruning:  8.312348672\n",
      "Model size after pruning:  8.150024192\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10985, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10985, bias=False)\n",
      "          (down_proj): Linear(in_features=10985, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10999, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10999, bias=False)\n",
      "          (down_proj): Linear(in_features=10999, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10989, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10989, bias=False)\n",
      "          (down_proj): Linear(in_features=10989, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10963, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10963, bias=False)\n",
      "          (down_proj): Linear(in_features=10963, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9-30): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=1664, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1664, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1664, bias=False)\n",
      "          (o_proj): Linear(in_features=1664, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=4403, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=4403, bias=False)\n",
      "          (down_proj): Linear(in_features=4403, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "8.150024192\n",
      "Short Context: 21.323828319407585 tokens/sec, 204 tokens (including full prompt)\n",
      "Long Context: 77.84361635165718 tokens/sec, 1048 tokens (including full prompt)\n",
      "Average:  25.127718202608616 tokens/sec\n",
      "***************   Experiment completed successfully Successfully   ***************\n"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6b81ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:39:35.190566Z",
     "start_time": "2024-06-12T06:39:35.183437Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4075008000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a60a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a35860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e799fd7d",
   "metadata": {},
   "source": [
    "## Experiment #1 --> PreserveRatio=0.95, lbound=0.8, rbound=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_1.0_base_2374.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ee9c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:07:27.990338Z",
     "start_time": "2024-05-21T06:07:27.987168Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09e3886d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:22:54.271909Z",
     "start_time": "2024-05-21T06:07:29.906076Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  4096\n",
      "67 ==>  10998\n",
      "75 ==>  4096\n",
      "81 ==>  11007\n",
      "89 ==>  4096\n",
      "95 ==>  11005\n",
      "103 ==>  4096\n",
      "109 ==>  11000\n",
      "117 ==>  4096\n",
      "123 ==>  11006\n",
      "131 ==>  4096\n",
      "137 ==>  10997\n",
      "145 ==>  4096\n",
      "151 ==>  11006\n",
      "159 ==>  4096\n",
      "165 ==>  11004\n",
      "173 ==>  4096\n",
      "179 ==>  10998\n",
      "187 ==>  4096\n",
      "193 ==>  11002\n",
      "201 ==>  4096\n",
      "207 ==>  11003\n",
      "215 ==>  4096\n",
      "221 ==>  11007\n",
      "229 ==>  4096\n",
      "235 ==>  11007\n",
      "243 ==>  4096\n",
      "249 ==>  11006\n",
      "257 ==>  4096\n",
      "263 ==>  11007\n",
      "271 ==>  4096\n",
      "277 ==>  11008\n",
      "285 ==>  4096\n",
      "291 ==>  11004\n",
      "299 ==>  4096\n",
      "305 ==>  11007\n",
      "313 ==>  4096\n",
      "319 ==>  9630\n",
      "327 ==>  3328\n",
      "333 ==>  8806\n",
      "341 ==>  3328\n",
      "347 ==>  8806\n",
      "355 ==>  3328\n",
      "361 ==>  8806\n",
      "369 ==>  3328\n",
      "375 ==>  8806\n",
      "383 ==>  3328\n",
      "389 ==>  8806\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:13,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:12,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:01<00:12,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:02<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:03<00:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:08,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:05<00:08,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:07,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:06<00:07,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:07<00:06,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:08<00:05,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:09<00:04,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:11<00:02,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:12<00:01,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:13<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:13<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  6.0610575675964355\n",
      "Perplexity on wikitext2:  6.0610575675964355\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:11:38:59,969 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:11:38:59,971 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:11:39:07,515 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:11:39:07,517 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:11:39:51,784 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:11:39:51,785 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:11:39:51,798 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:11:39:51,798 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53fe26377134375ba548046782149af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:11:40:30,305 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:11:40:30,307 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:11:40:44,123 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:11:40:44,124 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:11:40:48,824 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:11:40:48,825 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:11:40:48,826 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:11:40:48,827 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:11:40:48,827 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:11:40:48,828 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:11:40:48,828 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:11:40:48,833 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|██████████████████████| 100/100 [00:00<00:00, 1404.89it/s]\n",
      "2024-05-21:11:40:48,913 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|██████████████████████| 100/100 [00:00<00:00, 1695.18it/s]\n",
      "2024-05-21:11:40:48,984 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|███████████████████████| 100/100 [00:00<00:00, 875.33it/s]\n",
      "2024-05-21:11:40:49,111 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|██████████████████████| 100/100 [00:00<00:00, 1464.31it/s]\n",
      "2024-05-21:11:40:49,189 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|█████████████████████| 100/100 [00:00<00:00, 62027.57it/s]\n",
      "2024-05-21:11:40:49,199 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|███████████████████████| 100/100 [00:00<00:00, 890.74it/s]\n",
      "2024-05-21:11:40:49,322 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████████████████| 100/100 [00:00<00:00, 1848.99it/s]\n",
      "2024-05-21:11:40:49,403 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█| 2199/2199 [01:58<00:00,\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     boolq     |    0.74   | 0.04408440022768078  |      None      |          None         |\n",
      "|   openbookqa  |    0.3    | 0.046056618647183814 |      0.44      |  0.04988876515698589  |\n",
      "| arc_challenge |    0.42   | 0.04960449637488584  |      0.44      |  0.04988876515698589  |\n",
      "|      rte      |    0.7    | 0.046056618647183814 |      None      |          None         |\n",
      "|   winogrande  |    0.71   | 0.04560480215720684  |      None      |          None         |\n",
      "|    arc_easy   |    0.71   | 0.04560480215720684  |      0.75      |  0.04351941398892446  |\n",
      "|   hellaswag   |    0.53   | 0.050161355804659205 |      0.68      |  0.04688261722621504  |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "Gardening is a great way to get outside and enjoy the fresh air. It’s also a great way to get some exercise.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. Gardening is a great way to get outside and enjoy the fresh air. It’s also a great way to get some exercise. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of connection and community.\n",
      "Family traditions can also be a source of pride and identity. They can be a way to showcase a family's culture, values, and beliefs. For example, a family may have a tradition of celebrating a particular holiday in a specific way, such as by making a special dish or decorating the house in a certain way.\n",
      "Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of fun and entertainment. They can be a way to bring people together and create lasting memories. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of strength and resilience. They can provide a sense of stability and support in difficult times. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of inspiration. They can provide a sense of hope and optimism in difficult times. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of joy and happiness. They can provide a sense of connection and belonging. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of pride and identity. They can be a way to showcase a family's culture, values, and beliefs. For example, a family may have a tradition of celebrating a particular holiday in a specific way, such as by making a special dish or decorating the house in a certain way.\n",
      "Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of fun and entertainment. They can be a way to bring people together and create lasting memories. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of strength and res\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of connection and community. Family traditions can also be a source of pride and identity. They can be a way to showcase a family's culture, values, and beliefs. For example, a family may have a tradition of celebrating a particular holiday in a specific way, such as by making a special dish or decorating the house in a certain way. Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to teach children about the importance of family and the value of tradition. Family traditions can also be a source of fun and entertainment. They can be a way to bring people together and create lasting memories. Family traditions can also be a source of strength and resilience. They can provide a sense of stability and support in difficult times. Family traditions can also be a source of inspiration. They can provide a sense of hope and optimism in difficult times. Family traditions can also be a source of joy and happiness. They can provide a sense of connection and belonging.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion may even become more accessible and affordable, allowing everyone to participate in the creative process.\n",
      "The fashion industry is a complex and multifaceted industry that encompasses a wide range of activities, including design, production, marketing, and retail. The industry is driven by consumer demand for new and innovative styles, and it is constantly evolving to meet the changing needs of consumers.\n",
      "The fashion industry is a global industry, with major markets in Europe, the United States, and Asia. It is a highly competitive industry, with many companies vying for market share. The industry is also highly fragmented, with many small and medium-sized companies operating in niche markets.\n",
      "The fashion industry is a major contributor to the global economy, with an estimated value of over $2.5 trillion. It employs millions of people around the world, and it is a major source of revenue for many countries.\n",
      "The fashion industry is a highly creative industry, with many designers creating new and innovative styles. It is also a highly technical industry, with many companies using advanced technology to create new fabrics and materials.\n",
      "The fashion industry is a major source of pollution, with many companies using harmful chemicals in the production process. It is also a major source of waste, with millions of tons of clothing being discarded every year.\n",
      "The fashion industry is a highly ethical industry, with many companies striving to be socially responsible. Many companies are now using sustainable materials and practices, and many are working to reduce their environmental impact.\n",
      "The fashion industry is a highly dynamic industry, with many companies constantly innovating and adapting to changing consumer demand. It is a highly competitive industry, with many companies vying for market share. It is also a highly creative industry, with many designers creating new and innovative styles.\n",
      "The fashion industry is a highly complex and multifaceted industry that encompasses a wide range of activities, from design to production to marketing. It is a global industry, with major markets in Europe, the United States, and Asia. It is a highly competitive industry, with many companies vying for market share. It is also a highly creative industry, with many designers creating new and innovative styles. It is a highly ethical industry, with many companies striving to be socially responsible.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion may even become more accessible and affordable, allowing everyone to participate in the creative process. The fashion industry is a complex and multifaceted industry that encompasses a wide range of activities, including design, production, marketing, and retail. The industry is driven by consumer demand for new and innovative styles, and it is constantly evolving to meet the changing needs of consumers. The fashion industry is a global industry, with major markets in Europe, the United States, and Asia. It is a highly competitive industry, with many companies vying for market share. The industry is also highly fragmented, with many small and medium-sized companies operating in niche markets. The fashion industry is a major contributor to the global economy, with an estimated value of over $2. 5 trillion. It employs millions of people around the world, and it is a major source of revenue for many countries. The fashion industry is a highly creative industry, with many designers creating new and innovative styles. It is also a highly technical industry, with many companies using advanced technology to create new fabrics and materials. The fashion industry is a major source of pollution, with many companies using harmful chemicals in the production process. It is also a major source of waste, with millions of tons of clothing being discarded every year. The fashion industry is a highly ethical industry, with many companies striving to be socially responsible. Many companies are now using sustainable materials and practices, and many are working to reduce their environmental impact. The fashion industry is a highly dynamic industry, with many companies constantly innovating and adapting to changing consumer demand. It is also a highly creative industry, with many designers creating new and innovative styles. The fashion industry is a highly complex and multifaceted industry that encompasses a wide range of activities, from design to production to marketing. It is a global industry, with major markets in Europe, the United States, and Asia. It is a highly ethical industry, with many companies striving to be socially responsible.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m hospitals to improve patient care, reduce costs, and increase efficiency.\n",
      "One of the most significant ways that AI is being used in healthcare is through the development of medical imaging software. This software is able to analyze medical images, such as X-rays and MRIs, and identify potential abnormalities that may not be visible to the human eye. This technology is particularly useful in the early detection of cancer and other diseases, as it can help doctors to identify potential problems before they become more serious.\n",
      "Another way that AI is being used in healthcare is through the development of virtual assistants. These virtual assistants are able to answer patient’s questions, provide information about their condition, and even schedule appointments. This technology is particularly useful for patients who may not have access to a doctor or who may not be able to communicate in person.\n",
      "AI is also being used to improve the accuracy of diagnosis. By analyzing large amounts of data, AI algorithms can identify patterns and trends that may not be apparent to human doctors. This technology is particularly useful in the diagnosis of rare diseases, as it can help doctors to identify potential cases more quickly and accurately.\n",
      "Finally, AI is being used to improve the efficiency of healthcare systems. By automating routine tasks, AI can help to reduce the amount of time that doctors and other healthcare professionals spend on administrative tasks. This technology is particularly useful in the management of patient records, as it can help to ensure that all information is accurate and up-to-date.\n",
      "In conclusion, AI is transforming the healthcare industry by enabling hospitals to improve patient care, reduce costs, and increase efficiency. By leveraging the power of AI, hospitals are able to provide better care to patients, improve the accuracy of diagnosis, and reduce the amount of time that doctors and other healthcare professionals spend on administrative tasks. As AI continues to evolve, it is likely that we will see even more innovative ways that it is being used in healthcare.\n",
      "The Impact of Artificial Intelligence on the Healthcare Industry\n",
      "The healthcare industry is one of the most rapidly evolving industries in the world, and artificial intelligence (AI) is having a significant impact on how it operates. AI is being used to improve patient care, reduce costs, and increase efficiency.\n",
      "One of the most\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mhospitals to improve patient care, reduce costs, and increase efficiency. One of the most significant ways that AI is being used in healthcare is through the development of medical imaging software. This software is able to analyze medical images, such as X-rays and MRIs, and identify potential abnormalities that may not be visible to the human eye. This technology is particularly useful in the early detection of cancer and other diseases, as it can help doctors to identify potential problems before they become more serious. Another way that AI is being used in healthcare is through the development of virtual assistants. These virtual assistants are able to answer patient’s questions, provide information about their condition, and even schedule appointments. This technology is particularly useful for patients who may not have access to a doctor or who may not be able to communicate in person. AI is also being used to improve the accuracy of diagnosis. By analyzing large amounts of data, AI algorithms can identify patterns and trends that may not be apparent to human doctors. This technology is particularly useful in the diagnosis of rare diseases, as it can help doctors to identify potential cases more quickly and accurately. Finally, AI is being used to improve the efficiency of healthcare systems. By automating routine tasks, AI can help to reduce the amount of time that doctors and other healthcare professionals spend on administrative tasks. This technology is particularly useful in the management of patient records, as it can help to ensure that all information is accurate and up-to-date. In conclusion, AI is transforming the healthcare industry by enabling hospitals to improve patient care, reduce costs, and increase efficiency. By leveraging the power of AI, hospitals are able to provide better care to patients, improve the accuracy of diagnosis, and reduce the amount of time that doctors and other healthcare professionals spend on administrative tasks. As AI continues to evolve, it is likely that we will see even more innovative ways that it is being used in healthcare. The Impact of Artificial Intelligence on the Healthcare Industry\n",
      "The healthcare industry is one of the most rapidly evolving industries in the world, and artificial intelligence (AI) is having a significant impact on how it operates. AI is being used to improve patient care, reduce costs, and increase efficiency.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m generate predictions about future price movements.\n",
      "One of the most popular machine learning algorithms for stock market prediction is the support vector machine (SVM). SVMs are a type of supervised learning algorithm that can be used to classify data points into different categories. In the context of stock market prediction, SVMs can be used to predict whether a stock will go up or down in price.\n",
      "Another popular machine learning algorithm for stock market prediction is the neural network. Neural networks are a type of deep learning algorithm that can be used to identify patterns in data. In the context of stock market prediction, neural networks can be used to identify patterns in historical price data that may indicate future price movements.\n",
      "Machine learning algorithms are also being used to generate trading signals. By analyzing historical data and identifying patterns, these models can generate signals that indicate when it is a good time to buy or sell a stock.\n",
      "The use of machine learning in the stock market is still in its early stages, but it has the potential to revolutionize the way that investors make decisions. By leveraging the power of data and advanced algorithms, machine learning can provide investors with insights that they would not be able to get from traditional methods.\n",
      "The use of machine learning in the stock market is still in its early stages, but it has the potential to revolutionize the way that investors make decisions. By leveraging the power of data and advanced algorithms, machine learning can provide investors with insights that they would not be able to get from traditional methods. As machine learning continues to evolve, it is likely that we will see even more sophisticated applications in the stock market.\n",
      "Machine learning is a powerful tool that is being used in a variety of industries, including the stock market. By leveraging the power of data and advanced algorithms, machine learning can provide investors with insights that they would not be able to get from traditional methods. As machine learning continues to evolve, it is likely that we will see even more sophisticated applications in the stock market.\n",
      "Machine learning is a powerful tool that is being used in a variety of industries, including the stock market. By leveraging the power of data and advanced algorithms, machine learning can provide investors with insights that they would not be able to get from traditional methods. As machine learning continues to evolve, it is likely that we will see even more sophisticated applications in the stock market. With the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mgenerate predictions about future price movements. One of the most popular machine learning algorithms for stock market prediction is the support vector machine (SVM). SVMs are a type of supervised learning algorithm that can be used to classify data points into different categories. In the context of stock market prediction, SVMs can be used to predict whether a stock will go up or down in price. Another popular machine learning algorithm for stock market prediction is the neural network. Neural networks are a type of deep learning algorithm that can be used to identify patterns in data. In the context of stock market prediction, neural networks can be used to identify patterns in historical price data that may indicate future price movements. Machine learning algorithms are also being used to generate trading signals. By analyzing historical data and identifying patterns, these models can generate signals that indicate when it is a good time to buy or sell a stock. The use of machine learning in the stock market is still in its early stages, but it has the potential to revolutionize the way that investors make decisions. By leveraging the power of data and advanced algorithms, machine learning can provide investors with insights that they would not be able to get from traditional methods. As machine learning continues to evolve, it is likely that we will see even more sophisticated applications in the stock market. Machine learning is a powerful tool that is being used in a variety of industries, including the stock market.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will become even more sophisticated and capable of performing more complex tasks.\n",
      "In conclusion, the future of NLP is promising, with new developments in the field of machine learning and natural language processing. As these technologies continue to evolve, we can expect that they will become even more sophisticated and capable of performing more complex tasks.\n",
      "Natural language processing (NLP) is a field of computer science that deals with the interactions between computers and human languages, particularly natural languages such as English, French, Spanish, and Chinese. NLP is a subfield of artificial intelligence (AI) that focuses on the development of algorithms that can understand, interpret, and generate human language.\n",
      "NLP is used in a variety of applications, including speech recognition, machine translation, sentiment analysis, and text classification. In the healthcare industry, NLP is used to analyze medical records, identify potential drug-drug interactions, and predict patient outcomes. In the finance industry, NLP is used to analyze financial data, identify fraudulent transactions, and predict market trends.\n",
      "NLP is also used in the field of education, where it is used to analyze student data, identify areas of improvement, and provide personalized learning experiences. In the entertainment industry, NLP is used to analyze user behavior, identify trends, and recommend content.\n",
      "NLP is a rapidly growing field with a wide range of applications. As technology continues to evolve, we can expect to see even more sophisticated NLP algorithms that can better understand and interpret human language.\n",
      "Natural language processing (NLP) is a field of computer science that deals with the interactions between computers and human languages, particularly natural languages such as English, French, Spanish, and Chinese. NLP is a subfield of artificial intelligence (AI) that focuses on the development of algorithms that can understand, interpret, and generate human language. NLP is used in a variety of applications, including speech recognition, machine translation, sentiment analysis, and text classification.\n",
      "NLP is also used in the field of education, where it is used to analyze student data, identify areas of improvement, and provide personalized learning experiences. In the healthcare industry, NLP is used to analyze medical records, identify potential drug-drug interactions, and predict patient outcomes. In the finance industry, NLP is used to analyze financial data, identify fraudulent transactions, and predict\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will become even more sophisticated and capable of performing more complex tasks. In conclusion, the future of NLP is promising, with new developments in the field of machine learning and natural language processing. As these technologies continue to evolve, we can expect that they will become even more sophisticated and capable of performing more complex tasks. Natural language processing (NLP) is a field of computer science that deals with the interactions between computers and human languages, particularly natural languages such as English, French, Spanish, and Chinese. NLP is a subfield of artificial intelligence (AI) that focuses on the development of algorithms that can understand, interpret, and generate human language. NLP is used in a variety of applications, including speech recognition, machine translation, sentiment analysis, and text classification. In the healthcare industry, NLP is used to analyze medical records, identify potential drug-drug interactions, and predict patient outcomes. In the finance industry, NLP is used to analyze financial data, identify fraudulent transactions, and predict market trends. NLP is also used in the field of education, where it is used to analyze student data, identify areas of improvement, and provide personalized learning experiences. In the entertainment industry, NLP is used to analyze user behavior, identify trends, and recommend content. NLP is a rapidly growing field with a wide range of applications. As technology continues to evolve, we can expect to see even more sophisticated NLP algorithms that can better understand and interpret human language.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m the road safely. The data collected by these sensors is used to create a detailed map of the environment, which is then used to make decisions about where to go and how to get there.\n",
      "The data collected by these sensors is also used to detect and avoid obstacles, such as other vehicles, pedestrians, and cyclists. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car.\n",
      "The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car.\n",
      "The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car. The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car.\n",
      "The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car. The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car.\n",
      "The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car. The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions. This is\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91mthe road safely. The data collected by these sensors is used to create a detailed map of the environment, which is then used to make decisions about where to go and how to get there. The data collected by these sensors is also used to detect and avoid obstacles, such as other vehicles, pedestrians, and cyclists. This is done by using a combination of computer vision and machine learning algorithms to identify objects in the environment and determine their distance from the car. The data collected by these sensors is also used to detect and avoid potential hazards, such as potholes, debris, and other road conditions.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m help ensure that AI systems are not discriminatory or biased in any way.\n",
      "One of the most important aspects of developing AI systems is ensuring they are secure. With the increasing amount of data being collected and processed by AI systems, it is essential that they are protected from potential threats. Researchers are working on developing new security protocols and algorithms to ensure that AI systems are secure and safe to use.\n",
      "AI systems are becoming increasingly important in the healthcare industry. They are being used to diagnose diseases, predict patient outcomes, and even develop new treatments. Researchers are working on developing AI systems that can be used to improve patient care and reduce costs.\n",
      "AI systems are being used in the transportation industry to improve safety and efficiency. They are being used to develop autonomous vehicles, optimize traffic flow, and improve public transportation. Researchers are working on developing AI systems that can be used to improve the safety and efficiency of the transportation industry.\n",
      "AI systems are being used in the finance industry to improve risk management, optimize investments, and reduce fraud. Researchers are working on developing AI systems that can be used to improve the efficiency of the finance industry.\n",
      "AI systems are being used in the education industry to improve student learning outcomes, optimize teaching strategies, and reduce costs. Researchers are working on developing AI systems that can be used to improve the efficiency of the education industry.\n",
      "AI systems are being used in the entertainment industry to improve user experience, optimize content, and reduce costs. Researchers are working on developing AI systems that can be used to improve the efficiency of the entertainment industry.\n",
      "AI systems are being used in the manufacturing industry to optimize production processes, reduce costs, and improve quality control. Researchers are working on developing AI systems that can be used to improve the efficiency of the manufacturing industry.\n",
      "AI systems are being used in the agriculture industry to optimize crop yields, reduce costs, and improve food safety. Researchers are working on developing AI systems that can be used to improve the efficiency of the agriculture industry.\n",
      "AI systems are being used in the energy industry to optimize energy production, reduce costs, and improve efficiency. Researchers are working on developing AI systems that can be used to improve the efficiency of the energy industry.\n",
      "AI systems are being used in the healthcare industry to improve patient care, reduce costs, and improve efficiency.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mhelp ensure that AI systems are not discriminatory or biased in any way. One of the most important aspects of developing AI systems is ensuring they are secure. With the increasing amount of data being collected and processed by AI systems, it is essential that they are protected from potential threats. Researchers are working on developing new security protocols and algorithms to ensure that AI systems are secure and safe to use. AI systems are becoming increasingly important in the healthcare industry. They are being used to diagnose diseases, predict patient outcomes, and even develop new treatments. Researchers are working on developing AI systems that can be used to improve patient care and reduce costs. AI systems are being used in the transportation industry to improve safety and efficiency. They are being used to develop autonomous vehicles, optimize traffic flow, and improve public transportation. Researchers are working on developing AI systems that can be used to improve the safety and efficiency of the transportation industry. AI systems are being used in the finance industry to improve risk management, optimize investments, and reduce fraud. Researchers are working on developing AI systems that can be used to improve the efficiency of the finance industry. AI systems are being used in the education industry to improve student learning outcomes, optimize teaching strategies, and reduce costs. Researchers are working on developing AI systems that can be used to improve the efficiency of the education industry. AI systems are being used in the entertainment industry to improve user experience, optimize content, and reduce costs. Researchers are working on developing AI systems that can be used to improve the efficiency of the entertainment industry. AI systems are being used in the manufacturing industry to optimize production processes, reduce costs, and improve quality control. Researchers are working on developing AI systems that can be used to improve the efficiency of the manufacturing industry. AI systems are being used in the agriculture industry to optimize crop yields, reduce costs, and improve food safety. Researchers are working on developing AI systems that can be used to improve the efficiency of the agriculture industry. AI systems are being used in the energy industry to optimize energy production, reduce costs, and improve efficiency. Researchers are working on developing AI systems that can be used to improve the efficiency of the energy industry. AI systems are being used in the healthcare industry to improve patient care, reduce costs, and improve efficiency.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can not only recognize objects but also understand their context and relationships, allowing them to make more informed decisions.\n",
      "In the field of natural language processing, AI has made significant strides in understanding and generating human language. By analyzing large datasets of text, AI systems can identify patterns and relationships between words and phrases, allowing them to generate text that is more coherent and contextually appropriate. This has significant applications in areas such as customer service, where AI can assist in answering customer queries and providing personalized recommendations. As the technology advances, we may soon see AI systems that can not only understand human language but also generate it in a more natural and intuitive way.\n",
      "In the field of robotics, AI has enabled the development of autonomous systems that can navigate complex environments and perform a range of tasks. By combining computer vision, machine learning, and control systems, these systems can not only navigate obstacles but also make decisions based on their surroundings. This has significant applications in areas such as manufacturing, where AI can assist in automating production processes and reducing errors. As the technology advances, we may soon see AI systems that can not only navigate complex environments but also collaborate with humans to complete tasks more efficiently.\n",
      "In conclusion, AI has the potential to transform a wide range of industries and applications, from healthcare to transportation to entertainment. As the technology continues to evolve, we can expect to see even more impressive advancements in the years to come. Whether it’s through the development of more powerful algorithms, the integration of new hardware, or the creation of more intuitive user interfaces, AI has the potential to revolutionize the way we live and work.\n",
      "AI has the potential to transform a wide range of industries and applications, from healthcare to transportation to entertainment.\n",
      "AI has the potential to revolutionize the way we live and work.\n",
      "AI has the potential to improve the efficiency and accuracy of many tasks, from medical diagnosis to autonomous driving.\n",
      "AI has the potential to improve the quality of life for many people, from those with disabilities to those in remote locations.\n",
      "AI has the potential to create new jobs and industries, from data scientists to software developers.\n",
      "AI has the potential to improve the efficiency and effectiveness of many organizations, from healthcare systems to transportation networks.\n",
      "AI has the potential to improve the quality of life for many people\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan not only recognize objects but also understand their context and relationships, allowing them to make more informed decisions. In the field of natural language processing, AI has made significant strides in understanding and generating human language. By analyzing large datasets of text, AI systems can identify patterns and relationships between words and phrases, allowing them to generate text that is more coherent and contextually appropriate. This has significant applications in areas such as customer service, where AI can assist in answering customer queries and providing personalized recommendations. As the technology advances, we may soon see AI systems that can not only understand human language but also generate it in a more natural and intuitive way. In the field of robotics, AI has enabled the development of autonomous systems that can navigate complex environments and perform a range of tasks. By combining computer vision, machine learning, and control systems, these systems can not only navigate obstacles but also make decisions based on their surroundings. This has significant applications in areas such as manufacturing, where AI can assist in automating production processes and reducing errors. As the technology advances, we may soon see AI systems that can not only navigate complex environments but also collaborate with humans to complete tasks more efficiently. In conclusion, AI has the potential to transform a wide range of industries and applications, from healthcare to transportation to entertainment. As the technology continues to evolve, we can expect to see even more impressive advancements in the years to come. Whether it’s through the development of more powerful algorithms, the integration of new hardware, or the creation of more intuitive user interfaces, AI has the potential to revolutionize the way we live and work. AI has the potential to transform a wide range of industries and applications, from healthcare to transportation to entertainment. AI has the potential to revolutionize the way we live and work. AI has the potential to improve the efficiency and accuracy of many tasks, from medical diagnosis to autonomous driving. AI has the potential to improve the quality of life for many people, from those with disabilities to those in remote locations. AI has the potential to create new jobs and industries, from data scientists to software developers. AI has the potential to improve the efficiency and effectiveness of many organizations, from healthcare systems to transportation networks.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m the process of trial and error.\n",
      "The main idea of reinforcement learning is to maximize the reward.\n",
      "The reward is a scalar value that is assigned to each state-action pair.\n",
      "The goal of the agent is to maximize the total reward.\n",
      "The agent is a software program that controls the robot.\n",
      "The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91mthe process of trial and error. The main idea of reinforcement learning is to maximize the reward. The reward is a scalar value that is assigned to each state-action pair. The goal of the agent is to maximize the total reward. The agent is a software program that controls the robot.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m potentially break many of the existing cryptographic protocols.\n",
      "However, there are also potential benefits to quantum computing for cryptography. For example, quantum algorithms could be used to develop new and more secure cryptographic protocols that are resistant to quantum attacks. Additionally, quantum computing could be used to improve the efficiency of existing cryptographic protocols, making them more resistant to attack.\n",
      "Overall, the impact of quantum computing on cryptography is still uncertain. However, it is clear that the two fields are closely linked and that further research is needed to understand the full implications of quantum computing for cryptography.\n",
      "The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols.\n",
      "The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols. The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols.\n",
      "The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols. The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols. The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols. The impact of quantum computing on cryptography is still being studied and debated. Some experts believe that quantum computing could potentially break many of the existing cryptographic protocols, while others believe that quantum computing could be used to develop new and more secure cryptographic protocols\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mpotentially break many of the existing cryptographic protocols. However, there are also potential benefits to quantum computing for cryptography. For example, quantum algorithms could be used to develop new and more secure cryptographic protocols that are resistant to quantum attacks. Additionally, quantum computing could be used to improve the efficiency of existing cryptographic protocols, making them more resistant to attack. Overall, the impact of quantum computing on cryptography is still uncertain. However, it is clear that the two fields are closely linked and that further research is needed to understand the full implications of quantum computing for cryptography. The impact of quantum computing on cryptography is still being studied and debated.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m can transform industries and improve our lives.\n",
      "The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, it’s clear that AI will have a significant impact on our lives in the years to come. From improving healthcare to enhancing security, AI is poised to revolutionize many industries and make our lives easier. As we look to the future, it’s important to stay informed and engaged in the latest developments in AI, so we can all benefit from the incredible potential of this technology.\n",
      "Previous articleHow to Choose the Best AI Writing Assistant for Your Needs\n",
      "Next articleHow to Use AI to Improve Your Business’s Marketing Strategy\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy\n",
      "How to Choose the Best AI Writing Assistant for Your Needs\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy - Tech Travel Hub February 28, 2023 At 12:00 pm\n",
      "[…] How AI is Revolutionizing the Future of Healthcare […]\n",
      "How to Choose the Best AI Writing Assistant for Your Needs - Tech Travel Hub March 1, 2023 At 12:00 pm\n",
      "[…] How AI is Revolutionizing the Future of Healthcare […]\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy - Tech Travel Hub March 2, 2023 At 12:00 pm\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy - Tech Travel Hub March 10, 2023 At 12:00 pm\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy - Tech Travel Hub March 11, 2023 At 1:00 pm\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy - Tech Travel Hub March 12, 2023 At 12:00 pm\n",
      "How to Use AI to Improve Your Business’s Marketing Strategy - Tech Travel Hub April 1,\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mcan transform industries and improve our lives. The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, it’s clear that AI will have a significant impact on our lives in the years to come. From improving healthcare to enhancing security, AI is poised to revolutionize many industries and make our lives easier. As we look to the future, it’s important to stay informed and engaged in the latest developments in AI, so we can all benefit from the incredible potential of this technology.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m only using a small amount of data.\n",
      "\n",
      "\\begin{figure}[t]\n",
      "\\centering\n",
      "\\includegraphics[width=0.9\\linewidth]{figures/overview.pdf}\n",
      "\\caption{Overview of the proposed method. We first train a model on a large dataset and then fine-tune it on a small dataset. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data. The fine-tuning is done by using the model's parameters and the small dataset's data.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91monly using a small amount of data. \\begin{figure}[t]\n",
      "\\centering\n",
      "\\includegraphics[width=0. 9\\linewidth]{figures/overview. pdf}\n",
      "\\caption{Overview of the proposed method. We first train a model on a large dataset and then fine-tune it on a small dataset. The fine-tuning is done by using the model's parameters and the small dataset's data.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m increase trust and transparency in AI systems.\n",
      "One of the key challenges in XAI is the need to explain the decision-making process of a model in a way that is understandable to humans. This requires the ability to identify the key features that the model has learned to use in making its decisions, and to provide a rationale for why those features are important.\n",
      "One approach to XAI is to use visualization techniques to show how the model is making its decisions. For example, a model may be trained to recognize objects in an image, and a visualization tool can be used to show how the model is identifying different objects in the image.\n",
      "Another approach to XAI is to use interpretability techniques to identify the key features that the model has learned to use in making its decisions. For example, a model may be trained to recognize handwritten digits, and an interpretability tool can be used to identify the key features that the model is using to recognize different digits.\n",
      "There are several different interpretability techniques that can be used to explain the decision-making process of a model. Some of the most commonly used techniques include:\n",
      "1. LIME: LIME (Local Interpretable Model Explanations) is a technique that allows you to explain the decision-making process of a model by showing how the model is making its predictions for a specific input.\n",
      "2. SHAP: SHAP (SHapley Additive exPlanations) is a technique that allows you to explain the decision-making process of a model by showing how the model is making its predictions for a specific input.\n",
      "3. Explainable AI: Explainable AI (XAI) is a field of study that aims to explain the decision-making process of a model in a way that is understandable to humans.\n",
      "4. Explainable AI: Explainable AI (XAI) is a field of study that aims to explain the decision-making process of a model in a way that is understandable to humans. By providing insights into how models make decisions, XAI aims to increase trust and transparency in AI systems.\n",
      "5. Explainable AI: Explainable AI (XAI) is a field of study that aims to explain the decision-making process of a model in a way that is understandable to humans. By providing insights into how models make decisions, X\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mincrease trust and transparency in AI systems. One of the key challenges in XAI is the need to explain the decision-making process of a model in a way that is understandable to humans. This requires the ability to identify the key features that the model has learned to use in making its decisions, and to provide a rationale for why those features are important. One approach to XAI is to use visualization techniques to show how the model is making its decisions. For example, a model may be trained to recognize objects in an image, and a visualization tool can be used to show how the model is identifying different objects in the image. Another approach to XAI is to use interpretability techniques to identify the key features that the model has learned to use in making its decisions. For example, a model may be trained to recognize handwritten digits, and an interpretability tool can be used to identify the key features that the model is using to recognize different digits. There are several different interpretability techniques that can be used to explain the decision-making process of a model. Some of the most commonly used techniques include:\n",
      "1. LIME: LIME (Local Interpretable Model Explanations) is a technique that allows you to explain the decision-making process of a model by showing how the model is making its predictions for a specific input. 2. SHAP: SHAP (SHapley Additive exPlanations) is a technique that allows you to explain the decision-making process of a model by showing how the model is making its predictions for a specific input. 3. Explainable AI: Explainable AI (XAI) is a field of study that aims to explain the decision-making process of a model in a way that is understandable to humans. 4. By providing insights into how models make decisions, XAI aims to increase trust and transparency in AI systems. 5.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun slowly disappears into the horizon.\n",
      "The beach is a great place to relax and enjoy the view, but there are also plenty of activities to keep you busy. You can go for a swim, build a sandcastle, or just lay on the beach and soak up the sun.\n",
      "If you’re looking for a more active experience, you can go for a walk or a run on the beach. The sand is soft and the air is fresh, making it a great place to get some exercise.\n",
      "No matter what you’re looking for, the beach is a great place to spend a day. It’s a great way to relax and enjoy the view, or to get some exercise.\n",
      "1 What is the best beach in the world?\n",
      "2 What is the most beautiful beach in the world?\n",
      "3 What is the most beautiful beach in the world 2022?\n",
      "4 What is the most beautiful beach in the world 2021?\n",
      "5 What is the most beautiful beach in the world 2020?\n",
      "6 What is the most beautiful beach in the world 2019?\n",
      "7 What is the most beautiful beach in the world 2018?\n",
      "What is the best beach in the world?\n",
      "There are so many beautiful beaches in the world, it’s hard to choose just one as the best. However, there are a few that stand out from the rest.\n",
      "One of the most popular beaches in the world is Pink Sands Beach in the Bahamas. This beach is famous for its pink sand, which is said to be the result of the coral reefs that are nearby. The water is also very clear, making it a great place to swim.\n",
      "Another popular beach is Waikiki Beach in Hawaii. This beach is known for its long stretch of white sand and its clear, blue water. It’s also a great place to surf.\n",
      "If you’re looking for a more secluded beach, you might want to check out Whitehaven Beach in Australia. This beach is located on a small island and is only accessible by boat. It’s known for its white sand and clear, blue water.\n",
      "See also What Is The Best Beach In The World\n",
      "If you’re looking for a beach with a lot of activities, you might want to check out Cable Beach in Australia. This beach is located in the city of\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun slowly disappears into the horizon. The beach is a great place to relax and enjoy the view, but there are also plenty of activities to keep you busy. You can go for a swim, build a sandcastle, or just lay on the beach and soak up the sun. If you’re looking for a more active experience, you can go for a walk or a run on the beach. The sand is soft and the air is fresh, making it a great place to get some exercise. No matter what you’re looking for, the beach is a great place to spend a day. It’s a great way to relax and enjoy the view, or to get some exercise. 1 What is the best beach in the world?\n",
      "2 What is the most beautiful beach in the world?\n",
      "3 What is the most beautiful beach in the world 2022?\n",
      "4 What is the most beautiful beach in the world 2021?\n",
      "5 What is the most beautiful beach in the world 2020?\n",
      "6 What is the most beautiful beach in the world 2019?\n",
      "7 What is the most beautiful beach in the world 2018?\n",
      "What is the best beach in the world?\n",
      "There are so many beautiful beaches in the world, it’s hard to choose just one as the best. However, there are a few that stand out from the rest. One of the most popular beaches in the world is Pink Sands Beach in the Bahamas. This beach is famous for its pink sand, which is said to be the result of the coral reefs that are nearby. The water is also very clear, making it a great place to swim. Another popular beach is Waikiki Beach in Hawaii. This beach is known for its long stretch of white sand and its clear, blue water. It’s also a great place to surf. If you’re looking for a more secluded beach, you might want to check out Whitehaven Beach in Australia. This beach is located on a small island and is only accessible by boat. It’s known for its white sand and clear, blue water. See also What Is The Best Beach In The World\n",
      "If you’re looking for a beach with a lot of activities, you might want to check out Cable Beach in Australia.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "Traveling can also help you develop new skills and knowledge. By learning about different cultures, you can gain a better understanding of the world and how it works. This can help you become more successful in your career and life.\n",
      "Traveling can also help you gain a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware.\n",
      "Traveling can also help you gain a better understanding of the environment. By visiting different places, you can learn about the natural world and how to protect it. This can help you become more environmentally conscious and responsible.\n",
      "Traveling can also help you gain a better understanding of history. By visiting different places, you can learn about the past and how it has shaped the world today. This can help you become more informed and knowledgeable about the world.\n",
      "Traveling can also help you gain a better understanding of the future. By visiting different places, you can learn about the future and how it will affect the world. This can help you become more prepared and informed about the future.\n",
      "Traveling can also help you gain a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware.\n",
      "Traveling can also help you gain a better understanding of the environment. By visiting different places, you can learn about the natural world and how to protect it. This can help you become more environmentally conscious and responsible.\n",
      "Traveling can also help you gain a better understanding of history. By visiting different places, you can learn about the past and how it has shaped the world today. This can help you become more informed and knowledgeable about the world.\n",
      "Traveling can also help you gain a better understanding of the future. By visiting different places, you can learn about the future and how it will affect the world. This can help you become more prepared and informed about the future.\n",
      "Traveling can also help you gain a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware.\n",
      "Traveling can also help you gain a better understanding of the environment. By visiting different places, you can learn about the natural world and how to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. Traveling can also help you develop new skills and knowledge. By learning about different cultures, you can gain a better understanding of the world and how it works. This can help you become more successful in your career and life. Traveling can also help you gain a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware. Traveling can also help you gain a better understanding of the environment. By visiting different places, you can learn about the natural world and how to protect it. This can help you become more environmentally conscious and responsible. Traveling can also help you gain a better understanding of history. By visiting different places, you can learn about the past and how it has shaped the world today. This can help you become more informed and knowledgeable about the world. Traveling can also help you gain a better understanding of the future. By visiting different places, you can learn about the future and how it will affect the world. This can help you become more prepared and informed about the future.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.514047488\n",
      "Model size after pruning:  13.514047488\n",
      "Model size after pruning:  13.513998336\n",
      "Model size after pruning:  13.513998336\n",
      "Model size after pruning:  13.513728\n",
      "Model size after pruning:  13.513728\n",
      "Model size after pruning:  13.513678848\n",
      "Model size after pruning:  13.513678848\n",
      "Model size after pruning:  13.513580544\n",
      "Model size after pruning:  13.513580544\n",
      "Model size after pruning:  13.513334784\n",
      "Model size after pruning:  13.513334784\n",
      "Model size after pruning:  13.513187328\n",
      "Model size after pruning:  13.513187328\n",
      "Model size after pruning:  13.513064448\n",
      "Model size after pruning:  13.513064448\n",
      "Model size after pruning:  13.513039872\n",
      "Model size after pruning:  13.513039872\n",
      "Model size after pruning:  13.513015296\n",
      "Model size after pruning:  13.513015296\n",
      "Model size after pruning:  13.512966144\n",
      "Model size after pruning:  13.512966144\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512843264\n",
      "Model size after pruning:  13.512843264\n",
      "Model size after pruning:  13.512818688\n",
      "Model size after pruning:  13.512818688\n",
      "Model size after pruning:  13.47895296\n",
      "Model size after pruning:  13.453787136\n",
      "Model size after pruning:  13.399670784\n",
      "Model size after pruning:  13.37450496\n",
      "Model size after pruning:  13.320388608\n",
      "Model size after pruning:  13.295222784\n",
      "Model size after pruning:  13.241106432\n",
      "Model size after pruning:  13.215940608\n",
      "Model size after pruning:  13.161824256\n",
      "Model size after pruning:  13.136658432\n",
      "Model size after pruning:  13.08254208\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
      "          (down_proj): Linear(in_features=10998, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11000, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11000, bias=False)\n",
      "          (down_proj): Linear(in_features=11000, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10997, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10997, bias=False)\n",
      "          (down_proj): Linear(in_features=10997, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
      "          (down_proj): Linear(in_features=10998, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
      "          (down_proj): Linear(in_features=11002, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11003, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11003, bias=False)\n",
      "          (down_proj): Linear(in_features=11003, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15-16): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9630, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9630, bias=False)\n",
      "          (down_proj): Linear(in_features=9630, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23-27): 5 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
      "          (down_proj): Linear(in_features=8806, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "13.08254208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32, 128]' is invalid for input of size 13312",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32, 128]' is invalid for input of size 13312"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51de77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T19:47:07.755799Z",
     "start_time": "2024-05-19T19:47:07.747125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c83364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6eae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c1be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9533c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T05:21:12.307962Z",
     "start_time": "2024-05-21T05:21:12.304916Z"
    }
   },
   "source": [
    "## Experiment #2 --> PreserveRatio=0.95, lbound=0.8, rbound=1.0 (changes reward fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a871c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:30:38.203487Z",
     "start_time": "2024-05-21T06:30:38.199992Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_1.0_base_2396.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95656da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:30:39.118163Z",
     "start_time": "2024-05-21T06:30:39.115062Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f57d3f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:45:57.427747Z",
     "start_time": "2024-05-21T06:30:40.092678Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  4096\n",
      "67 ==>  11007\n",
      "75 ==>  4096\n",
      "81 ==>  11007\n",
      "89 ==>  4096\n",
      "95 ==>  11005\n",
      "103 ==>  4096\n",
      "109 ==>  11007\n",
      "117 ==>  4096\n",
      "123 ==>  11008\n",
      "131 ==>  4096\n",
      "137 ==>  11008\n",
      "145 ==>  4096\n",
      "151 ==>  11004\n",
      "159 ==>  4096\n",
      "165 ==>  11006\n",
      "173 ==>  4096\n",
      "179 ==>  11002\n",
      "187 ==>  4096\n",
      "193 ==>  11007\n",
      "201 ==>  4096\n",
      "207 ==>  11008\n",
      "215 ==>  4096\n",
      "221 ==>  11006\n",
      "229 ==>  4096\n",
      "235 ==>  11008\n",
      "243 ==>  4096\n",
      "249 ==>  11008\n",
      "257 ==>  4096\n",
      "263 ==>  11004\n",
      "271 ==>  4096\n",
      "277 ==>  11007\n",
      "285 ==>  4096\n",
      "291 ==>  11006\n",
      "299 ==>  4096\n",
      "305 ==>  11006\n",
      "313 ==>  4096\n",
      "319 ==>  9577\n",
      "327 ==>  3328\n",
      "333 ==>  8806\n",
      "341 ==>  3328\n",
      "347 ==>  8806\n",
      "355 ==>  3328\n",
      "361 ==>  8806\n",
      "369 ==>  3328\n",
      "375 ==>  8806\n",
      "383 ==>  3328\n",
      "389 ==>  8806\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:12,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:12,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:01<00:12,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:02<00:11,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:03<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:08,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:05<00:08,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:08<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:09<00:04,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:10<00:03,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:11<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:12<00:01,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:13<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:14<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  6.0593366622924805\n",
      "Perplexity on wikitext2:  6.0593366622924805\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:12:02:27,428 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:02:27,430 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:02:32,848 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:02:32,850 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:12:03:16,883 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:12:03:16,884 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:12:03:16,898 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:12:03:16,899 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3cc7d189694cbfbd0a0a715b52749d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:12:03:42,639 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:03:42,640 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:03:58,287 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:03:58,288 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:04:08,741 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:12:04:08,743 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:12:04:08,743 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:12:04:08,744 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:12:04:08,745 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:12:04:08,745 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:12:04:08,746 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:12:04:08,751 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 62303.98it/s]\n",
      "2024-05-21:12:04:08,761 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1516.18it/s]\n",
      "2024-05-21:12:04:08,835 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 866.67it/s]\n",
      "2024-05-21:12:04:08,962 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1670.75it/s]\n",
      "2024-05-21:12:04:09,034 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 826.85it/s]\n",
      "2024-05-21:12:04:09,168 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1499.64it/s]\n",
      "2024-05-21:12:04:09,244 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2010.59it/s]\n",
      "2024-05-21:12:04:09,319 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [01:53<00:00, 19.31it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|   winogrande  |    0.69   | 0.04648231987117316  |      None      |          None         |\n",
      "|      rte      |    0.72   | 0.045126085985421276 |      None      |          None         |\n",
      "| arc_challenge |    0.42   | 0.04960449637488584  |      0.43      |  0.049756985195624284 |\n",
      "|   openbookqa  |    0.31   | 0.04648231987117316  |      0.44      |  0.04988876515698589  |\n",
      "|    arc_easy   |    0.7    | 0.046056618647183814 |      0.76      |  0.04292346959909284  |\n",
      "|     boolq     |    0.74   | 0.04408440022768078  |      None      |          None         |\n",
      "|   hellaswag   |    0.53   | 0.050161355804659205 |      0.68      |  0.04688261722621504  |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "Gardening is a great way to get outside and enjoy the fresh air. It’s also a great way to get some exercise.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\n",
      "Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress. Gardening is a great way to get some\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. Gardening is a great way to get outside and enjoy the fresh air. It’s also a great way to get some exercise. Gardening is a great way to get some fresh air and exercise. It’s also a great way to relax and de-stress.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of connection and community.\n",
      "Family traditions can also be a source of pride and identity. They can be a way to showcase a family's culture, values, and beliefs. For example, a family may have a tradition of celebrating a particular holiday in a specific way, such as a special meal or a cultural dance.\n",
      "Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of fun and entertainment. They can be a way to bring people together and create lasting memories. They can also be a way to celebrate special occasions and milestones.\n",
      "Family traditions can also be a source of inspiration. They can be a way to explore new ideas and experiences. They can also be a way to challenge assumptions and explore different perspectives.\n",
      "Family traditions can also be a source of strength. They can be a way to support each other through difficult times. They can also be a way to celebrate successes and achievements.\n",
      "Family traditions can also be a source of unity. They can be a way to bring people together and create a sense of belonging. They can also be a way to celebrate diversity and respect different perspectives.\n",
      "Family traditions can also be a source of wisdom. They can be a way to learn from the past and apply it to the present. They can also be a way to teach children about the importance of family and the value of tradition.\n",
      "Family traditions can also be a source of joy. They can be a way to celebrate special occasions and milestones. They can also be a way to create lasting memories and relationships.\n",
      "Family traditions can also be a source of comfort. They can be a way to provide a sense of stability and predictability in an ever-changing world. They can also be a way to showcase a family's culture, values, and beliefs.\n",
      "Family traditions can also be a source of strength. They can be a way to support each other through difficult times. They can also be a way to celebrate successes and achievements.\n",
      "Family traditions can also be a source of unity. They can be a way to bring people together and create a sense of belonging\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of connection and community. Family traditions can also be a source of pride and identity. They can be a way to showcase a family's culture, values, and beliefs. For example, a family may have a tradition of celebrating a particular holiday in a specific way, such as a special meal or a cultural dance. Family traditions can also be a source of comfort and security. They can provide a sense of stability and predictability in an ever-changing world. They can also be a way to teach children about the importance of family and the value of tradition. Family traditions can also be a source of fun and entertainment. They can be a way to bring people together and create lasting memories. They can also be a way to celebrate special occasions and milestones. Family traditions can also be a source of inspiration. They can be a way to explore new ideas and experiences. They can also be a way to challenge assumptions and explore different perspectives. Family traditions can also be a source of strength. They can be a way to support each other through difficult times. They can also be a way to celebrate successes and achievements. Family traditions can also be a source of unity. They can be a way to bring people together and create a sense of belonging. They can also be a way to celebrate diversity and respect different perspectives. Family traditions can also be a source of wisdom. They can be a way to learn from the past and apply it to the present. Family traditions can also be a source of joy. They can be a way to celebrate special occasions and milestones. They can also be a way to create lasting memories and relationships. Family traditions can also be a source of comfort. They can be a way to provide a sense of stability and predictability in an ever-changing world.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion may even become more accessible and affordable, allowing everyone to participate in the creative process.\n",
      "The fashion industry is a complex and multifaceted industry that encompasses a wide range of activities, including design, production, marketing, and retail. The industry is driven by consumer demand for new and innovative styles, and it is constantly evolving to meet the changing needs of consumers.\n",
      "The fashion industry is a global industry, with major markets in Europe, the United States, and Asia. It is a highly competitive industry, with many companies vying for market share. The industry is also highly fragmented, with many small and independent companies operating in niche markets.\n",
      "The fashion industry is a major contributor to the global economy, with an estimated value of over $2.4 trillion in 2019. It employs millions of people around the world, from designers to models to retail workers. The industry is also a major source of revenue for many countries, with France, Italy, and the United States being some of the largest exporters of fashion products.\n",
      "The fashion industry is a complex and multifaceted industry that encompasses a wide range of activities, including design, production, marketing, and retail. The industry is driven by consumer demand for new and innovative styles, and it is constantly evolving to meet the changing needs of consumers. The industry is a major contributor to the global economy, employing millions of people around the world and generating significant revenue for many countries. As the industry continues to evolve, it is likely to remain a significant player in the global economy for many years to come.\n",
      "The fashion industry is a highly competitive and dynamic industry that is constantly evolving to meet the changing needs of consumers. With the rise of social media and the internet, the industry has become even more globalized, allowing for greater access to new styles and trends from around the world.\n",
      "The fashion industry is a major contributor to the global economy, with an estimated value of over $2.4 trillion in 2019. It employs millions of people around the world, from designers to models to retail workers. The industry is also a major source of revenue for many countries, with France, Italy, and the United States being some of the largest exporters of fashion products.\n",
      "The\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion may even become more accessible and affordable, allowing everyone to participate in the creative process. The fashion industry is a complex and multifaceted industry that encompasses a wide range of activities, including design, production, marketing, and retail. The industry is driven by consumer demand for new and innovative styles, and it is constantly evolving to meet the changing needs of consumers. The fashion industry is a global industry, with major markets in Europe, the United States, and Asia. It is a highly competitive industry, with many companies vying for market share. The industry is also highly fragmented, with many small and independent companies operating in niche markets. The fashion industry is a major contributor to the global economy, with an estimated value of over $2. 4 trillion in 2019. It employs millions of people around the world, from designers to models to retail workers. The industry is also a major source of revenue for many countries, with France, Italy, and the United States being some of the largest exporters of fashion products. The industry is a major contributor to the global economy, employing millions of people around the world and generating significant revenue for many countries. As the industry continues to evolve, it is likely to remain a significant player in the global economy for many years to come. The fashion industry is a highly competitive and dynamic industry that is constantly evolving to meet the changing needs of consumers. With the rise of social media and the internet, the industry has become even more globalized, allowing for greater access to new styles and trends from around the world.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m hospitals to improve patient care, reduce costs, and increase efficiency.\n",
      "One of the most significant ways that AI is being used in healthcare is through the development of medical imaging software. This software is able to analyze medical images, such as X-rays and MRIs, and identify potential abnormalities that may not be visible to the human eye. This technology is particularly useful in the early detection of cancer and other diseases, as it can help doctors to identify potential problems before they become more serious.\n",
      "Another way that AI is being used in healthcare is through the development of virtual assistants. These virtual assistants are able to answer patient’s questions, provide information about their condition, and even schedule appointments. This technology is particularly useful for patients who may not have access to a doctor or who may not be able to communicate in person.\n",
      "AI is also being used to improve the accuracy of diagnosis. By analyzing large amounts of data, AI can identify patterns and trends that may not be apparent to human doctors. This technology is particularly useful in the diagnosis of rare diseases, as it can help doctors to identify potential problems more quickly and accurately.\n",
      "Finally, AI is being used to improve the efficiency of healthcare systems. By automating routine tasks, AI can free up healthcare workers to focus on more complex tasks. This technology is particularly useful in the management of patient records, as it can help to ensure that all information is accurate and up-to-date.\n",
      "Overall, AI is having a significant impact on the healthcare industry. By enabling hospitals to improve patient care, reduce costs, and increase efficiency, AI is helping to make healthcare more accessible and affordable for everyone.\n",
      "The Impact of Artificial Intelligence on the Healthcare Industry\n",
      "The healthcare industry is undergoing a revolutionary transformation with the introduction of artificial intelligence (AI). AI is a technology that enables machines to learn, understand, and make decisions on their own. It is being used in a variety of ways to improve the quality of care and reduce costs in the healthcare industry.\n",
      "One of the most significant ways that AI is being used in healthcare is through the development of medical imaging software. This software is able to analyze medical images, such as X-rays and MRIs, and identify potential abnormalities that may not be visible to the human\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mhospitals to improve patient care, reduce costs, and increase efficiency. One of the most significant ways that AI is being used in healthcare is through the development of medical imaging software. This software is able to analyze medical images, such as X-rays and MRIs, and identify potential abnormalities that may not be visible to the human eye. This technology is particularly useful in the early detection of cancer and other diseases, as it can help doctors to identify potential problems before they become more serious. Another way that AI is being used in healthcare is through the development of virtual assistants. These virtual assistants are able to answer patient’s questions, provide information about their condition, and even schedule appointments. This technology is particularly useful for patients who may not have access to a doctor or who may not be able to communicate in person. AI is also being used to improve the accuracy of diagnosis. By analyzing large amounts of data, AI can identify patterns and trends that may not be apparent to human doctors. This technology is particularly useful in the diagnosis of rare diseases, as it can help doctors to identify potential problems more quickly and accurately. Finally, AI is being used to improve the efficiency of healthcare systems. By automating routine tasks, AI can free up healthcare workers to focus on more complex tasks. This technology is particularly useful in the management of patient records, as it can help to ensure that all information is accurate and up-to-date. Overall, AI is having a significant impact on the healthcare industry. By enabling hospitals to improve patient care, reduce costs, and increase efficiency, AI is helping to make healthcare more accessible and affordable for everyone. The Impact of Artificial Intelligence on the Healthcare Industry\n",
      "The healthcare industry is undergoing a revolutionary transformation with the introduction of artificial intelligence (AI). AI is a technology that enables machines to learn, understand, and make decisions on their own. It is being used in a variety of ways to improve the quality of care and reduce costs in the healthcare industry.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m generate predictions about future price movements.\n",
      "One of the most popular machine learning algorithms for stock market prediction is the support vector machine (SVM). SVMs are a type of supervised learning algorithm that can be used to classify data points into different categories. In the context of stock market prediction, SVMs can be used to predict whether a stock will rise or fall in price over a certain period of time.\n",
      "Another popular machine learning algorithm for stock market prediction is the neural network. Neural networks are a type of deep learning algorithm that can be used to identify patterns in large datasets. In the context of stock market prediction, neural networks can be used to identify patterns in historical data that may indicate future price movements.\n",
      "Machine learning algorithms are also being used to generate trading signals. By analyzing historical data and identifying patterns, these models can generate signals that indicate when it is a good time to buy or sell a stock.\n",
      "In conclusion, machine learning algorithms are being increasingly used to predict stock market trends and generate trading signals. By analyzing historical data and identifying patterns, these models can generate predictions about future price movements and generate trading signals that can be used to make profitable trades.\n",
      "The use of machine learning algorithms in the stock market has been a topic of much debate in recent years. While some argue that these algorithms are a valuable tool for investors, others believe that they are a threat to the integrity of the market. In this article, we will explore the pros and cons of using machine learning algorithms in the stock market.\n",
      "One of the main advantages of using machine learning algorithms in the stock market is that they can help investors to make more informed decisions. By analyzing large amounts of data, these algorithms can identify patterns and trends that may not be apparent to human investors. This can help investors to make more accurate predictions about the future performance of a stock.\n",
      "Another advantage of using machine learning algorithms in the stock market is that they can help to reduce the risk of making bad investment decisions. By analyzing large amounts of data, these algorithms can identify patterns and trends that may not be apparent to human investors. This can help investors to avoid making bad investment decisions that could lead to significant losses.\n",
      "However, there are also some potential drawbacks to using machine learning algorithms in the stock market. One of the main concerns is that these algorithms may not be able to accurately predict the future performance\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mgenerate predictions about future price movements. One of the most popular machine learning algorithms for stock market prediction is the support vector machine (SVM). SVMs are a type of supervised learning algorithm that can be used to classify data points into different categories. In the context of stock market prediction, SVMs can be used to predict whether a stock will rise or fall in price over a certain period of time. Another popular machine learning algorithm for stock market prediction is the neural network. Neural networks are a type of deep learning algorithm that can be used to identify patterns in large datasets. In the context of stock market prediction, neural networks can be used to identify patterns in historical data that may indicate future price movements. Machine learning algorithms are also being used to generate trading signals. By analyzing historical data and identifying patterns, these models can generate signals that indicate when it is a good time to buy or sell a stock. In conclusion, machine learning algorithms are being increasingly used to predict stock market trends and generate trading signals. By analyzing historical data and identifying patterns, these models can generate predictions about future price movements and generate trading signals that can be used to make profitable trades. The use of machine learning algorithms in the stock market has been a topic of much debate in recent years. While some argue that these algorithms are a valuable tool for investors, others believe that they are a threat to the integrity of the market. In this article, we will explore the pros and cons of using machine learning algorithms in the stock market. One of the main advantages of using machine learning algorithms in the stock market is that they can help investors to make more informed decisions. By analyzing large amounts of data, these algorithms can identify patterns and trends that may not be apparent to human investors. This can help investors to make more accurate predictions about the future performance of a stock. Another advantage of using machine learning algorithms in the stock market is that they can help to reduce the risk of making bad investment decisions. This can help investors to avoid making bad investment decisions that could lead to significant losses. However, there are also some potential drawbacks to using machine learning algorithms in the stock market.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will become even more powerful and intuitive, enabling us to interact with computers in a more natural and intuitive way.\n",
      "The future of human-computer interaction is bright, with new technologies emerging every day that are changing the way we interact with computers. From virtual reality to augmented reality, from natural language processing to machine learning, these technologies are transforming the way we work, play, and communicate. As these technologies continue to evolve, we can expect that they will become even more powerful and intuitive, enabling us to interact with computers in a more natural and intuitive way.\n",
      "Previous articleThe Future of Human-Computer Interaction: How AI and Machine Learning are Changing the Way We Work\n",
      "Next articleThe Future of Human-Computer Interaction: How Virtual Reality and Augmented Reality are Changing the Way We Experience the World\n",
      "The Future of Human-Computer Interaction: How Virtual Reality and Augmented Reality are Changing the Way We Experience the World\n",
      "The Future of Human-Computer Interaction: How AI and Machine Learning are Changing the Way We Work\n",
      "The Future of Human-Computer Interaction: How Virtual Reality and Augmented Reality are Changing the Way We Experience the World The future of human-computer interaction is bright, with new technologies emerging every day that are changing the way we interact with computers. From virtual reality to augmented reality, from natural language processing to machine learning, these technologies are transforming the way we work, play, and communicate. As these technologies continue to evolve, we can expect that they will become even more powerful and intuitive, enabling us to interact with computers in a more natural and intuitive way.\n",
      "The Future of Human-Computer Interaction: How AI and Machine Learning are Changing the Way We Work The future of human-computer interaction is bright, with new technologies emerging every day that are changing the way we interact with computers. From virtual reality to augmented reality, from natural language processing to machine learning, these technologies are transforming the way we work, play, and communicate. As these technologies continue to evolve, we can expect that they will become even more powerful and intuitive, enabling us to interact with computers in a more natural and intuitive way.\n",
      "The Future of Human-Computer Interaction: How Virtual Reality\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will become even more powerful and intuitive, enabling us to interact with computers in a more natural and intuitive way. The future of human-computer interaction is bright, with new technologies emerging every day that are changing the way we interact with computers. From virtual reality to augmented reality, from natural language processing to machine learning, these technologies are transforming the way we work, play, and communicate. As these technologies continue to evolve, we can expect that they will become even more powerful and intuitive, enabling us to interact with computers in a more natural and intuitive way. Previous articleThe Future of Human-Computer Interaction: How AI and Machine Learning are Changing the Way We Work\n",
      "Next articleThe Future of Human-Computer Interaction: How Virtual Reality and Augmented Reality are Changing the Way We Experience the World\n",
      "The Future of Human-Computer Interaction: How Virtual Reality and Augmented Reality are Changing the Way We Experience the World\n",
      "The Future of Human-Computer Interaction: How AI and Machine Learning are Changing the Way We Work\n",
      "The Future of Human-Computer Interaction: How Virtual Reality and Augmented Reality are Changing the Way We Experience the World The future of human-computer interaction is bright, with new technologies emerging every day that are changing the way we interact with computers. The Future of Human-Computer Interaction: How AI and Machine Learning are Changing the Way We Work The future of human-computer interaction is bright, with new technologies emerging every day that are changing the way we interact with computers.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m safely and efficiently.\n",
      "The use of AI in self-driving cars has several advantages. First, it allows the car to make decisions quickly and accurately, based on real-time data from its sensors. This means that the car can avoid potential hazards and navigate safely in a variety of situations.\n",
      "Second, AI can be used to optimize the car’s route, taking into account factors such as traffic patterns, weather conditions, and road conditions. This can help the car to arrive at its destination more quickly and efficiently.\n",
      "Third, AI can be used to improve the car’s performance in difficult driving conditions. For example, it can be used to detect and avoid obstacles in low-light conditions, or to navigate through heavy traffic.\n",
      "Finally, AI can be used to improve the car’s safety features. For example, it can be used to detect and avoid potential hazards, such as pedestrians or other vehicles, and to respond quickly to any emergency situations.\n",
      "Overall, the use of AI in self-driving cars has the potential to revolutionize the way we travel. It can help to improve safety, efficiency, and convenience, and to reduce the environmental impact of transportation.\n",
      "The use of artificial intelligence (AI) in self-driving cars has been a topic of much debate in recent years. While some argue that AI is a necessary component of autonomous vehicles, others are concerned about the potential risks and ethical implications of such technology.\n",
      "One of the main arguments in favor of the use of AI in self-driving cars is that it can improve safety. AI-powered systems can detect and respond to potential hazards more quickly and accurately than a human driver. This could reduce the number of accidents caused by human error.\n",
      "Another argument in favor of the use of AI in self-driving cars is that it can reduce the environmental impact of transportation. AI-powered systems can optimize routes and reduce the amount of time spent in traffic, which could reduce emissions from cars.\n",
      "However, there are also concerns about the use of AI in self-driving cars. Some people are concerned that AI-powered systems could be vulnerable to hacking or malfunctioning, which could lead to accidents. Others are concerned about the potential for AI to be used for\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91msafely and efficiently. The use of AI in self-driving cars has several advantages. First, it allows the car to make decisions quickly and accurately, based on real-time data from its sensors. This means that the car can avoid potential hazards and navigate safely in a variety of situations. Second, AI can be used to optimize the car’s route, taking into account factors such as traffic patterns, weather conditions, and road conditions. This can help the car to arrive at its destination more quickly and efficiently. Third, AI can be used to improve the car’s performance in difficult driving conditions. For example, it can be used to detect and avoid obstacles in low-light conditions, or to navigate through heavy traffic. Finally, AI can be used to improve the car’s safety features. For example, it can be used to detect and avoid potential hazards, such as pedestrians or other vehicles, and to respond quickly to any emergency situations. Overall, the use of AI in self-driving cars has the potential to revolutionize the way we travel. It can help to improve safety, efficiency, and convenience, and to reduce the environmental impact of transportation. The use of artificial intelligence (AI) in self-driving cars has been a topic of much debate in recent years. While some argue that AI is a necessary component of autonomous vehicles, others are concerned about the potential risks and ethical implications of such technology. One of the main arguments in favor of the use of AI in self-driving cars is that it can improve safety. AI-powered systems can detect and respond to potential hazards more quickly and accurately than a human driver. This could reduce the number of accidents caused by human error. Another argument in favor of the use of AI in self-driving cars is that it can reduce the environmental impact of transportation. AI-powered systems can optimize routes and reduce the amount of time spent in traffic, which could reduce emissions from cars. However, there are also concerns about the use of AI in self-driving cars. Some people are concerned that AI-powered systems could be vulnerable to hacking or malfunctioning, which could lead to accidents.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m help ensure that AI systems are not discriminatory or biased in any way.\n",
      "One of the most important aspects of AI is its ability to learn and adapt. As more data becomes available, AI systems will be able to learn and improve over time. This will allow them to become more accurate and efficient in their tasks.\n",
      "AI is a rapidly growing field with a wide range of applications. From automating tasks to creating new products, AI is changing the way we live and work. As the technology continues to evolve, it is likely that AI will become even more powerful and widespread.\n",
      "AI is a rapidly growing field with a wide range of applications. From automating tasks to creating new products, AI is changing the way we live and work. As the technology continues to evolve, it is likely that AI will become even more powerful and widespread. With the right tools and resources, anyone can get started with AI.\n",
      "AI is a rapidly growing field with a wide range of applications. From automating tasks to creating new products, AI is changing the way we live and work. As the technology continues to evolve, it is likely that AI will become even more powerful and widespread. With the right tools and resources, anyone can get started with AI. By understanding the basics of AI, you can begin to explore the possibilities of this exciting technology.\n",
      "AI is a rapidly growing field with a wide range of applications. From automating tasks to creating new products, AI is changing the way we live and work. As the technology continues to evolve, it is likely that AI will become even more powerful and widespread. With the right tools and resources, anyone can get started with AI. By understanding the basics of AI, you can begin to explore the possibilities of this exciting technology.\n",
      "AI is a rapidly growing field with a wide range of applications. From automating tasks to creating new products, AI is changing the way we live and work. As the technology continues to evolve, it is likely that AI will become even more powerful and widespread. With the right tools and resources, anyone can get started with AI. By understanding the basics of AI, you can begin to explore the possibilities of this exciting technology. With the right tools and resources, anyone can get started with AI. By understanding the basics of A\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mhelp ensure that AI systems are not discriminatory or biased in any way. One of the most important aspects of AI is its ability to learn and adapt. As more data becomes available, AI systems will be able to learn and improve over time. This will allow them to become more accurate and efficient in their tasks. AI is a rapidly growing field with a wide range of applications. From automating tasks to creating new products, AI is changing the way we live and work. As the technology continues to evolve, it is likely that AI will become even more powerful and widespread. With the right tools and resources, anyone can get started with AI. By understanding the basics of AI, you can begin to explore the possibilities of this exciting technology.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can not only recognize objects but also understand their context and relationships, allowing them to make more informed decisions.\n",
      "In the field of natural language processing, AI has made significant strides in understanding and generating human language. By analyzing large datasets of text, AI systems can identify patterns and relationships between words and phrases, allowing them to generate text that is more coherent and contextually appropriate. This has significant applications in areas such as customer service, where AI can assist in answering customer queries and providing personalized recommendations. As the technology advances, we may soon see AI systems that can not only understand human language but also generate it in a more natural and human-like manner.\n",
      "In the field of robotics, AI has enabled the development of autonomous systems that can navigate and interact with their environment without human intervention. By leveraging computer vision and other sensory inputs, these systems can identify objects, navigate obstacles, and perform a range of tasks. This has significant applications in areas such as manufacturing, where AI-enabled robots can assist in assembly lines and warehouses. As the technology advances, we may soon see AI systems that can not only navigate and interact with their environment but also learn and adapt to new situations on their own.\n",
      "In conclusion, AI has the potential to transform a wide range of industries and applications, from healthcare to transportation to entertainment. As the technology continues to evolve, we can expect to see even more sophisticated and powerful AI systems that can not only perform specific tasks but also understand and interact with their environment in a more natural and intuitive manner. While there are still challenges to be addressed, the future of AI is promising, and we can expect to see significant advancements in the years to come.\n",
      "AI is a rapidly evolving field that is transforming the way we live and work. From self-driving cars to virtual assistants, AI is being integrated into our daily lives in a variety of ways. As the technology continues to develop, we can expect to see even more sophisticated and powerful AI systems that can not only perform specific tasks but also understand and interact with their environment in a more natural and intuitive manner. While there are still challenges to be addressed, the future of AI is promising, and we can expect to see significant advancements in the years to come.\n",
      "AI is a\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan not only recognize objects but also understand their context and relationships, allowing them to make more informed decisions. In the field of natural language processing, AI has made significant strides in understanding and generating human language. By analyzing large datasets of text, AI systems can identify patterns and relationships between words and phrases, allowing them to generate text that is more coherent and contextually appropriate. This has significant applications in areas such as customer service, where AI can assist in answering customer queries and providing personalized recommendations. As the technology advances, we may soon see AI systems that can not only understand human language but also generate it in a more natural and human-like manner. In the field of robotics, AI has enabled the development of autonomous systems that can navigate and interact with their environment without human intervention. By leveraging computer vision and other sensory inputs, these systems can identify objects, navigate obstacles, and perform a range of tasks. This has significant applications in areas such as manufacturing, where AI-enabled robots can assist in assembly lines and warehouses. As the technology advances, we may soon see AI systems that can not only navigate and interact with their environment but also learn and adapt to new situations on their own. In conclusion, AI has the potential to transform a wide range of industries and applications, from healthcare to transportation to entertainment. As the technology continues to evolve, we can expect to see even more sophisticated and powerful AI systems that can not only perform specific tasks but also understand and interact with their environment in a more natural and intuitive manner. While there are still challenges to be addressed, the future of AI is promising, and we can expect to see significant advancements in the years to come. AI is a rapidly evolving field that is transforming the way we live and work. From self-driving cars to virtual assistants, AI is being integrated into our daily lives in a variety of ways. As the technology continues to develop, we can expect to see even more sophisticated and powerful AI systems that can not only perform specific tasks but also understand and interact with their environment in a more natural and intuitive manner.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m the process of trial and error.\n",
      "The main idea of reinforcement learning is to maximize the reward.\n",
      "The reward is a scalar value that is assigned to each state-action pair.\n",
      "The goal of the agent is to maximize the total reward.\n",
      "The agent is a software program that controls the robot.\n",
      "The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software program that controls the robot. The agent is a software\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91mthe process of trial and error. The main idea of reinforcement learning is to maximize the reward. The reward is a scalar value that is assigned to each state-action pair. The goal of the agent is to maximize the total reward. The agent is a software program that controls the robot.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m potentially break many of the existing cryptographic protocols.\n",
      "However, there are also potential benefits to quantum computing for cryptography. For example, quantum-resistant cryptographic protocols could be developed to protect against attacks by quantum computers. Additionally, quantum computing could be used to develop new types of cryptographic algorithms that are more secure than those currently in use.\n",
      "Overall, the impact of quantum computing on cryptography is still uncertain. However, it is clear that the field of cryptography is likely to undergo significant changes in the coming years as a result of the development of quantum computing.\n",
      "The impact of quantum computing on cryptography is still being studied, but it is likely to be significant. Quantum computers are capable of performing calculations much faster than classical computers, which could potentially allow them to break many of the current cryptographic algorithms. However, there are also potential benefits to quantum computing for cryptography, such as the development of quantum-resistant cryptographic protocols.\n",
      "What is the impact of quantum computing on cryptography?\n",
      "The impact of quantum computing on cryptography is still being studied, but it is likely to be significant. Quantum computers are capable of performing calculations much faster than classical computers, which could potentially allow them to break many of the current cryptographic algorithms. However, there are also potential benefits to quantum computing for cryptography, such as the development of quantum-resistant cryptographic protocols.\n",
      "What are the potential benefits of quantum computing for cryptography?\n",
      "The potential benefits of quantum computing for cryptography include the development of quantum-resistant cryptographic protocols and the ability to solve complex problems much faster than classical computers.\n",
      "What are the potential risks of quantum computing for cryptography?\n",
      "The potential risks of quantum computing for cryptography include the possibility that quantum computers could be used to break many of the current cryptographic algorithms. Additionally, there is a risk that quantum computing could be used to develop new types of cryptographic algorithms that are more vulnerable to attack.\n",
      "What is the future of cryptography in the face of quantum computing?\n",
      "The future of cryptography in the face of quantum computing is uncertain. However, it is clear that the field of cryptography is likely to undergo significant changes in the coming years as a result of the development of quantum computing.\n",
      "What are some of the challenges that cryptography will face in the face of quantum computing?\n",
      "Some of the challenges that cryptography will face in the face of quantum computing include the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mpotentially break many of the existing cryptographic protocols. However, there are also potential benefits to quantum computing for cryptography. For example, quantum-resistant cryptographic protocols could be developed to protect against attacks by quantum computers. Additionally, quantum computing could be used to develop new types of cryptographic algorithms that are more secure than those currently in use. Overall, the impact of quantum computing on cryptography is still uncertain. However, it is clear that the field of cryptography is likely to undergo significant changes in the coming years as a result of the development of quantum computing. The impact of quantum computing on cryptography is still being studied, but it is likely to be significant. Quantum computers are capable of performing calculations much faster than classical computers, which could potentially allow them to break many of the current cryptographic algorithms. However, there are also potential benefits to quantum computing for cryptography, such as the development of quantum-resistant cryptographic protocols. What is the impact of quantum computing on cryptography?\n",
      "The impact of quantum computing on cryptography is still being studied, but it is likely to be significant. What are the potential benefits of quantum computing for cryptography?\n",
      "The potential benefits of quantum computing for cryptography include the development of quantum-resistant cryptographic protocols and the ability to solve complex problems much faster than classical computers. What are the potential risks of quantum computing for cryptography?\n",
      "The potential risks of quantum computing for cryptography include the possibility that quantum computers could be used to break many of the current cryptographic algorithms. Additionally, there is a risk that quantum computing could be used to develop new types of cryptographic algorithms that are more vulnerable to attack. What is the future of cryptography in the face of quantum computing?\n",
      "The future of cryptography in the face of quantum computing is uncertain.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m can transform industries and improve our lives.\n",
      "The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, we can expect to see even more impressive advancements in the years to come.\n",
      "Previous articleHow to Choose the Best AI Chatbot for Your Business\n",
      "Next articleHow to Choose the Best AI Chatbot for Your Business\n",
      "How to Choose the Best AI Chatbot for Your Business\n",
      "How to Choose the Best AI Chatbot for Your Business The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, we can expect to see even more impressive advancements in the years to come.\n",
      "How to Choose the Best AI Chatbot for Your Business The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, we can expect to see even more impressive advancements in the years to come.\n",
      "How to Choose the Best AI Chatbot for Your Business The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, we can expect to see even more impressive advancements in the years to come. The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that can transform industries and improve our lives.\n",
      "How to Choose the Best AI Chatbot for Your Business The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, we can expect to see even more impressive advancements in the years to come. The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mcan transform industries and improve our lives. The future of AI is bright, with new developments and applications emerging every day. As we continue to explore the potential of this technology, we can expect to see even more impressive advancements in the years to come. Previous articleHow to Choose the Best AI Chatbot for Your Business\n",
      "Next articleHow to Choose the Best AI Chatbot for Your Business\n",
      "How to Choose the Best AI Chatbot for Your Business\n",
      "How to Choose the Best AI Chatbot for Your Business The future of AI is bright, with new developments and applications emerging every day. How to Choose the Best AI Chatbot for Your Business The future of AI is bright, with new developments and applications emerging every day. The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that can transform industries and improve our lives.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m only using a small amount of data.\n",
      "\n",
      "\\begin{figure}[t]\n",
      "\\centering\n",
      "\\includegraphics[width=0.9\\linewidth]{figures/overview.pdf}\n",
      "\\caption{Overview of the proposed method. We first train a model on a large dataset and then fine-tune it on a small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-tuning is done by using the model's parameters and the small dataset. The fine-\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91monly using a small amount of data. \\begin{figure}[t]\n",
      "\\centering\n",
      "\\includegraphics[width=0. 9\\linewidth]{figures/overview. pdf}\n",
      "\\caption{Overview of the proposed method. We first train a model on a large dataset and then fine-tune it on a small dataset. The fine-tuning is done by using the model's parameters and the small dataset.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m increase trust and transparency in AI systems.\n",
      "One of the key challenges in XAI is the need to explain the decision-making process of a model in a way that is understandable to humans. This requires the ability to identify the key features that the model has learned to use in making its decisions, and to provide a clear explanation of how these features are used in the decision-making process.\n",
      "One approach to XAI is to use visualization techniques to provide insights into the decision-making process of a model. By visualizing the features that the model has learned to use, it is possible to gain insights into how the model is making its decisions. This can be particularly useful in cases where the model is making decisions that are difficult to understand or that are not aligned with human expectations.\n",
      "Another approach to XAI is to use interpretability techniques to provide insights into the decision-making process of a model. These techniques aim to identify the key features that the model has learned to use in making its decisions, and to provide a clear explanation of how these features are used in the decision-making process. This can be particularly useful in cases where the model is making decisions that are difficult to understand or that are not aligned with human expectations.\n",
      "In conclusion, XAI is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to increase trust and transparency in AI systems. Visualization and interpretability techniques are two key approaches to XAI that can be used to gain insights into the decision-making process of a model.\n",
      "The Role of Visualization and Interpretability in Explainable AI (XAI)\n",
      "Explainable AI (XAI) is a rapidly growing field that aims to provide a better understanding of how artificial intelligence (AI) systems make decisions. One of the key challenges in XAI is the need to explain the decision-making process of a model in a way that is understandable to humans. This requires the ability to identify the key features that the model has learned to use in making its decisions, and to provide a clear explanation of how these features are used in the decision-making process.\n",
      "One approach to XAI is to use visualization techniques to provide insights into the decision-making process of a model. By visualizing the features that the model has learned to use\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mincrease trust and transparency in AI systems. One of the key challenges in XAI is the need to explain the decision-making process of a model in a way that is understandable to humans. This requires the ability to identify the key features that the model has learned to use in making its decisions, and to provide a clear explanation of how these features are used in the decision-making process. One approach to XAI is to use visualization techniques to provide insights into the decision-making process of a model. By visualizing the features that the model has learned to use, it is possible to gain insights into how the model is making its decisions. This can be particularly useful in cases where the model is making decisions that are difficult to understand or that are not aligned with human expectations. Another approach to XAI is to use interpretability techniques to provide insights into the decision-making process of a model. These techniques aim to identify the key features that the model has learned to use in making its decisions, and to provide a clear explanation of how these features are used in the decision-making process. In conclusion, XAI is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to increase trust and transparency in AI systems. Visualization and interpretability techniques are two key approaches to XAI that can be used to gain insights into the decision-making process of a model. The Role of Visualization and Interpretability in Explainable AI (XAI)\n",
      "Explainable AI (XAI) is a rapidly growing field that aims to provide a better understanding of how artificial intelligence (AI) systems make decisions.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun sinks into the horizon. The sound of the waves crashing against the shore is a soothing and calming sound, and the smell of the salty air is invigorating.\n",
      "The beach is a great place to relax and unwind, with plenty of space to stretch out and take in the beauty of the surroundings. There are also plenty of activities to enjoy, such as swimming, sunbathing, and building sandcastles.\n",
      "The beach is a great place to spend a day with friends and family, or to simply enjoy some alone time. It is a great way to escape the hustle and bustle of everyday life, and to reconnect with nature.\n",
      "The beach is a great place to relax and unwind, and to take in the beauty of the natural world. It is a great way to escape the hustle and bustle of everyday life, and to reconnect with nature.\n",
      "1 What is the best beach in the world?\n",
      "2 What is the most beautiful beach in the world?\n",
      "3 What is the most beautiful beach in the world 2022?\n",
      "4 What is the most beautiful beach in the world 2021?\n",
      "5 What is the most beautiful beach in the world 2020?\n",
      "6 What is the most beautiful beach in the world 2019?\n",
      "7 What is the most beautiful beach in the world 2018?\n",
      "What is the best beach in the world?\n",
      "There are so many beautiful beaches in the world, it’s hard to choose just one as the best. But if you’re looking for a beach that’s perfect for relaxing, swimming, and sunbathing, then you can’t go wrong with any of the following:\n",
      "1. Pink Sands Beach, Harbour Island, Bahamas\n",
      "This beach is famous for its pink sand, which is said to be the result of the coral reefs that surround the island. The water is also incredibly clear, making it a great place to go for a swim.\n",
      "2. Whitehaven Beach, Whitsunday Islands, Australia\n",
      "This beach is located on the island of Hamilton in the Whitsunday Islands, and it’s one of the most popular tourist destinations in Australia. The white sand is said to be some of the purest in the world\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun sinks into the horizon. The sound of the waves crashing against the shore is a soothing and calming sound, and the smell of the salty air is invigorating. The beach is a great place to relax and unwind, with plenty of space to stretch out and take in the beauty of the surroundings. There are also plenty of activities to enjoy, such as swimming, sunbathing, and building sandcastles. The beach is a great place to spend a day with friends and family, or to simply enjoy some alone time. It is a great way to escape the hustle and bustle of everyday life, and to reconnect with nature. The beach is a great place to relax and unwind, and to take in the beauty of the natural world. 1 What is the best beach in the world?\n",
      "2 What is the most beautiful beach in the world?\n",
      "3 What is the most beautiful beach in the world 2022?\n",
      "4 What is the most beautiful beach in the world 2021?\n",
      "5 What is the most beautiful beach in the world 2020?\n",
      "6 What is the most beautiful beach in the world 2019?\n",
      "7 What is the most beautiful beach in the world 2018?\n",
      "What is the best beach in the world?\n",
      "There are so many beautiful beaches in the world, it’s hard to choose just one as the best. But if you’re looking for a beach that’s perfect for relaxing, swimming, and sunbathing, then you can’t go wrong with any of the following:\n",
      "1. Pink Sands Beach, Harbour Island, Bahamas\n",
      "This beach is famous for its pink sand, which is said to be the result of the coral reefs that surround the island. The water is also incredibly clear, making it a great place to go for a swim. 2. Whitehaven Beach, Whitsunday Islands, Australia\n",
      "This beach is located on the island of Hamilton in the Whitsunday Islands, and it’s one of the most popular tourist destinations in Australia.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "Traveling can also help you develop new skills and knowledge. By learning about different cultures, you can gain a better understanding of the world and how it works. This can help you become more successful in your career and life.\n",
      "Traveling can also help you develop a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware.\n",
      "Traveling can also help you develop a better understanding of the environment. By visiting different places, you can gain insight into how different cultures and societies interact with the environment. This can help you become more environmentally conscious and responsible.\n",
      "Traveling can also help you develop a better understanding of history. By visiting different places, you can gain insight into how different cultures and societies have evolved over time. This can help you become more informed and knowledgeable about the world.\n",
      "Traveling can also help you develop a better understanding of the world. By visiting different places, you can gain insight into how different cultures and societies interact with each other. This can help you become more understanding and empathetic.\n",
      "Traveling can also help you develop a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware.\n",
      "Traveling can also help you develop a better understanding of the environment. By visiting different places, you can gain insight into how different cultures and societies interact with the environment. This can help you become more environmentally conscious and responsible.\n",
      "Traveling can also help you develop a better understanding of history. By visiting different places, you can gain insight into how different cultures and societies have evolved over time. This can help you become more informed and knowledgeable about the world.\n",
      "Traveling can also help you develop a better understanding of the world. By visiting different places, you can gain insight into how different cultures and societies interact with each other. This can help you become more understanding and empathetic.\n",
      "Traveling can also help you develop a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware.\n",
      "Traveling can also help you develop a better understanding of the environment.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. Traveling can also help you develop new skills and knowledge. By learning about different cultures, you can gain a better understanding of the world and how it works. This can help you become more successful in your career and life. Traveling can also help you develop a better understanding of yourself. By exploring new places, you can gain insight into your own values and beliefs. This can help you become more confident and self-aware. Traveling can also help you develop a better understanding of the environment. By visiting different places, you can gain insight into how different cultures and societies interact with the environment. This can help you become more environmentally conscious and responsible. Traveling can also help you develop a better understanding of history. By visiting different places, you can gain insight into how different cultures and societies have evolved over time. This can help you become more informed and knowledgeable about the world. Traveling can also help you develop a better understanding of the world. By visiting different places, you can gain insight into how different cultures and societies interact with each other. This can help you become more understanding and empathetic.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.514293248\n",
      "Model size after pruning:  13.514293248\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513900032\n",
      "Model size after pruning:  13.513900032\n",
      "Model size after pruning:  13.51385088\n",
      "Model size after pruning:  13.51385088\n",
      "Model size after pruning:  13.478682624\n",
      "Model size after pruning:  13.4535168\n",
      "Model size after pruning:  13.399400448\n",
      "Model size after pruning:  13.374234624\n",
      "Model size after pruning:  13.320118272\n",
      "Model size after pruning:  13.294952448\n",
      "Model size after pruning:  13.240836096\n",
      "Model size after pruning:  13.215670272\n",
      "Model size after pruning:  13.16155392\n",
      "Model size after pruning:  13.136388096\n",
      "Model size after pruning:  13.082271744\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4-5): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8-9): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
      "          (down_proj): Linear(in_features=11002, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16-17): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
      "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20-21): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9577, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9577, bias=False)\n",
      "          (down_proj): Linear(in_features=9577, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23-27): 5 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
      "          (down_proj): Linear(in_features=8806, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "13.082271744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32, 128]' is invalid for input of size 13312",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32, 128]' is invalid for input of size 13312"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d4099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5151656d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T05:21:12.307962Z",
     "start_time": "2024-05-21T05:21:12.304916Z"
    }
   },
   "source": [
    "## Experiment #3 --> PreserveRatio=0.95, lbound=0.8, rbound=0.95 (changes reward fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ab0c23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:50:38.116578Z",
     "start_time": "2024-05-21T06:50:38.112868Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_0.95_base_2420.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "067b7749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:50:38.800674Z",
     "start_time": "2024-05-21T06:50:38.797365Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f72353c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:09:52.572710Z",
     "start_time": "2024-05-21T06:50:40.318145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  3840\n",
      "67 ==>  10458\n",
      "75 ==>  3840\n",
      "81 ==>  10458\n",
      "89 ==>  3840\n",
      "95 ==>  10458\n",
      "103 ==>  3840\n",
      "109 ==>  10458\n",
      "117 ==>  3840\n",
      "123 ==>  10458\n",
      "131 ==>  3840\n",
      "137 ==>  10458\n",
      "145 ==>  3712\n",
      "151 ==>  10458\n",
      "159 ==>  3840\n",
      "165 ==>  10458\n",
      "173 ==>  3840\n",
      "179 ==>  10458\n",
      "187 ==>  3840\n",
      "193 ==>  9979\n",
      "201 ==>  3840\n",
      "207 ==>  10458\n",
      "215 ==>  3840\n",
      "221 ==>  10458\n",
      "229 ==>  3840\n",
      "235 ==>  10458\n",
      "243 ==>  3840\n",
      "249 ==>  10458\n",
      "257 ==>  3840\n",
      "263 ==>  10416\n",
      "271 ==>  3840\n",
      "277 ==>  10182\n",
      "285 ==>  3840\n",
      "291 ==>  10458\n",
      "299 ==>  3840\n",
      "305 ==>  10458\n",
      "313 ==>  3840\n",
      "319 ==>  10458\n",
      "327 ==>  3840\n",
      "333 ==>  10163\n",
      "341 ==>  3840\n",
      "347 ==>  10458\n",
      "355 ==>  3840\n",
      "361 ==>  10445\n",
      "369 ==>  3840\n",
      "375 ==>  10458\n",
      "383 ==>  3840\n",
      "389 ==>  10418\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:13,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:01<00:12,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:11,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:02<00:11,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:04<00:10,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:09,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:05<00:08,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:07,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:07<00:06,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:09<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:09<00:04,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:13<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:13<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:14<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  19.17513656616211\n",
      "Perplexity on wikitext2:  19.17513656616211\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:12:22:37,862 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:22:37,865 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:22:42,989 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:22:42,990 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:12:23:27,784 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:12:23:27,798 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:12:23:27,813 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:12:23:27,816 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a923d848d1454a898111f6d62d54e6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:12:23:49,992 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:23:49,994 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:24:11,043 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:24:11,047 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:24:32,186 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:12:24:32,190 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:12:24:32,191 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:12:24:32,192 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:12:24:32,193 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:12:24:32,194 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:12:24:32,195 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:12:24:32,199 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 404.00it/s]\n",
      "2024-05-21:12:24:32,478 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 92.06it/s]\n",
      "2024-05-21:12:24:33,613 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1493.15it/s]\n",
      "2024-05-21:12:24:33,696 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 827.63it/s]\n",
      "2024-05-21:12:24:33,835 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 50846.21it/s]\n",
      "2024-05-21:12:24:33,851 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1596.67it/s]\n",
      "2024-05-21:12:24:33,932 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1461.76it/s]\n",
      "2024-05-21:12:24:34,016 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [03:05<00:00, 11.89it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|    arc_easy   |    0.6    | 0.04923659639173309  |      0.67      |  0.04725815626252607  |\n",
      "|   hellaswag   |    0.43   | 0.049756985195624284 |      0.65      |  0.04793724854411019  |\n",
      "|     boolq     |    0.7    | 0.046056618647183814 |      None      |          None         |\n",
      "| arc_challenge |    0.32   | 0.04688261722621504  |      0.34      |  0.04760952285695236  |\n",
      "|   winogrande  |    0.61   | 0.04902071300001975  |      None      |          None         |\n",
      "|   openbookqa  |    0.23   | 0.04229525846816506  |      0.41      |  0.04943110704237102  |\n",
      "|      rte      |    0.56   | 0.049888765156985884 |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "Gardening is a great way to get some exercise and stay health.\n",
      "Gardening is a great way to get some exercise and stay health. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay healthy. It’s a great way to get some exercise and stay health\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. Gardening is a great way to get some exercise and stay health. It’s a great way to get some exercise and stay healthy.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of connection and connection to the past.\n",
      "Family traditions are a great way to teach children about the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and the importance of family and\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of connection and connection to the past.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will continue to be a powerful tool for self-expression, allowing people to express their individualities through their clothing choices. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individualities through their clothing choices.\n",
      "The fashion industry is a multi-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-bill-b\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will continue to be a powerful tool for self-expression, allowing people to express their individualities through their clothing choices. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individualities through their clothing choices.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is under undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\n",
      "The healthcare industry is undergoing a transformation by enabling healthcare providers to provide more efficient and effective care\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mhealthcare providers to provide more efficient and effective care. The healthcare industry is under undergoing a transformation by enabling healthcare providers to provide more efficient and effective care.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m help investors make more informed decisions about when to buy and sell stocks.\n",
      "One popular type of algorithm is the neural network algorithm, which is a type of algorithm that is based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "Another type of algorithm is the genetic algorithm, which is a type of algorithm that is based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The final type of algorithm is the genetic algorithm, which is a type of algorithm that is based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\n",
      "The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mhelp investors make more informed decisions about when to buy and sell stocks. One popular type of algorithm is the neural network algorithm, which is a type of algorithm that is based on the idea of the brain. These algorithms are increasingly being used to predict stock market trends. Another type of algorithm is the genetic algorithm, which is a type of algorithm that is based on the idea of the brain. The final type of algorithm is the genetic algorithm, which is a type of algorithm that is based on the idea of the brain. The best way to predict stock market trends is to use a variety of algorithms that are based on the idea of the brain.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\n",
      "The future of human-computer interaction\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will become even more useful and valuable in our daily lives. The future of human-computer interaction is bright. As these technologies continue to evol and improve, they will become even more useful and valuable in our daily lives.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m roads safely and efficiently.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "The technology is still in its infancy, but it has the potential to revolutionize the way we travel.\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91mroads safely and efficiently.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m help organizations assess the ethical risks of their AI systems and develop strategies for mitigating those risks.\n",
      "One such framework is the “Ethical AI Manifesto,” which was published in 2018 by a group of leading AI researchers. The manifesto outlines a set of principles that organizations should follow when developing and deploying AI systems, including trans transparency, fairness, and accountability.\n",
      "Another important challenge in developing AI systems is ensuring that they are accessible to people with dis disabilities. Researchers are working on creating AI systems that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those that can be used by people with disabilities, including those\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mhelp organizations assess the ethical risks of their AI systems and develop strategies for mitigating those risks. One such framework is the “Ethical AI Manifesto,” which was published in 2018 by a group of leading AI researchers. The manifesto outlines a set of principles that organizations should follow when developing and deploying AI systems, including trans transparency, fairness, and accountability. Another important challenge in developing AI systems is ensuring that they are accessible to people with dis disabilities.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can not, not only detect objects, but also that can be used to improve image segmentation.\n",
      "In the field of computer vision, the ability to detect objects is a critical component of the technology. By mimicking the human visual system, these networks can identify objects, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can help doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that can not only detect objects, but also that can be used to improve image segmentation.\n",
      "In the field of computer vision, the ability to detect objects is a critical component of the technology. By mimicking the human visual system, these networks can identify objects, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can help doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that can not only detect objects, but also that can be used to improve image segmentation.\n",
      "In the field of computer vision, the ability to detect objects is a critical component of the technology. By mimicking the human visual system, these networks can identify objects, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can help doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that can not only detect objects, but also that can be used to improve image segmentation.\n",
      "In the field of computer vision, the ability to detect objects is a critical component of the technology. By mimicking the human visual system, these networks can identify objects, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can help doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that can not only detect objects, but also that can be used to improve image segmentation.\n",
      "In the field of computer vision, the ability to detect objects is a critical component of the technology. By mimicking the human visual system, these networks can identify objects, detect objects, and even perform image segmentation. This\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan not, not only detect objects, but also that can be used to improve image segmentation. In the field of computer vision, the ability to detect objects is a critical component of the technology. By mimicking the human visual system, these networks can identify objects, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can help doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that can not only detect objects, but also that can be used to improve image segmentation.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m the use of reinforcement signals.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\n",
      "The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91mthe use of reinforcement signals. The use of reinforcement signals is a powerful tool for learning optimal behaviors. It allows the agent to learn from the environment without having to rely on a specific reward function.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m potentially break the most widely-used cryptographic algorithms in a matter of minutes or even less. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The good news is that there is a lot of research being done on quantum-resistant cryptography. This includes the development of new algorithms that are designed to be resist-able to quantum computing. This is an important area of research, as it could have a major impact on the security of online transactions and data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud.\n",
      "The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography. This could have serious implications for the security of online\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mpotentially break the most widely-used cryptographic algorithms in a matter of minutes or even less. This could have serious implications for the security of online transactions, as well as for the security of data stored in the cloud. The good news is that there is a lot of research being done on quantum-resistant cryptography. This includes the development of new algorithms that are designed to be resist-able to quantum computing. This is an important area of research, as it could have a major impact on the security of online transactions and data stored in the cloud. The future of cryptography is an exciting one. With the advent of quantum computing, we can expect to see a revolution in the field of cryptography.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m can be applied to a wide range of industries.\n",
      "The future of AI is an exciting and rapidly evolving field. As we look to the future, we can expect to see the development of new and more powerful machine learning techniques that can be applied to a wide range of industries. We can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mcan be applied to a wide range of industries. The future of AI is an exciting and rapidly evolving field. As we look to the future, we can expect to see the development of new and more powerful machine learning techniques that can be applied to a wide range of industries. We can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries. As we look to the future, we can also expect to see the development of more powerful and more efficient AI systems that can be applied to a wide range of industries.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m.\n",
      "Learning allows models trained on large datasets to be fine-tuned for specific tasks by.\n",
      "Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-tuned for specific tasks by. Learning allows models trained on large datasets to be fine-\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91mLearning allows models trained on large datasets to be fine-tuned for specific tasks by.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m help improve the trust and trustworthiness of AI systems.\n",
      "In this article, we will explore the benefits of XAI and how it can be used to improve the trustworthiness of AI systems.\n",
      "What is explainable AI (XAI)?\n",
      "Explainable AI (XAI) is a term used to describe the ability of a system to explain how it makes decisions.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\n",
      "XAI is becoming increasingly important as AI systems are deployed in\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mhelp improve the trust and trustworthiness of AI systems. In this article, we will explore the benefits of XAI and how it can be used to improve the trustworthiness of AI systems. What is explainable AI (XAI)?\n",
      "Explainable AI (XAI) is a term used to describe the ability of a system to explain how it makes decisions. XAI is becoming increasingly important as AI systems are deployed in more and more areas of our lives.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun dips below the horizon. The sunset is a natural phenomenon that occurs every every day, and it is a great to see.\n",
      "The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great to see. The sunset is a natural phenomenon that occurs every every day, and it is a great\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun dips below the horizon. The sunset is a natural phenomenon that occurs every every day, and it is a great to see.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "Traveling to new countries can be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself. When you’re in a new country, you’re often exposed to different customs and traditions. You can also learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself. When you’re in a new country, you’re often exposed to different perspectives and experiences. You can also learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\n",
      "Travel\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. Traveling to new countries can be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself. When you’re in a new country, you’re often exposed to different customs and traditions. You can also learn about yourself by interacting with people who have different perspectives and experiences. When you’re in a new country, you’re often exposed to different perspectives and experiences. Traveling to new countries can be a great way to learn about yourself. It can also be a great way to learn about different cultures and traditions. It can also be a great way to learn about yourself by interacting with people who have different perspectives and experiences.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.506199552\n",
      "Model size after pruning:  13.492682752\n",
      "Model size after pruning:  13.484294144\n",
      "Model size after pruning:  13.470777344\n",
      "Model size after pruning:  13.462388736\n",
      "Model size after pruning:  13.448871936\n",
      "Model size after pruning:  13.440483328\n",
      "Model size after pruning:  13.426966528\n",
      "Model size after pruning:  13.41857792\n",
      "Model size after pruning:  13.40506112\n",
      "Model size after pruning:  13.396672512\n",
      "Model size after pruning:  13.383155712\n",
      "Model size after pruning:  13.3705728\n",
      "Model size after pruning:  13.357056\n",
      "Model size after pruning:  13.348667392\n",
      "Model size after pruning:  13.335150592\n",
      "Model size after pruning:  13.326761984\n",
      "Model size after pruning:  13.313245184\n",
      "Model size after pruning:  13.304856576\n",
      "Model size after pruning:  13.279567872\n",
      "Model size after pruning:  13.271179264\n",
      "Model size after pruning:  13.257662464\n",
      "Model size after pruning:  13.249273856\n",
      "Model size after pruning:  13.235757056\n",
      "Model size after pruning:  13.227368448\n",
      "Model size after pruning:  13.213851648\n",
      "Model size after pruning:  13.20546304\n",
      "Model size after pruning:  13.19194624\n",
      "Model size after pruning:  13.183557632\n",
      "Model size after pruning:  13.16900864\n",
      "Model size after pruning:  13.160620032\n",
      "Model size after pruning:  13.140320256\n",
      "Model size after pruning:  13.131931648\n",
      "Model size after pruning:  13.118414848\n",
      "Model size after pruning:  13.11002624\n",
      "Model size after pruning:  13.09650944\n",
      "Model size after pruning:  13.088120832\n",
      "Model size after pruning:  13.074604032\n",
      "Model size after pruning:  13.066215424\n",
      "Model size after pruning:  13.045448704\n",
      "Model size after pruning:  13.037060096\n",
      "Model size after pruning:  13.023543296\n",
      "Model size after pruning:  13.015154688\n",
      "Model size after pruning:  13.0013184\n",
      "Model size after pruning:  12.992929792\n",
      "Model size after pruning:  12.979412992\n",
      "Model size after pruning:  12.971024384\n",
      "Model size after pruning:  12.956524544\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4-9): 6 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11-12): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9979, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9979, bias=False)\n",
      "          (down_proj): Linear(in_features=9979, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14-17): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10416, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10416, bias=False)\n",
      "          (down_proj): Linear(in_features=10416, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10182, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10182, bias=False)\n",
      "          (down_proj): Linear(in_features=10182, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20-22): 3 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10163, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10163, bias=False)\n",
      "          (down_proj): Linear(in_features=10163, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10445, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10445, bias=False)\n",
      "          (down_proj): Linear(in_features=10445, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
      "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=10418, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=10418, bias=False)\n",
      "          (down_proj): Linear(in_features=10418, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "12.956524544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32, 128]' is invalid for input of size 15360",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32, 128]' is invalid for input of size 15360"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92166cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88643b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T05:21:12.307962Z",
     "start_time": "2024-05-21T05:21:12.304916Z"
    }
   },
   "source": [
    "## Experiment #4 --> PreserveRatio=0.95, lbound=0.8, rbound=0.9 (changes reward fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b54f2db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:11:32.068284Z",
     "start_time": "2024-05-21T07:11:32.064679Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_0.9_base_2422.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcfadb75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:11:33.266855Z",
     "start_time": "2024-05-21T07:11:33.263404Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e689a91e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:27:00.764381Z",
     "start_time": "2024-05-21T07:11:42.109427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  3712\n",
      "67 ==>  9509\n",
      "75 ==>  3712\n",
      "81 ==>  9233\n",
      "89 ==>  3328\n",
      "95 ==>  9907\n",
      "103 ==>  3584\n",
      "109 ==>  9907\n",
      "117 ==>  3712\n",
      "123 ==>  9701\n",
      "131 ==>  3712\n",
      "137 ==>  9907\n",
      "145 ==>  3712\n",
      "151 ==>  9907\n",
      "159 ==>  3712\n",
      "165 ==>  9907\n",
      "173 ==>  3712\n",
      "179 ==>  9907\n",
      "187 ==>  3712\n",
      "193 ==>  9488\n",
      "201 ==>  3712\n",
      "207 ==>  9907\n",
      "215 ==>  3712\n",
      "221 ==>  9885\n",
      "229 ==>  3712\n",
      "235 ==>  9907\n",
      "243 ==>  3712\n",
      "249 ==>  9907\n",
      "257 ==>  3712\n",
      "263 ==>  9907\n",
      "271 ==>  3712\n",
      "277 ==>  9907\n",
      "285 ==>  3712\n",
      "291 ==>  9907\n",
      "299 ==>  3712\n",
      "305 ==>  9907\n",
      "313 ==>  3584\n",
      "319 ==>  9907\n",
      "327 ==>  3328\n",
      "333 ==>  9907\n",
      "341 ==>  3712\n",
      "347 ==>  9568\n",
      "355 ==>  3584\n",
      "361 ==>  9907\n",
      "369 ==>  3712\n",
      "375 ==>  9907\n",
      "383 ==>  3712\n",
      "389 ==>  9907\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:14,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:13,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:01<00:12,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:11,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:02<00:11,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:03<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:08,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:05<00:08,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:09<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:09<00:04,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:12<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:13<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:14<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  25.54514503479004\n",
      "Perplexity on wikitext2:  25.54514503479004\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:12:43:29,620 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:43:29,622 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:43:35,900 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:43:35,900 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:12:44:16,285 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:12:44:16,286 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:12:44:16,298 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:12:44:16,298 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0187533254017a67905c2f08aa9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:12:44:48,942 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:44:48,944 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:45:03,647 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:12:45:03,648 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:12:45:07,953 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:12:45:07,954 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:12:45:07,955 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:12:45:07,955 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:12:45:07,956 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:12:45:07,957 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:12:45:07,958 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:12:45:07,961 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1470.05it/s]\n",
      "2024-05-21:12:45:08,045 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 36326.90it/s]\n",
      "2024-05-21:12:45:08,056 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 850.41it/s]\n",
      "2024-05-21:12:45:08,186 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1470.63it/s]\n",
      "2024-05-21:12:45:08,263 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 874.97it/s]\n",
      "2024-05-21:12:45:08,389 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1611.54it/s]\n",
      "2024-05-21:12:45:08,463 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1934.31it/s]\n",
      "2024-05-21:12:45:08,541 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [01:55<00:00, 19.10it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     boolq     |    0.68   | 0.046882617226215034 |      None      |          None         |\n",
      "|   winogrande  |    0.6    | 0.04923659639173309  |      None      |          None         |\n",
      "| arc_challenge |    0.27   |  0.0446196043338474  |      0.35      |  0.047937248544110196 |\n",
      "|      rte      |    0.49   | 0.05024183937956912  |      None      |          None         |\n",
      "|    arc_easy   |    0.52   | 0.050211673156867795 |      0.55      |          0.05         |\n",
      "|   openbookqa  |    0.19   | 0.039427724440366234 |      0.36      |  0.04824181513244218  |\n",
      "|   hellaswag   |    0.45   |         0.05         |      0.57      |  0.049756985195624284 |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "Gardening is a great way to get your kids to eat more vegetables.\n",
      "Gardening is a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. Gardening is a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of identity and connection.\n",
      "In the United States, the most popular Thanksgiving traditions include watching football, watching parades, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanksgiving Parade, watching the Macy's Thanks\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of identity and connection.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will continue to be a powerful tool for self-expression, allowing individuals to express their individualities through their clothing choices.\n",
      "The 10:10:10 rule of fashion is a concept that suggests that fashion is a cycle of ten years. It is a cycle of ten years because it takes ten years for a new trend to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will continue to be a powerful tool for self-expression, allowing individuals to express their individualities through their clothing choices. The 10:10:10 rule of fashion is a concept that suggests that fashion is a cycle of ten years. It is a cycle of ten years because it takes ten years for a new trend to be accepted by the public. It is a cycle of ten years because it takes ten years for a new style to be accepted by the public.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m it to provide better, more efficient, and more effective care.\n",
      "\n",
      "Comment: I agree. I'm also a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\n",
      "\n",
      "Comment: I'm a big fan of the \"hive\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mit to provide better, more efficient, and more effective care. Comment: I agree. I'm also a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well. Comment: I'm a big fan of the \"hive mind\" and the \"wisdom of crowds\" as well.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m help investors make more informed decisions about when to buy and when to sell.\n",
      "One of the most popular types of algorithms is the ARIMAX algorithm. This algorithm was developed by by G. Arthur Ger. Arthur. It is a type of time series model that is used to predict the future value of a stock.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 1.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 19.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 197.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 197.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 197.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 197.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 197.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 197.\n",
      "The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger,\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mhelp investors make more informed decisions about when to buy and when to sell. One of the most popular types of algorithms is the ARIMAX algorithm. This algorithm was developed by by G. Arthur Ger. Arthur. It is a type of time series model that is used to predict the future value of a stock. The ARIMAX algorithm is a type of time series model that is used to predict the future value of a stock. It is named after the first of the three authors, G. Arthur Ger, who developed the model in 1. Arthur Ger, who developed the model in 19. Arthur Ger, who developed the model in 197.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will continue to play, and be a part of, the future of human-computer interaction.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\n",
      "The future of human-computer interaction is likely\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will continue to play, and be a part of, the future of human-computer interaction. The future of human-computer interaction is likely to involve a combination of technologies that are not yet available. For example, it is possible that we will see a computer that can think and act like a human, but it is also possible that we will see a computer that is not.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m safely and safely.\n",
      "The technology is still in its infancy, but it is rapidly evol.\n",
      "The technology is still in its infancy, but it is rapidly evol.\n",
      "The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but it is rapidly evol. The technology is still in its infancy, but\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91msafely and safely. The technology is still in its infancy, but it is rapidly evol.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m help AI systems to be more transparent and account-able, and are also working on creating AI systems that can be more transparent and accountable.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasing more likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\n",
      "The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasingly likely that we\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mhelp AI systems to be more transparent and account-able, and are also working on creating AI systems that can be more transparent and accountable. The future of AI is looking more and more like a reality. With the rapid advances in technology, it is becoming increasing more likely that we will be able to create AI systems that are more powerful and more efficient than ever before. With the rapid advances in technology, it is becoming increasingly likely that we will be able to create AI systems that are more powerful and more efficient than ever before.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can not.\n",
      "The field of computer vision has come a long way in the past. From the early days of the 19, when the field was in its infancy, to the present day, when it is a mature and rapidly growing field. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1,950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy. The field of computer vision has come a long way since the 1950s, when it was in its infancy\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan not. The field of computer vision has come a long way in the past. From the early days of the 19, when the field was in its infancy, to the present day, when it is a mature and rapidly growing field. The field of computer vision has come a long way since the 1950s, when it was in its infancy.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m,, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m be used to create algorithms that are much faster than the best classical computers.\n",
      "This could lead to a new era of security, where only the most advanced algorithms are used to protect data.\n",
      "The future of cryptography is likely to be a mix of traditional and quantum algorithms, with the latter being used to protect the most sensitive data.\n",
      "This is a very exciting time for the field of cryptography, and it is likely that the future will be full of new ideas and innov.\n",
      "The future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\n",
      "The future of cryptography is likely to be a mix of the two. The first is\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mbe used to create algorithms that are much faster than the best classical computers. This could lead to a new era of security, where only the most advanced algorithms are used to protect data. The future of cryptography is likely to be a mix of traditional and quantum algorithms, with the latter being used to protect the most sensitive data. This is a very exciting time for the field of cryptography, and it is likely that the future will be full of new ideas and innov. The future of cryptography is likely to be a mix of the two. The first is that the future of cryptography is likely to be a mix of the two. The second is that the future of cryptography is likely to be a mix of the two.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m can be used to improve the world.\n",
      "The future of AI is bright. We will continue to see more powerful and efficient systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world. We will also see more advanced systems that can be used to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mcan be used to improve the world. The future of AI is bright. We will continue to see more powerful and efficient systems that can be used to improve the world. We will also see more advanced systems that can be used to improve the world.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m.\n",
      "Learning is the process of how a child learns. It is a process of learning to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91mLearning is the process of how a child learns.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m help developers improve the accuracy of their models and improve the safety of the systems they are used in.\n",
      "\n",
      "\\section{What is a Neural Network?}\n",
      "\n",
      "\\begin{figure}[!h]\n",
      "\\centering\n",
      "\\includegraphics[width=0.:]{figs/__what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_is_a_what_\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mhelp developers improve the accuracy of their models and improve the safety of the systems they are used in. \\section{What is a Neural Network?}\n",
      "\n",
      "\\begin{figure}[!h]\n",
      "\\centering\n",
      "\\includegraphics[width=0.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun begins to set.\n",
      "The sun is a natural source of light that is the most most powerful and most important of the three. It is the most powerful because it is the most intense, and it is the most important because it is the most intense.\n",
      "The sun is the most important because it is the most intense. It is the most important because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most intense because it is the most intense. It is the most inten.\n",
      "The sun is the most important because it is the most intense. It is the most intense because it is the most intense. It is the most inten. It is the most inten. It is the most inten. It is the most inten. It is the most.\n",
      "The sun is the most important because it is the most inten. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It is the most. It\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun begins to set. The sun is a natural source of light that is the most most powerful and most important of the three. It is the most powerful because it is the most intense, and it is the most important because it is the most intense. The sun is the most important because it is the most intense. It is the most important because it is the most intense. It is the most intense because it is the most intense. It is the most inten. It is the most. The sun is the most important because it is the most inten.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "Traveling to new countries can be a great way to learn about different cultures and traditions. It allows you to experience the world in a more meaningful way.\n",
      "Traveling to new countries allows you to experience different cultures and traditions. It allows you to interact with people in a more meaningful way.\n",
      "The world is a big place, and there is a lot to see. There are many different cultures and traditions to be found in the world.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tour tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tourist.\n",
      "The world is a big place, and there are many different cultures and traditions to be found in it\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. Traveling to new countries can be a great way to learn about different cultures and traditions. It allows you to experience the world in a more meaningful way. Traveling to new countries allows you to experience different cultures and traditions. It allows you to interact with people in a more meaningful way. The world is a big place, and there is a lot to see. There are many different cultures and traditions to be found in the world. The world is a big place, and there are many different cultures and traditions to be found in it. There are many different ways to be a tour tourist.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.502005248\n",
      "Model size after pruning:  13.465165824\n",
      "Model size after pruning:  13.452582912\n",
      "Model size after pruning:  13.408960512\n",
      "Model size after pruning:  13.383794688\n",
      "Model size after pruning:  13.356736512\n",
      "Model size after pruning:  13.339959296\n",
      "Model size after pruning:  13.31290112\n",
      "Model size after pruning:  13.300318208\n",
      "Model size after pruning:  13.268197376\n",
      "Model size after pruning:  13.255614464\n",
      "Model size after pruning:  13.228556288\n",
      "Model size after pruning:  13.215973376\n",
      "Model size after pruning:  13.1889152\n",
      "Model size after pruning:  13.176332288\n",
      "Model size after pruning:  13.149274112\n",
      "Model size after pruning:  13.1366912\n",
      "Model size after pruning:  13.109633024\n",
      "Model size after pruning:  13.097050112\n",
      "Model size after pruning:  13.059694592\n",
      "Model size after pruning:  13.04711168\n",
      "Model size after pruning:  13.020053504\n",
      "Model size after pruning:  13.007470592\n",
      "Model size after pruning:  12.979871744\n",
      "Model size after pruning:  12.967288832\n",
      "Model size after pruning:  12.940230656\n",
      "Model size after pruning:  12.927647744\n",
      "Model size after pruning:  12.900589568\n",
      "Model size after pruning:  12.888006656\n",
      "Model size after pruning:  12.86094848\n",
      "Model size after pruning:  12.848365568\n",
      "Model size after pruning:  12.821307392\n",
      "Model size after pruning:  12.80872448\n",
      "Model size after pruning:  12.781666304\n",
      "Model size after pruning:  12.769083392\n",
      "Model size after pruning:  12.742025216\n",
      "Model size after pruning:  12.725248\n",
      "Model size after pruning:  12.698189824\n",
      "Model size after pruning:  12.673024\n",
      "Model size after pruning:  12.645965824\n",
      "Model size after pruning:  12.633382912\n",
      "Model size after pruning:  12.597993472\n",
      "Model size after pruning:  12.581216256\n",
      "Model size after pruning:  12.55415808\n",
      "Model size after pruning:  12.541575168\n",
      "Model size after pruning:  12.514516992\n",
      "Model size after pruning:  12.50193408\n",
      "Model size after pruning:  12.474875904\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9509, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9509, bias=False)\n",
      "          (down_proj): Linear(in_features=9509, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9233, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9233, bias=False)\n",
      "          (down_proj): Linear(in_features=9233, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9701, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9701, bias=False)\n",
      "          (down_proj): Linear(in_features=9701, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9-12): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9488, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9488, bias=False)\n",
      "          (down_proj): Linear(in_features=9488, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9885, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9885, bias=False)\n",
      "          (down_proj): Linear(in_features=9885, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16-21): 6 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9568, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9568, bias=False)\n",
      "          (down_proj): Linear(in_features=9568, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26-27): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "12.474875904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32, 128]' is invalid for input of size 14848",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32, 128]' is invalid for input of size 14848"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f8b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53fd4ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T05:21:12.307962Z",
     "start_time": "2024-05-21T05:21:12.304916Z"
    }
   },
   "source": [
    "## Experiment #5 --> PreserveRatio=0.95, lbound=0.7, rbound=0.9 (changes reward fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078d5004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:28:40.234129Z",
     "start_time": "2024-05-21T07:28:40.230510Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.7_0.9_base_2424.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1bab467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:28:41.157945Z",
     "start_time": "2024-05-21T07:28:41.154426Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bb942f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:45:15.277602Z",
     "start_time": "2024-05-21T07:28:41.669378Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  3712\n",
      "67 ==>  9521\n",
      "75 ==>  3712\n",
      "81 ==>  9250\n",
      "89 ==>  3200\n",
      "95 ==>  9907\n",
      "103 ==>  3584\n",
      "109 ==>  9907\n",
      "117 ==>  3712\n",
      "123 ==>  9725\n",
      "131 ==>  3712\n",
      "137 ==>  9907\n",
      "145 ==>  3712\n",
      "151 ==>  9907\n",
      "159 ==>  3712\n",
      "165 ==>  9907\n",
      "173 ==>  3712\n",
      "179 ==>  9907\n",
      "187 ==>  3712\n",
      "193 ==>  9548\n",
      "201 ==>  3712\n",
      "207 ==>  9907\n",
      "215 ==>  3712\n",
      "221 ==>  9907\n",
      "229 ==>  3712\n",
      "235 ==>  9907\n",
      "243 ==>  3712\n",
      "249 ==>  9907\n",
      "257 ==>  3712\n",
      "263 ==>  9907\n",
      "271 ==>  3712\n",
      "277 ==>  9907\n",
      "285 ==>  3712\n",
      "291 ==>  9907\n",
      "299 ==>  3712\n",
      "305 ==>  9907\n",
      "313 ==>  3584\n",
      "319 ==>  9907\n",
      "327 ==>  3328\n",
      "333 ==>  9907\n",
      "341 ==>  3712\n",
      "347 ==>  9821\n",
      "355 ==>  3584\n",
      "361 ==>  9907\n",
      "369 ==>  3712\n",
      "375 ==>  9907\n",
      "383 ==>  3712\n",
      "389 ==>  9907\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:13,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:12,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:01<00:12,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:02<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:03<00:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:08,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:05<00:08,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:06<00:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:07<00:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:08<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:09<00:04,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:10<00:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:11<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:12<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:13<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:14<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  25.70524024963379\n",
      "Perplexity on wikitext2:  25.70524024963379\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:13:00:34,919 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:13:00:34,921 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:13:00:39,895 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:13:00:39,896 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:13:01:21,243 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:13:01:21,245 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:13:01:21,259 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:13:01:21,260 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c848631d8b3422cb557c95c505eb36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:13:01:36,070 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:13:01:36,072 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:13:01:42,568 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:13:01:42,570 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:13:02:14,399 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:13:02:14,400 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:13:02:14,401 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:13:02:14,402 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:13:02:14,402 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:13:02:14,403 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:13:02:14,404 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:13:02:14,409 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 789.15it/s]\n",
      "2024-05-21:13:02:14,549 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 63636.84it/s]\n",
      "2024-05-21:13:02:14,559 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1888.42it/s]\n",
      "2024-05-21:13:02:14,639 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1541.21it/s]\n",
      "2024-05-21:13:02:14,729 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1461.90it/s]\n",
      "2024-05-21:13:02:14,808 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 834.91it/s]\n",
      "2024-05-21:13:02:14,940 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1297.95it/s]\n",
      "2024-05-21:13:02:15,027 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [02:05<00:00, 17.55it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "| arc_challenge |    0.29   | 0.045604802157206845 |      0.36      |  0.048241815132442176 |\n",
      "|   winogrande  |    0.57   | 0.04975698519562427  |      None      |          None         |\n",
      "|   hellaswag   |    0.44   | 0.04988876515698589  |      0.55      |          0.05         |\n",
      "|   openbookqa  |    0.2    | 0.04020151261036846  |      0.35      |  0.047937248544110196 |\n",
      "|     boolq     |    0.68   | 0.04688261722621504  |      None      |          None         |\n",
      "|    arc_easy   |    0.52   | 0.050211673156867795 |      0.58      |  0.04960449637488583  |\n",
      "|      rte      |    0.49   | 0.05024183937956912  |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "Gardening is a great way to get your kids to eat more vegetables.\n",
      "Gardening is a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. Gardening is a great way to get your kids to eat more vegetables. It’s a great way to get your kids to eat more vegetables.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of identity and connection.\n",
      "In the United States, the most popular Thanksgiving Day traditions include watching watching football, watching parades, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksgiving Day Parade, watching the Macy's Thanksg\n",
      "In the United States, the Thanksg is a time of year when families gather to give watch the Macy's Thanksgiving Day Parade, watch the Macy's Thanksgiving Day Parade, watch the Macy's Thanksgiving Day Parade, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy's Thanksg, watch the Macy\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of identity and connection.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will be a powerful tool for self-expression, allowing people to express their individualities through their clothing choices.\n",
      "The 10 fashion trends that will rule 2023\n",
      "The 10 fashion trends that will rule 2 fashion will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223. The 10 fashion trends that will rule 2023. The 10 fashion trends that\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will be a powerful tool for self-expression, allowing people to express their individualities through their clothing choices. The 10 fashion trends that will rule 2023\n",
      "The 10 fashion trends that will rule 2 fashion will rule 2023. The 10 fashion trends that will rule 2023. The 10 fashion trends that will rule 223.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m the delivery of more effective, efficient, and more affordable care.\n",
      "The healthcare industry is one of the largest indust in the world by revenue, with a market worth of $ $100 trillion in 2014.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a lux.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trill in 2 in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a luxury.\n",
      "The health\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mthe delivery of more effective, efficient, and more affordable care. The healthcare industry is one of the largest indust in the world by revenue, with a market worth of $ $100 trillion in 2014. The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trillion in 2014. This is largely due to the fact that health is a necessity, not a lux. The healthcare industry is one of the largest industries in the world in terms of revenue, with a value of $100 trill in 2 in 2014. This is largely due to the fact that health is a necessity, not a luxury.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m help invest investors to make more informed decisions and potentially increase their chances of success.\n",
      "The stock market is a complex system that is constantly changing. It is difficult to predict exactly what will happen in the future, but using a variety of indic indicators can can help investors to make more informed decisions and increase their chances of success.\n",
      "The stock market is a system that is constantly changing. It is difficult to predict exactly what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\n",
      "The stock market is a system that is constantly changing. It is difficult to predict exactly what will happen in the future, but by using a variety of indicators, investors can increase their chances of success. This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\n",
      "This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\n",
      "The stock market is a system that is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success. This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\n",
      "This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success. This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\n",
      "The stock market is a system that is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success. This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\n",
      "The stock market is a system that is constantly changing and it is difficult to predict what will happen in the future,\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mhelp invest investors to make more informed decisions and potentially increase their chances of success. The stock market is a complex system that is constantly changing. It is difficult to predict exactly what will happen in the future, but using a variety of indic indicators can can help investors to make more informed decisions and increase their chances of success. The stock market is a system that is constantly changing. It is difficult to predict exactly what will happen in the future, but by using a variety of indicators, investors can increase their chances of success. This is because the market is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success. The stock market is a system that is constantly changing and it is difficult to predict what will happen in the future, but by using a variety of indicators, investors can increase their chances of success.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will continue to play, and be a part of, the future of human-computer interaction.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m safely and safely.\n",
      "The data is collected by the car’s sensors and processed by the car’s computer. The car’s computer processes the data and uses the data to make the car’s own decisions.\n",
      "The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91msafely and safely. The data is collected by the car’s sensors and processed by the car’s computer. The car’s computer processes the data and uses the data to make the car’s own decisions. The car’s computer uses the data to make the car’s own decisions.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m help AI systems to be more transparent and account-able, such as by making.\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Explainability:} The ability of AI systems to explain their decisions and actions in a way that humans can understand is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to explain the reasons behind AI systems' actions and decisions.\n",
      "\\item \\textbf{Transparency:} The ability of AI systems to be transparent about their actions and decisions is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to be more transparent about the reasons behind AI systems' actions and decisions.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness}\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to be more ethical and unbiased.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to be more ethical and unbiased.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mhelp AI systems to be more transparent and account-able, such as by making. \\begin{itemize}\n",
      "\\item \\textbf{Explainability:} The ability of AI systems to explain their decisions and actions in a way that humans can understand is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to explain the reasons behind AI systems' actions and decisions. \\item \\textbf{Transparency:} The ability of AI systems to be transparent about their actions and decisions is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to be more transparent about the reasons behind AI systems' actions and decisions. \\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness}\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI. Researchers are working on creating algorithms that can be used to be more ethical and unbiased. \\end{itemize}\n",
      "\n",
      "\\subsection{Ethics and unbiasedness:} The ability of AI systems to be ethical and unbiased is a key challenge in the field of AI.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can not.\n",
      "The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, AI can help, and we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, we can see. The field of computer vision has the potential to transform the way we interact with the world. By mimick\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan not. The field of computer vision has the potential to transform the way we interact with the world. By mimicking the human visual system, AI can help, and we can see. By mimicking the human visual system, we can see.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m,, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m be used to create algorithms that are much faster than the best classical computers.\n",
      "This could lead to a new era of security, where only the most advanced computers are able to keep.\n",
      "\n",
      "\n",
      "### 3. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 5. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 6. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 7. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 8. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 9. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 10. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 11. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 12. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 13. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 14. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 15. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 16. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 17. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 18. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 19. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 19. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 20. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 21. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 22. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 13. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 14. What will happen if the quantum computer\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mbe used to create algorithms that are much faster than the best classical computers. This could lead to a new era of security, where only the most advanced computers are able to keep. ### 3. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 5. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 6. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 7. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 8. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 9. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 10. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 11. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 12. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 13. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 14. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 15. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 16. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 17. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 18. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 19. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 20. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 21. What will happen if the quantum computer is not available?\n",
      "\n",
      "\n",
      "\n",
      "### 22.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m can be used to improve the world.\n",
      "The future of AI is bright. We will continue to see more powerful and efficient systems that can be used to improve the world. We will also see more secure systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data. We will also see more systems that can be used to protect data\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mcan be used to improve the world. The future of AI is bright. We will continue to see more powerful and efficient systems that can be used to improve the world. We will also see more secure systems that can be used to protect data.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m.\n",
      "Learning is the process of how a child learns. It is a process of learning to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to learn to\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91mLearning is the process of how a child learns.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m help developers improve the accuracy of their models and improve the safety of the systems they are used in.\n",
      "\n",
      "\\section{What is a Neural Network?}\n",
      "\n",
      "\\begin{figure}[!h]\n",
      "\\centering\n",
      "\\includegraphics[width=0.:]{figs/__what_is_a_neural_network_.png}\n",
      "\\\n",
      "\\caption{What is a neural network?}\n",
      "\\label{fig:_what_is_a_}\n",
      "\\:\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\n",
      "\\caption{What is a neural network?\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mhelp developers improve the accuracy of their models and improve the safety of the systems they are used in. \\section{What is a Neural Network?}\n",
      "\n",
      "\\begin{figure}[!h]\n",
      "\\centering\n",
      "\\includegraphics[width=0. :]{figs/__what_is_a_neural_network_.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the sun begins to set.\n",
      "\n",
      "The sunset is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe sun begins to set. The sunset is a natural phenomenon that occurs when the sun is seen. It is a natural phenomenon that occurs when the sun is seen.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "Traveling to new countries can be a great way to learn about different cultures and traditions. It allows you to experience the world in a more meaningful way.\n",
      "For example, you can try:\n",
      "Tasting local foods\n",
      "Inter with people\n",
      "Traveling to new countries can be a great way to learn about different cultures and traditions. It allows you to experience the world in a more meaningful way. For example, you can try:\n",
      "Tasting local foods:\n",
      "Eating local foods is a great way to learn about a culture. It allows you to taste the world in a more meaningful way. For example, you can taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way.\n",
      "Eating local foods allows you to taste\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. Traveling to new countries can be a great way to learn about different cultures and traditions. It allows you to experience the world in a more meaningful way. For example, you can try:\n",
      "Tasting local foods\n",
      "Inter with people\n",
      "Traveling to new countries can be a great way to learn about different cultures and traditions. For example, you can try:\n",
      "Tasting local foods:\n",
      "Eating local foods is a great way to learn about a culture. It allows you to taste the world in a more meaningful way. For example, you can taste the world in a more meaningful way. Eating local foods allows you to taste the world in a more meaningful way.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.502005248\n",
      "Model size after pruning:  13.465460736\n",
      "Model size after pruning:  13.452877824\n",
      "Model size after pruning:  13.409673216\n",
      "Model size after pruning:  13.380313088\n",
      "Model size after pruning:  13.353254912\n",
      "Model size after pruning:  13.336477696\n",
      "Model size after pruning:  13.30941952\n",
      "Model size after pruning:  13.296836608\n",
      "Model size after pruning:  13.2653056\n",
      "Model size after pruning:  13.252722688\n",
      "Model size after pruning:  13.225664512\n",
      "Model size after pruning:  13.2130816\n",
      "Model size after pruning:  13.186023424\n",
      "Model size after pruning:  13.173440512\n",
      "Model size after pruning:  13.146382336\n",
      "Model size after pruning:  13.133799424\n",
      "Model size after pruning:  13.106741248\n",
      "Model size after pruning:  13.094158336\n",
      "Model size after pruning:  13.058277376\n",
      "Model size after pruning:  13.045694464\n",
      "Model size after pruning:  13.018636288\n",
      "Model size after pruning:  13.006053376\n",
      "Model size after pruning:  12.9789952\n",
      "Model size after pruning:  12.966412288\n",
      "Model size after pruning:  12.939354112\n",
      "Model size after pruning:  12.9267712\n",
      "Model size after pruning:  12.899713024\n",
      "Model size after pruning:  12.887130112\n",
      "Model size after pruning:  12.860071936\n",
      "Model size after pruning:  12.847489024\n",
      "Model size after pruning:  12.820430848\n",
      "Model size after pruning:  12.807847936\n",
      "Model size after pruning:  12.78078976\n",
      "Model size after pruning:  12.768206848\n",
      "Model size after pruning:  12.741148672\n",
      "Model size after pruning:  12.724371456\n",
      "Model size after pruning:  12.69731328\n",
      "Model size after pruning:  12.672147456\n",
      "Model size after pruning:  12.64508928\n",
      "Model size after pruning:  12.632506368\n",
      "Model size after pruning:  12.603334656\n",
      "Model size after pruning:  12.58655744\n",
      "Model size after pruning:  12.559499264\n",
      "Model size after pruning:  12.546916352\n",
      "Model size after pruning:  12.519858176\n",
      "Model size after pruning:  12.507275264\n",
      "Model size after pruning:  12.480217088\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9521, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9521, bias=False)\n",
      "          (down_proj): Linear(in_features=9521, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9250, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9250, bias=False)\n",
      "          (down_proj): Linear(in_features=9250, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (o_proj): Linear(in_features=3200, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9725, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9725, bias=False)\n",
      "          (down_proj): Linear(in_features=9725, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9-12): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9548, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9548, bias=False)\n",
      "          (down_proj): Linear(in_features=9548, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14-21): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
      "          (down_proj): Linear(in_features=9821, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26-27): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "12.480217088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32, 128]' is invalid for input of size 14848",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32, 128]' is invalid for input of size 14848"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa86d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6498629c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T05:21:12.307962Z",
     "start_time": "2024-05-21T05:21:12.304916Z"
    }
   },
   "source": [
    "## Experiment #6 --> PreserveRatio=0.7, lbound=0.65, rbound=1.0 (changes reward fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6983ef07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:35:06.706024Z",
     "start_time": "2024-05-21T08:35:06.701459Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.7_0.65_1.0_base_2428.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be74d28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:35:07.286844Z",
     "start_time": "2024-05-21T08:35:07.279191Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052756a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:29:01.225499Z",
     "start_time": "2024-05-21T08:35:07.743557Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  4096\n",
      "67 ==>  7155\n",
      "75 ==>  4096\n",
      "81 ==>  7155\n",
      "89 ==>  4096\n",
      "95 ==>  7155\n",
      "103 ==>  4096\n",
      "109 ==>  7155\n",
      "117 ==>  4096\n",
      "123 ==>  7155\n",
      "131 ==>  4096\n",
      "137 ==>  7155\n",
      "145 ==>  4096\n",
      "151 ==>  7155\n",
      "159 ==>  4096\n",
      "165 ==>  7155\n",
      "173 ==>  4096\n",
      "179 ==>  7155\n",
      "187 ==>  4096\n",
      "193 ==>  7345\n",
      "201 ==>  3584\n",
      "207 ==>  8227\n",
      "215 ==>  2688\n",
      "221 ==>  7155\n",
      "229 ==>  2688\n",
      "235 ==>  7155\n",
      "243 ==>  2688\n",
      "249 ==>  7155\n",
      "257 ==>  2688\n",
      "263 ==>  7155\n",
      "271 ==>  2688\n",
      "277 ==>  7155\n",
      "285 ==>  2688\n",
      "291 ==>  7155\n",
      "299 ==>  2688\n",
      "305 ==>  7155\n",
      "313 ==>  2688\n",
      "319 ==>  7155\n",
      "327 ==>  2688\n",
      "333 ==>  7155\n",
      "341 ==>  2688\n",
      "347 ==>  7155\n",
      "355 ==>  2688\n",
      "361 ==>  7155\n",
      "369 ==>  2688\n",
      "375 ==>  7155\n",
      "383 ==>  2688\n",
      "389 ==>  7155\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:02<00:51,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:02<00:27,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:03<00:19,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:03<00:15,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:04<00:13,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:04<00:11,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:05<00:10,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:05<00:10,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:07<00:12,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:07<00:10,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:08<00:09,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:08<00:07,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:09<00:06,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:09<00:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:10<00:06,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:11<00:05,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:11<00:04,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:12<00:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:13<00:03,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:13<00:03,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:14<00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:15<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:15<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:16<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:16<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  35.05202865600586\n",
      "Perplexity on wikitext2:  35.05202865600586\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:14:11:34,808 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:14:11:34,822 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:14:11:42,552 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:14:11:42,562 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:14:12:51,169 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:14:12:51,180 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:14:12:51,209 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:14:12:51,213 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a047957eb74adbb6c510ff6da4e2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:14:14:38,238 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:14:14:38,243 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:14:14:46,357 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:14:14:46,377 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:14:14:58,178 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:14:14:58,196 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:14:14:58,197 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:14:14:58,198 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:14:14:58,198 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:14:14:58,199 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:14:14:58,200 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:14:14:58,204 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 372.71it/s]\n",
      "2024-05-21:14:14:58,510 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1361.75it/s]\n",
      "2024-05-21:14:14:58,637 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 375.89it/s]\n",
      "2024-05-21:14:14:58,924 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 114.80it/s]\n",
      "2024-05-21:14:14:59,931 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 329.17it/s]\n",
      "2024-05-21:14:15:00,319 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 315.27it/s]\n",
      "2024-05-21:14:15:00,698 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 47313.07it/s]\n",
      "2024-05-21:14:15:00,720 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [08:35<00:00,  4.26it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "| arc_challenge |    0.24   | 0.042923469599092816 |      0.35      |   0.0479372485441102  |\n",
      "|      rte      |    0.52   | 0.050211673156867795 |      None      |          None         |\n",
      "|     boolq     |    0.65   |  0.0479372485441102  |      None      |          None         |\n",
      "|    arc_easy   |    0.47   | 0.050161355804659205 |      0.49      |  0.05024183937956912  |\n",
      "|   hellaswag   |    0.4    | 0.04923659639173309  |      0.55      |          0.05         |\n",
      "|   openbookqa  |    0.19   | 0.03942772444036624  |      0.33      |  0.047258156262526045 |\n",
      "|   winogrande  |    0.58   | 0.049604496374885836 |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow.\n",
      "1. Start by preparing the soil in the garden. This is a good time to consider the soil in the garden. This is a good time to start by removing the soil in the garden.\n",
      "2. Planting the soil in the garden. This is a good time to plant the soil in the garden.\n",
      "3. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "4. Filling the soil in the garden. This is a good time to fill the soil in the garden.\n",
      "5. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "6. Growing the soil in the garden. This is a good time to grow the soil in the garden.\n",
      "7. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "8. Growing the soil in the garden. This is a good time to grow the soil in the garden.\n",
      "9. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "10. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "11. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "12. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "13. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "14. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "15. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "16. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "17. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "18. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "19. Watering the soil in the garden. This is a good time to water the soil in the garden.\n",
      "20. Watering the soil in the garden. This is a good time to water the soil in the garden.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow. 1. Start by preparing the soil in the garden. This is a good time to consider the soil in the garden. This is a good time to start by removing the soil in the garden. 2. Planting the soil in the garden. This is a good time to plant the soil in the garden. 3. Watering the soil in the garden. This is a good time to water the soil in the garden. 4. Filling the soil in the garden. This is a good time to fill the soil in the garden. 5. 6. Growing the soil in the garden. This is a good time to grow the soil in the garden. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m a sense of community and connection.\n",
      "The importance of family is often understated. It is a fact that the family is the most important thing in the world. It is a fact that the family is the most important thing in the world. It is a fact that the family is the most important thing in the world.\n",
      "The family is the most important thing in the world because it is the family that is the most important thing in the world.\n",
      "The family is the most important thing in the world because the family is the most important thing in the world.\n",
      "The family is the most important thing in the world because the family is the most important thing in the world.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important thing in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "The family is the most important in the world because the family is the most important.\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91ma sense of community and connection. The importance of family is often understated. It is a fact that the family is the most important thing in the world. The family is the most important thing in the world because it is the family that is the most important thing in the world. The family is the most important thing in the world because the family is the most important thing in the world. The family is the most important thing in the world because the family is the most important.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will be a form of expression that is more inclusive and more diverse. This will be a form of expression that is more inclusive and more.\n",
      "The fashion industry is a large and diverse industry that is growing rapidly. This is a huge and large industry that is growing rapidly. The fashion industry is a large and large industry that is growing rapidly. The fashion industry is a large and large industry that is growing rapidly. The fashion industry is a large and small industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly. The fashion industry\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will be a form of expression that is more inclusive and more diverse. This will be a form of expression that is more inclusive and more. The fashion industry is a large and diverse industry that is growing rapidly. This is a huge and large industry that is growing rapidly. The fashion industry is a large and large industry that is growing rapidly. The fashion industry is a large and small industry that is growing rapidly. The fashion industry is a small industry that is growing rapidly. The fashion industry is a large industry that is growing rapidly.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m innovative companies to improve patient outcomes and reduce healthcare costs.\n",
      "\n",
      "### 2. The Internet of Things\n",
      "\n",
      "The Internet of Things, also known as the Internet of the Internet of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the Industry Energy is driving the industry.\n",
      "\n",
      "### 4. The Cloud\n",
      "\n",
      "\n",
      "The Cloud is a term that refers to the Internet as a service.\n",
      "\n",
      "### 5. The Cloud\n",
      "\n",
      "The Cloud is a term that refers to the Internet as a service.\n",
      "\n",
      "### 6. The Cloud\n",
      "\n",
      "The Cloud is a term that refers to the Internet as a service.\n",
      "\n",
      "### 7. The of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91minnovative companies to improve patient outcomes and reduce healthcare costs. ### 2. The Internet of Things\n",
      "\n",
      "The Internet of Things, also known as the Internet of the Internet of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the Industry Energy is driving the industry. ### 4. The Cloud\n",
      "\n",
      "\n",
      "The Cloud is a term that refers to the Internet as a service. ### 5. The Cloud\n",
      "\n",
      "The Cloud is a term that refers to the Internet as a service. ### 6. ### 7.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m help you to make a more accurate prediction of the market’s future.\n",
      "The market is a dynamic one. It is important to keep this in mind when making a decision to buy or sell a stock. The market is a dynamic market that is constantly changing. This means that you need to be able to predict the market’s future. This is a good way to predict the market’s future.\n",
      "The market is a dynamic market that is constantly changing. This means that you need to be able to predict the market’s future. This is a good way to predict the market’s future.\n",
      "The market is a dynamic market that is constantly changing. This means that you need to be able to predict the market’s future. This is a good way to predict the market’s future.\n",
      "The market is a dynamic market that is constantly changing. This means that you need to be able to predict the market’s future. This is a good way to predict the market’s future.\n",
      "The market is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that is a market that\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mhelp you to make a more accurate prediction of the market’s future. The market is a dynamic one. It is important to keep this in mind when making a decision to buy or sell a stock. The market is a dynamic market that is constantly changing. This means that you need to be able to predict the market’s future. This is a good way to predict the market’s future.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will continue to improve.\n",
      "The future of technology is likely to be one in which the technology is able to learn and understand.\n",
      "The future of technology is likely to be one in which the technology is able to learn and understand, and, in turn, to be able to communicate with humans.\n",
      "The future of technology is likely to be one in which the technology is able to learn and understand,, and, in turn, to communicate with humans.\n",
      "The future of technology is likely to be one of the most important aspects of human life.\n",
      "The future of technology is likely to be one of the most important aspects of human life.\n",
      "The future of technology is likely to be one of the most important aspects of human life.\n",
      "The future of technology is likely to be one of the most important aspects of human life.\n",
      "The future of technology is likely to be a major part of human life.\n",
      "The future of technology is likely to be a major part of human life.\n",
      "The future of technology is likely to be a major part of human life.\n",
      "The future of technology is likely to be a major part of human life.\n",
      "The future of technology is likely to be a major part of human life.\n",
      "The future of technology is likely to be a major part of human life.\n",
      "The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of human life.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of the world.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of the world.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of the world.\n",
      "The future of technology is a major part of human life. The future of technology is a major part of the world.\n",
      "The future of technology is a major part of human life. The future of technology is a major\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will continue to improve. The future of technology is likely to be one in which the technology is able to learn and understand. The future of technology is likely to be one in which the technology is able to learn and understand, and, in turn, to be able to communicate with humans. The future of technology is likely to be one in which the technology is able to learn and understand,, and, in turn, to communicate with humans. The future of technology is likely to be one of the most important aspects of human life. The future of technology is likely to be a major part of human life. The future of technology is a major part of human life. The future of technology is a major part of the world.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m the world. The world is a complex system of roads, streets, and highways, and the car must be able to navigate these roads safely and efficiently.\n",
      "The car must be able to detect other vehicles, and the driver must be able to control the car.\n",
      "The car must be able to drive. The car must be able to drive. The car must be able to drive. The car must be able to drive. The car must be able to drive. The vehicle must be able to drive.\n",
      "The car must be able to drive. The vehicle must be able to drive. The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car must be able to drive.\n",
      "The car must be able to drive. The car can be to drive.\n",
      "The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive.\n",
      "The car can be to drive. The car can be to drive. The car can be to drive.\n",
      "The car can to drive. The car can to drive.\n",
      "The car can to drive. The car can to drive. The car can to drive.\n",
      "The car can to drive. The car can to drive. The car can to drive.\n",
      "The car can to drive. The\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91mthe world. The world is a complex system of roads, streets, and highways, and the car must be able to navigate these roads safely and efficiently. The car must be able to detect other vehicles, and the driver must be able to control the car. The car must be able to drive. The vehicle must be able to drive. The car can be to drive. The car can to drive.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m be used to create a more efficient and safe, and in the end, the system is able to be used to create a more secure and efficient.\n",
      "The ability to create a more efficient and more secure system is the ability to create a more efficient and more secure system is the ability to create a more efficient and more secure system is the ability to create a more efficient and more secure system is the ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient system is the ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more efficient.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and more.\n",
      "The ability to create a more efficient and\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mbe used to create a more efficient and safe, and in the end, the system is able to be used to create a more secure and efficient. The ability to create a more efficient and more secure system is the ability to create a more efficient and more secure system is the ability to create a more efficient and more secure system is the ability to create a more efficient and more secure system is the ability to create a more efficient and more efficient. The ability to create a more efficient and more efficient system is the ability to create a more efficient and more efficient. The ability to create a more efficient and more efficient. The ability to create a more efficient and more.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m are capable of learning and making decisions on their own.\n",
      "The future is a long way away from us. But the future is now. The future is now, and the future is a world in which artificial intelligence is a part of everyday life.\n",
      "The future is a world in which artificial intelligence is a part of everyday life, and the future is a world in which artificial intelligence is a part of everyday life.\n",
      "The future is a world in which artificial intelligence is a part of everyday life.\n",
      "The future is a world in which artificial intelligence is a.\n",
      "The future is a world in which the past is a.\n",
      "The future is a world in which the past is.\n",
      "The future is a world in which the present is.\n",
      "The past is a future.\n",
      "The future is a.\n",
      "The future is a.\n",
      "The past is a.\n",
      "The future is a.\n",
      "The future is a.\n",
      "The past is a.\n",
      "The future is a.\n",
      "The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mare capable of learning and making decisions on their own. The future is a long way away from us. But the future is now. The future is now, and the future is a world in which artificial intelligence is a part of everyday life. The future is a world in which artificial intelligence is a part of everyday life, and the future is a world in which artificial intelligence is a part of everyday life. The future is a world in which artificial intelligence is a part of everyday life. The future is a world in which artificial intelligence is a. The future is a world in which the past is a. The future is a world in which the past is. The future is a world in which the present is. The past is a future. The future is a. The past is a. The.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m the process of learning.\n",
      "\n",
      "Comment: I'm not sure I'm going to be able to explain this to them. I'm going to be able to explain it to myself.\n",
      "\n",
      "Comment: @Jim, I'm not sure I'm going to be able to explain this to you.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out to see them.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out.\n",
      "\n",
      "Comment: @J.S. I'm not going\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91mthe process of learning. Comment: I'm not sure I'm going to be able to explain this to them. I'm going to be able to explain it to myself. Comment: @Jim, I'm not sure I'm going to be able to explain this to you. Comment: @J. S. I'm not going to be able to see your comments. I'm going to have to wait until I'm not writing this out to see them. I'm going to have to wait until I'm not writing this out.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m be used to crack the security of many online banking websites.\n",
      "The problem of the day is a problem that is likely to be solved by the time the day is done. The problem is a problem that is likely to be solved by the time the day is done. The problem is a problem that is likely to be solved by the day.\n",
      "The problem is a problem that is solved by the day. The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day. The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is solved by the day.\n",
      "The problem is a problem that is a problem.\n",
      "The problem is a problem that is a.\n",
      "The problem is a problem that is a.m.\n",
      "The problem is a problem that is a.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.m. the.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mbe used to crack the security of many online banking websites. The problem of the day is a problem that is likely to be solved by the time the day is done. The problem is a problem that is likely to be solved by the time the day is done. The problem is a problem that is likely to be solved by the day. The problem is a problem that is solved by the day. The problem is a problem that is a problem. The problem is a problem that is a. m. the.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m will enable organizations to make the most of the huge amount of information.\n",
      "The future is likely to be a world in which the number of devices connected to the internet is increasing rapidly. This will require the development of new methods of handling large amounts of information. The technology of the future will be one in which the number of devices connected to the internet is increasing rapidly. The technology of the future will be one of the most important aspects of the future.\n",
      "The future is likely to be a world in which the number of devices connected to the internet is increasing rapidly. The technology of the future is a major part of the future. The technology of the future is a major part of the future.\n",
      "The future is a great place to work.\n",
      "The future is a great place to work.\n",
      "The future is a great place to work.\n",
      "The future is a great place to work. The future is a great place to work.\n",
      "The future is a great place to work. The future is a great place to work.\n",
      "The future is a great place to work. The future is a great place to work.\n",
      "The future is a great place to work. The future is a great place to work.\n",
      "The future is a great place to work. The future is a great place to work.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to live.\n",
      "The future is a great place to live. The future is a great place to.\n",
      "The future is a great place to. The future is a great place to.\n",
      "The future is a great place to. The future is a great place to.\n",
      "The future is a great place to. The future is a great place to.\n",
      "The future is a great place to. The future is a great place to. The future is a great place to.\n",
      "The future is a great place to. The future is a great place to. The future is\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mwill enable organizations to make the most of the huge amount of information. The future is likely to be a world in which the number of devices connected to the internet is increasing rapidly. This will require the development of new methods of handling large amounts of information. The technology of the future will be one in which the number of devices connected to the internet is increasing rapidly. The technology of the future will be one of the most important aspects of the future. The technology of the future is a major part of the future. The future is a great place to work. The future is a great place to live. The future is a great place to.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m using the model to predict on a smaller number of samples.\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{CNN} is a type that is a type of neural network. A computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer is a computer\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91musing the model to predict on a smaller number of samples. \\begin{itemize}\n",
      "\\item \\textbf{CNN} is a type that is a type of neural network.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m help humans understand and control the process of making decisions.\n",
      "The goal of this article is to provide a few tips on how to use the best possible tools for the job.\n",
      "The first step in the process of making a decision is to identify the problem you are trying to solve. This is a process that can be done by you.\n",
      "The second step in the process of making a decision is to identify the criteria you will be using to make the decision. This is a process that can be done by you.\n",
      "The third step in the process of making a decision is to identify the solutions you will be getting. This is a process that can be done by you.\n",
      "The fourth step in the process of making a decision is to identify the solutions you are getting. This is a process that can be done by you.\n",
      "The fifth step in the process of making a decision is to make the decision to make the decision to make the decision to make the decision to make the decision to make the decision to make the end of the article.\n",
      "The first step in the process of making a decision is to make the decision to make the time to make the time to make the time to make the time to make the time to make the end of the article.\n",
      "The second step in the process of making a decision is to make the time to make the end of the article.\n",
      "The third step in the process of making a decision is to make the time to make the end of the time to make the end of the time to make the end of the time to make the end of the time to make the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mhelp humans understand and control the process of making decisions. The goal of this article is to provide a few tips on how to use the best possible tools for the job. The first step in the process of making a decision is to identify the problem you are trying to solve. This is a process that can be done by you. The second step in the process of making a decision is to identify the criteria you will be using to make the decision. The third step in the process of making a decision is to identify the solutions you will be getting. The fourth step in the process of making a decision is to identify the solutions you are getting. The fifth step in the process of making a decision is to make the decision to make the decision to make the decision to make the decision to make the decision to make the decision to make the end of the article. The first step in the process of making a decision is to make the decision to make the time to make the time to make the time to make the time to make the time to make the end of the article. The second step in the process of making a decision is to make the time to make the end of the article.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the water is a sh of a blue.\n",
      "\n",
      "\\section{Conclusion}\n",
      "\n",
      "In this paper, we have discussed the use of computer-generated imagery in the film industry. We have discussed the use of computer-generated imagery in the film industry. We have discussed the use of computer-generated imagery in the film industry. We have discussed the use of computer-generated imagery in the film. We have discussed the use of computer.\n",
      "\n",
      "\\end{theo}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "\\end\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe water is a sh of a blue. \\section{Conclusion}\n",
      "\n",
      "In this paper, we have discussed the use of computer-generated imagery in the film industry. We have discussed the use of computer-generated imagery in the film industry. We have discussed the use of computer-generated imagery in the film. We have discussed the use of computer.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world.\n",
      "2. It’s free. You don’t have to spend a dime on your travels. You can travel the world on your own.\n",
      "3. You can travel the world on your own. You can travel the world on your own.\n",
      "4. You can travel the world on your own. You can travel the world on your own.\n",
      "5. You can travel the world on your own. You can travel the world on your own.\n",
      "6. You can travel the world on your own.\n",
      "7. You can travel the world on your own.\n",
      "8. You can read the article on your computer.\n",
      "9. You can watch the movie on your computer.\n",
      "10. You can watch the movie on your computer.\n",
      "11. You can watch the movie on your computer.\n",
      "12. You can do anything on your computer.\n",
      "13. You can do anything on your computer.\n",
      "14. You can do anything on your computer.\n",
      "15. You can do anything on your computer.\n",
      "16. You can watch the movie on your computer.\n",
      "17. You can do the movie on your computer.\n",
      "18. You can do anything on your computer.\n",
      "19. You can do everything on your computer.\n",
      "20. You can do everything on your computer.\n",
      "21. You can find the article on your computer.\n",
      "22. You can find the article on your computer.\n",
      "23. You can find the best of the best.\n",
      "24. You can find the best of the best.\n",
      "25. You can find the best of the best.\n",
      "26. You can find the best of the best.\n",
      "27. You can find the best of the best.\n",
      "28. You can find the best of the best.\n",
      "29. You can find the best of the best.\n",
      "30. You can get the best of the best.\n",
      "31. You can get the best of the best.\n",
      "32. You can get the best of the best.\n",
      "33. You can get the best of the best.\n",
      "34. You can get the best of the best.\n",
      "35. You can get the best of the best.\n",
      "36. You can get the best of the best.\n",
      "37. You can get the best of the best.\n",
      "38. You can get the best\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world. 2. It’s free. You don’t have to spend a dime on your travels. You can travel the world on your own. 3. 4. 5. 6. 7. 8. You can read the article on your computer. 9. You can watch the movie on your computer. 10. 11. 12. You can do anything on your computer. 13. 14. 15. 16. 17. You can do the movie on your computer. 18. 19. You can do everything on your computer. 20. 21. You can find the article on your computer. 22. 23. You can find the best of the best. 24. 25. 26. 27. 28. 29. 30. You can get the best of the best. 31. 32. 33. 34. 35. 36. 37. 38.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.419896832\n",
      "Model size after pruning:  13.419896832\n",
      "Model size after pruning:  13.325205504\n",
      "Model size after pruning:  13.325205504\n",
      "Model size after pruning:  13.230514176\n",
      "Model size after pruning:  13.230514176\n",
      "Model size after pruning:  13.135822848\n",
      "Model size after pruning:  13.135822848\n",
      "Model size after pruning:  13.04113152\n",
      "Model size after pruning:  13.04113152\n",
      "Model size after pruning:  12.946440192\n",
      "Model size after pruning:  12.946440192\n",
      "Model size after pruning:  12.851748864\n",
      "Model size after pruning:  12.851748864\n",
      "Model size after pruning:  12.757057536\n",
      "Model size after pruning:  12.757057536\n",
      "Model size after pruning:  12.662366208\n",
      "Model size after pruning:  12.662366208\n",
      "Model size after pruning:  12.57234432\n",
      "Model size after pruning:  12.555567104\n",
      "Model size after pruning:  12.487221248\n",
      "Model size after pruning:  12.441083904\n",
      "Model size after pruning:  12.346392576\n",
      "Model size after pruning:  12.300255232\n",
      "Model size after pruning:  12.205563904\n",
      "Model size after pruning:  12.15942656\n",
      "Model size after pruning:  12.064735232\n",
      "Model size after pruning:  12.018597888\n",
      "Model size after pruning:  11.92390656\n",
      "Model size after pruning:  11.877769216\n",
      "Model size after pruning:  11.783077888\n",
      "Model size after pruning:  11.736940544\n",
      "Model size after pruning:  11.642249216\n",
      "Model size after pruning:  11.596111872\n",
      "Model size after pruning:  11.501420544\n",
      "Model size after pruning:  11.4552832\n",
      "Model size after pruning:  11.360591872\n",
      "Model size after pruning:  11.314454528\n",
      "Model size after pruning:  11.2197632\n",
      "Model size after pruning:  11.173625856\n",
      "Model size after pruning:  11.078934528\n",
      "Model size after pruning:  11.032797184\n",
      "Model size after pruning:  10.938105856\n",
      "Model size after pruning:  10.891968512\n",
      "Model size after pruning:  10.797277184\n",
      "Model size after pruning:  10.75113984\n",
      "Model size after pruning:  10.656448512\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4-12): 9 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7345, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7345, bias=False)\n",
      "          (down_proj): Linear(in_features=7345, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=8227, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=8227, bias=False)\n",
      "          (down_proj): Linear(in_features=8227, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15-27): 13 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
      "          (o_proj): Linear(in_features=2688, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "10.656448512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32, 128]' is invalid for input of size 14336",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32, 128]' is invalid for input of size 14336"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306a06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3240c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b00d004",
   "metadata": {},
   "source": [
    "# Chat Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5987b8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:59:27.716221Z",
     "start_time": "2024-05-21T10:59:01.155096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 16:29:02.060747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 16:29:03.233174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12d897b61494a0cab6ebb721b786d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_chat_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_chat_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb11c0",
   "metadata": {},
   "source": [
    "## Experiment #1 --> preserve_ratio= 0.95, lbound = 0.8, rbound = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea6dfa77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:51:22.476324Z",
     "start_time": "2024-05-21T09:51:22.472495Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_1.0_chat_2430.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d0ced2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:51:23.532467Z",
     "start_time": "2024-05-21T09:51:23.528185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1b916e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:51:24.861227Z",
     "start_time": "2024-05-21T09:51:24.812589Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb51a55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:20:10.176308Z",
     "start_time": "2024-05-21T09:51:25.703682Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  4096\n",
      "67 ==>  11007\n",
      "75 ==>  4096\n",
      "81 ==>  11007\n",
      "89 ==>  4096\n",
      "95 ==>  11006\n",
      "103 ==>  4096\n",
      "109 ==>  11007\n",
      "117 ==>  4096\n",
      "123 ==>  11007\n",
      "131 ==>  4096\n",
      "137 ==>  11006\n",
      "145 ==>  4096\n",
      "151 ==>  11005\n",
      "159 ==>  4096\n",
      "165 ==>  11008\n",
      "173 ==>  4096\n",
      "179 ==>  11005\n",
      "187 ==>  4096\n",
      "193 ==>  11007\n",
      "201 ==>  4096\n",
      "207 ==>  11005\n",
      "215 ==>  4096\n",
      "221 ==>  11001\n",
      "229 ==>  4096\n",
      "235 ==>  11007\n",
      "243 ==>  4096\n",
      "249 ==>  11002\n",
      "257 ==>  4096\n",
      "263 ==>  11007\n",
      "271 ==>  4096\n",
      "277 ==>  11005\n",
      "285 ==>  4096\n",
      "291 ==>  11006\n",
      "299 ==>  4096\n",
      "305 ==>  11008\n",
      "313 ==>  4096\n",
      "319 ==>  9587\n",
      "327 ==>  3328\n",
      "333 ==>  8806\n",
      "341 ==>  3328\n",
      "347 ==>  8806\n",
      "355 ==>  3328\n",
      "361 ==>  8806\n",
      "369 ==>  3328\n",
      "375 ==>  8806\n",
      "383 ==>  3328\n",
      "389 ==>  8806\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:01<00:33,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:20,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:02<00:16,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:03<00:14,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:03<00:12,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:04<00:11,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:04<00:10,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:05<00:09,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:06<00:09,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:06<00:08,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:07<00:08,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:07<00:08,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:08<00:07,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:08<00:06,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:09<00:05,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:09<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:10<00:04,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:11<00:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:11<00:03,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:12<00:02,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:12<00:02,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:13<00:01,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:14<00:01,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:14<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:15<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  7.793191909790039\n",
      "Perplexity on wikitext2:  7.793191909790039\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:15:25:21,940 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:15:25:21,945 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:15:25:28,526 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:15:25:28,530 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:15:26:31,711 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:15:26:31,713 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:15:26:31,728 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:15:26:31,729 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e12f32bce940d68fd6b6c402ff2db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:15:27:21,045 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:15:27:21,050 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:15:27:58,789 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:15:27:58,792 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:15:28:16,217 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:15:28:16,222 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:15:28:16,226 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:15:28:16,236 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:15:28:16,240 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:15:28:16,245 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:15:28:16,250 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:15:28:16,256 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 244.86it/s]\n",
      "2024-05-21:15:28:16,690 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 564.57it/s]\n",
      "2024-05-21:15:28:16,918 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 519.78it/s]\n",
      "2024-05-21:15:28:17,138 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 44374.78it/s]\n",
      "2024-05-21:15:28:17,172 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 474.86it/s]\n",
      "2024-05-21:15:28:17,431 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 217.80it/s]\n",
      "2024-05-21:15:28:17,906 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 754.67it/s]\n",
      "2024-05-21:15:28:18,105 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [05:48<00:00,  6.30it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|    arc_easy   |    0.7    | 0.046056618647183814 |      0.7       |  0.046056618647183814 |\n",
      "|   openbookqa  |    0.32   | 0.04688261722621505  |      0.4       |  0.04923659639173309  |\n",
      "|      rte      |    0.75   | 0.04351941398892446  |      None      |          None         |\n",
      "|   winogrande  |    0.69   | 0.04648231987117316  |      None      |          None         |\n",
      "|   hellaswag   |    0.52   | 0.05021167315686781  |      0.7       |  0.046056618647183814 |\n",
      "| arc_challenge |    0.44   | 0.04988876515698589  |      0.44      |  0.04988876515698589  |\n",
      "|     boolq     |    0.77   | 0.04229525846816505  |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Think of some family rules to promote a healthy family relationship \u001b[0m\u001b[91mGreat! Here are some family rules that can help promote a healthy family relationship:\n",
      "\n",
      "1. Practice active listening: Make sure to listen to each other without interrupting or judging. Give each person the space to express their thoughts and feelings.\n",
      "2. Respect each other's boundaries: It's important to respect each other's personal space and privacy. Don't intrude on someone's alone time without their permission.\n",
      "3. Use \"I\" statements: When expressing a concern or issue, use \"I\" statements instead of \"you\" statements. This can help avoid blame and defensiveness.\n",
      "4. Avoid criticizing or judging each other: Focus on finding solutions together instead of placing blame or judging each other.\n",
      "5. Show appreciation and gratitude: Make an effort to express gratitude and appreciation for each other. This can help strengthen the bond between family members.\n",
      "6. Take turns: Take turns in sharing your thoughts, feelings, and experiences. This can help create a sense of fairness and equality within the family.\n",
      "7. Be open-minded: Be open to different perspectives and ideas. Avoid being rigid in your beliefs and be willing to listen to others.\n",
      "8. Practice forgiveness: Forgive each other for mistakes and hurtful actions. Holding onto resentment can create tension and conflict within the family.\n",
      "9. Show empathy: Make an effort to understand each other's emotions and perspectives. Show empathy and support when someone is going through a difficult time.\n",
      "10. Have fun together: Make time for fun and enjoyable activities together. This can help create a positive and loving atmosphere within the family.\n",
      "\n",
      "Remember, family rules are not meant to be rigid or restrictive. They are meant to provide a framework for healthy communication and relationship building within the family. By following these rules, you can create a positive and loving environment for everyone in the family.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Think of some family rules to promote a healthy family relationship \u001b[0m\u001b[91mGreat! Here are some family rules that can help promote a healthy family relationship:\n",
      "\n",
      "1. Practice active listening: Make sure to listen to each other without interrupting or judging. Give each person the space to express their thoughts and feelings. 2. Respect each other's boundaries: It's important to respect each other's personal space and privacy. Don't intrude on someone's alone time without their permission. 3. Use \"I\" statements: When expressing a concern or issue, use \"I\" statements instead of \"you\" statements. This can help avoid blame and defensiveness. 4. Avoid criticizing or judging each other: Focus on finding solutions together instead of placing blame or judging each other. 5. Show appreciation and gratitude: Make an effort to express gratitude and appreciation for each other. This can help strengthen the bond between family members. 6. Take turns: Take turns in sharing your thoughts, feelings, and experiences. This can help create a sense of fairness and equality within the family. 7. Be open-minded: Be open to different perspectives and ideas. Avoid being rigid in your beliefs and be willing to listen to others. 8. Practice forgiveness: Forgive each other for mistakes and hurtful actions. Holding onto resentment can create tension and conflict within the family. 9. Show empathy: Make an effort to understand each other's emotions and perspectives. Show empathy and support when someone is going through a difficult time. 10. Have fun together: Make time for fun and enjoyable activities together. This can help create a positive and loving atmosphere within the family. Remember, family rules are not meant to be rigid or restrictive. They are meant to provide a framework for healthy communication and relationship building within the family. By following these rules, you can create a positive and loving environment for everyone in the family.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the series A Song of Ice and Fire, who is the founder of House Karstark? \u001b[0m\u001b[91mThank you for asking! I'm happy to help you with your question. However, I must inform you that there is no such character named \"House Karstark\" in the A Song of Ice and Fire series by George R. R. Martin.\n",
      "It's possible that you may be thinking of House Karstayn, which is a House in the series. House Karstayn is a noble House from the North, known for their loyalty to the Starks and their involvement in the War of the Usurper. They are not a major House in the series, but they do play a small role in the story.\n",
      "If you have any other questions or if there's anything else I can help you with, please feel free to ask!\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the series A Song of Ice and Fire, who is the founder of House Karstark? \u001b[0m\u001b[91mThank you for asking! I'm happy to help you with your question. However, I must inform you that there is no such character named \"House Karstark\" in the A Song of Ice and Fire series by George R. R. Martin. It's possible that you may be thinking of House Karstayn, which is a House in the series. House Karstayn is a noble House from the North, known for their loyalty to the Starks and their involvement in the War of the Usurper. They are not a major House in the series, but they do play a small role in the story.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m which weighs more, cold or hot water? \u001b[0m\u001b[91mThank you for your question! I'm here to help you with any questions you may have, while being safe and respectful. However, I must point out that the question you've asked doesn't make sense as cold and hot water have the same weight. They are both forms of water with the same density and therefore, they weigh the same amount. So, neither cold nor hot water weighs more than the other. I hope you have any other questions that are factually correct and I'll be happy to help! 😊\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m which weighs more, cold or hot water? \u001b[0m\u001b[91mThank you for your question! I'm here to help you with any questions you may have, while being safe and respectful. However, I must point out that the question you've asked doesn't make sense as cold and hot water have the same weight. They are both forms of water with the same density and therefore, they weigh the same amount. So, neither cold nor hot water weighs more than the other.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Write a short paragraph about why you should not have both a pet cat and a pet bird. \u001b[0m\u001b[91mHello! I'm here to help you with your question. However, I must inform you that it is not advisable to have both a pet cat and a pet bird as they are both wild animals that belong in their natural habitats, not as domesticated pets. Both cats and birds have specific needs and requirements that are difficult to meet in a household environment, and it is important to prioritize their welfare and well-being. For example, cats are predators that require a constant supply of food and water, and they also need a lot of space to roam and exercise. Similarly, birds require a lot of attention and care, including regular feeding, cleaning of their cages, and providing them with appropriate toys and stimulation.\n",
      "It is important to consider the ethical implications of keeping two different species as pets, as it can be harmful to both the animals and the humans involved. Instead of having both a cat and a bird, it is recommended to choose one species that you are able to provide the proper care and attention to. This will not only ensure the well-being of the animal, but it will also help to prevent any potential conflicts or problems that may arise from having two different species in the same household.\n",
      "I hope this information is helpful. If you have any other questions, please feel free to ask!\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Write a short paragraph about why you should not have both a pet cat and a pet bird. \u001b[0m\u001b[91mHello! I'm here to help you with your question. However, I must inform you that it is not advisable to have both a pet cat and a pet bird as they are both wild animals that belong in their natural habitats, not as domesticated pets. Both cats and birds have specific needs and requirements that are difficult to meet in a household environment, and it is important to prioritize their welfare and well-being. For example, cats are predators that require a constant supply of food and water, and they also need a lot of space to roam and exercise. Similarly, birds require a lot of attention and care, including regular feeding, cleaning of their cages, and providing them with appropriate toys and stimulation. It is important to consider the ethical implications of keeping two different species as pets, as it can be harmful to both the animals and the humans involved. Instead of having both a cat and a bird, it is recommended to choose one species that you are able to provide the proper care and attention to. This will not only ensure the well-being of the animal, but it will also help to prevent any potential conflicts or problems that may arise from having two different species in the same household. I hope this information is helpful.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Is beauty objective or subjective? \u001b[0m\u001b[91mHello! I'm here to help you with your question. The concept of beauty is a complex and multifaceted one, and there is ongoing debate among philosophers, psychologists, and cultural critics about whether it is objective or subjective.\n",
      "Objective beauty refers to the idea that there are objective standards of beauty that are independent of individual perspectives and cultural norms. According to this view, certain features or characteristics are inherently beautiful, regardless of how they are perceived or valued by different people. For example, some might argue that the symmetry of a face or the proportions of a body are objective features of beauty.\n",
      "On the other hand, subjective beauty refers to the idea that beauty is a personal and subjective experience that is shaped by individual tastes, cultural norms, and historical context. According to this view, what one person finds beautiful may not be the same for another, and there is no objective standard of beauty that applies to everyone.\n",
      "While there is no consensus on the nature of beauty, many philosophers and psychologists argue that it is likely to be a combination of both objective and subjective factors. For example, a face may have objective features of symmetry and proportions that are widely considered to be beautiful, but the way that these features are perceived and valued can vary greatly across different cultures and individuals.\n",
      "In conclusion, whether beauty is objective or subjective is a matter of ongoing debate, and there is no clear answer. However, it is important to recognize that beauty is a complex and multifaceted concept that is shaped by a variety of factors, including cultural norms, individual tastes, and historical context.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Is beauty objective or subjective? \u001b[0m\u001b[91mHello! I'm here to help you with your question. The concept of beauty is a complex and multifaceted one, and there is ongoing debate among philosophers, psychologists, and cultural critics about whether it is objective or subjective. Objective beauty refers to the idea that there are objective standards of beauty that are independent of individual perspectives and cultural norms. According to this view, certain features or characteristics are inherently beautiful, regardless of how they are perceived or valued by different people. For example, some might argue that the symmetry of a face or the proportions of a body are objective features of beauty. On the other hand, subjective beauty refers to the idea that beauty is a personal and subjective experience that is shaped by individual tastes, cultural norms, and historical context. According to this view, what one person finds beautiful may not be the same for another, and there is no objective standard of beauty that applies to everyone. While there is no consensus on the nature of beauty, many philosophers and psychologists argue that it is likely to be a combination of both objective and subjective factors. For example, a face may have objective features of symmetry and proportions that are widely considered to be beautiful, but the way that these features are perceived and valued can vary greatly across different cultures and individuals. In conclusion, whether beauty is objective or subjective is a matter of ongoing debate, and there is no clear answer. However, it is important to recognize that beauty is a complex and multifaceted concept that is shaped by a variety of factors, including cultural norms, individual tastes, and historical context.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is SVM? \u001b[0m\u001b[91mHello! I'm here to help you with your question. SVM (Support Vector Machine) is a type of supervised learning algorithm used in machine learning. It's a popular choice for classification problems, particularly when dealing with large datasets.\n",
      "In simple terms, SVM tries to find the best hyperplane (a line or plane) that can be used to separate two different classes of data. The goal is to maximize the margin between the two classes, which can improve the accuracy of the predictions.\n",
      "There are different types of SVMs, including linear SVM, polynomial SVM, and neural network-based SVM. Each type has its own strengths and weaknesses, and the choice of which one to use depends on the specific problem being solved.\n",
      "I hope this explanation helps! If you have any further questions or need more information, feel free to ask.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is SVM? \u001b[0m\u001b[91mHello! I'm here to help you with your question. SVM (Support Vector Machine) is a type of supervised learning algorithm used in machine learning. It's a popular choice for classification problems, particularly when dealing with large datasets. In simple terms, SVM tries to find the best hyperplane (a line or plane) that can be used to separate two different classes of data. The goal is to maximize the margin between the two classes, which can improve the accuracy of the predictions. There are different types of SVMs, including linear SVM, polynomial SVM, and neural network-based SVM. Each type has its own strengths and weaknesses, and the choice of which one to use depends on the specific problem being solved. I hope this explanation helps! If you have any further questions or need more information, feel free to ask.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is the current capital of Japan? \u001b[0m\u001b[91mThank you for asking! The current capital of Japan is Tokyo. I'm glad to provide you with accurate and reliable information. However, I must point out that the question is quite simple and easy to answer, and there's no need to provide any further explanation. Is there anything else I can help you with?\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is the current capital of Japan? \u001b[0m\u001b[91mThank you for asking! The current capital of Japan is Tokyo. I'm glad to provide you with accurate and reliable information. However, I must point out that the question is quite simple and easy to answer, and there's no need to provide any further explanation.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Name 10 colors \u001b[0m\u001b[91mOf course! I'd be happy to help you with that! Here are 10 colors:\n",
      "\n",
      "1. Red\n",
      "2. Blue\n",
      "3. Green\n",
      "4. Yellow\n",
      "5. Orange\n",
      "6. Purple\n",
      "7. Pink\n",
      "8. Brown\n",
      "9. Grey\n",
      "10. Black\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Name 10 colors \u001b[0m\u001b[91mOf course! I'd be happy to help you with that! Here are 10 colors:\n",
      "\n",
      "1. Red\n",
      "2. Blue\n",
      "3. Green\n",
      "4. Yellow\n",
      "5. Orange\n",
      "6. Purple\n",
      "7. Pink\n",
      "8. Brown\n",
      "9. Grey\n",
      "10. Black\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m How should I invest my money? \u001b[0m\u001b[91mThank you for asking! I'm glad you're thinking about investing your money. However, I must point out that the question is quite broad and there are many factors to consider before making any investment decisions. It's important to understand your financial goals, risk tolerance, and time horizon before investing.\n",
      "I cannot provide personalized financial advice, nor can I recommend any specific investment products. However, I can offer some general guidance on how to approach investing.\n",
      "Firstly, it's important to understand the different types of investments available, such as stocks, bonds, mutual funds, and real estate. Each of these investments has its own risks and potential returns, and it's important to consider which one(s) align with your financial goals and risk tolerance.\n",
      "Secondly, it's important to do your own research and due diligence on any potential investment opportunities. This includes reading up on the company's financial statements, understanding the industry and market trends, and assessing the potential risks and returns.\n",
      "Lastly, it's important to diversify your investment portfolio by investing in a variety of assets and industries. This can help to minimize risk and maximize potential returns.\n",
      "I hope this information is helpful in getting you started on your investment journey. However, please keep in mind that investing always carries some level of risk, and it's important to consult with a financial advisor or professional before making any investment decisions.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m How should I invest my money? \u001b[0m\u001b[91mThank you for asking! I'm glad you're thinking about investing your money. However, I must point out that the question is quite broad and there are many factors to consider before making any investment decisions. It's important to understand your financial goals, risk tolerance, and time horizon before investing. I cannot provide personalized financial advice, nor can I recommend any specific investment products. However, I can offer some general guidance on how to approach investing. Firstly, it's important to understand the different types of investments available, such as stocks, bonds, mutual funds, and real estate. Each of these investments has its own risks and potential returns, and it's important to consider which one(s) align with your financial goals and risk tolerance. Secondly, it's important to do your own research and due diligence on any potential investment opportunities. This includes reading up on the company's financial statements, understanding the industry and market trends, and assessing the potential risks and returns. Lastly, it's important to diversify your investment portfolio by investing in a variety of assets and industries. This can help to minimize risk and maximize potential returns. I hope this information is helpful in getting you started on your investment journey. However, please keep in mind that investing always carries some level of risk, and it's important to consult with a financial advisor or professional before making any investment decisions.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What are some ways to improve the value of your home? \u001b[0m\u001b[91mThank you for your kind and respectful request! I'm here to help you with any questions you may have, while ensuring that my responses are safe, socially unbiased, and positive in nature.\n",
      "To improve the value of your home, here are some suggestions:\n",
      "\n",
      "1. Update your home's curb appeal: The exterior of your home is the first thing potential buyers will see. Make sure your home's exterior is well-maintained, clean, and visually appealing. This can be done by adding some landscaping, painting the front door, and updating the siding or roof.\n",
      "\n",
      "2. Renovate your kitchen and bathrooms: The kitchen and bathrooms are the most important rooms in a home, and they can greatly impact a home's value. Consider updating your kitchen with new appliances, countertops, and cabinets. Similarly, updating your bathrooms with new fixtures, tiles, and a fresh coat of paint can make a big difference.\n",
      "\n",
      "3. Add energy-efficient features: Energy-efficient features are highly valued by homebuyers. Consider adding solar panels, a new furnace, or energy-efficient windows to make your home more energy-efficient.\n",
      "\n",
      "4. Create a functional and comfortable outdoor living space: An outdoor living space can be a great selling point for potential buyers. Consider adding a deck, patio, or garden to your home, and make sure it's well-maintained and comfortable.\n",
      "\n",
      "5. Declutter and organize: A clutter-free and well-organized home can make a big difference in a buyer's perception of your home's value. Consider decluttering and organizing your home, and making sure everything has a place.\n",
      "\n",
      "I hope these suggestions are helpful in improving the value of your home. If you have any other questions, please feel free to ask!\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What are some ways to improve the value of your home? \u001b[0m\u001b[91mThank you for your kind and respectful request! I'm here to help you with any questions you may have, while ensuring that my responses are safe, socially unbiased, and positive in nature. To improve the value of your home, here are some suggestions:\n",
      "\n",
      "1. Update your home's curb appeal: The exterior of your home is the first thing potential buyers will see. Make sure your home's exterior is well-maintained, clean, and visually appealing. This can be done by adding some landscaping, painting the front door, and updating the siding or roof. 2. Renovate your kitchen and bathrooms: The kitchen and bathrooms are the most important rooms in a home, and they can greatly impact a home's value. Consider updating your kitchen with new appliances, countertops, and cabinets. Similarly, updating your bathrooms with new fixtures, tiles, and a fresh coat of paint can make a big difference. 3. Add energy-efficient features: Energy-efficient features are highly valued by homebuyers. Consider adding solar panels, a new furnace, or energy-efficient windows to make your home more energy-efficient. 4. Create a functional and comfortable outdoor living space: An outdoor living space can be a great selling point for potential buyers. Consider adding a deck, patio, or garden to your home, and make sure it's well-maintained and comfortable. 5. Declutter and organize: A clutter-free and well-organized home can make a big difference in a buyer's perception of your home's value. Consider decluttering and organizing your home, and making sure everything has a place. I hope these suggestions are helpful in improving the value of your home.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What does fasting mean? \u001b[0m\u001b[91mThank you for asking! Fasting is a term that refers to the practice of abstaining from food and drink for a period of time, typically for spiritual, medical, or other reasons. There are many different types of fasting, including:\n",
      "1. Total fasting: This involves abstaining from all food and drink, including water.\n",
      "2. Intermittent fasting: This involves alternating periods of eating and fasting.\n",
      "3. Partial fasting: This involves limiting certain foods or drinks, but not all of them.\n",
      "It's important to note that fasting is not appropriate or safe for everyone, especially children, pregnant or breastfeeding women, people with certain medical conditions, and those who are malnourished or underweight. Before starting any type of fasting, it's important to consult with a healthcare professional to determine if it's safe and appropriate for you.\n",
      "I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What does fasting mean? \u001b[0m\u001b[91mThank you for asking! Fasting is a term that refers to the practice of abstaining from food and drink for a period of time, typically for spiritual, medical, or other reasons. There are many different types of fasting, including:\n",
      "1. Total fasting: This involves abstaining from all food and drink, including water. 2. Intermittent fasting: This involves alternating periods of eating and fasting. 3. Partial fasting: This involves limiting certain foods or drinks, but not all of them. It's important to note that fasting is not appropriate or safe for everyone, especially children, pregnant or breastfeeding women, people with certain medical conditions, and those who are malnourished or underweight. Before starting any type of fasting, it's important to consult with a healthcare professional to determine if it's safe and appropriate for you. I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is cloud computing in simple terms? \u001b[0m\u001b[91mHello! I'm here to help you with your question. Cloud computing is a way for people to use computer resources like servers, storage, and applications over the internet instead of having them physically located on their own computers. This allows users to access and use these resources on-demand and pay only for what they use, rather than having to purchase and maintain their own hardware.\n",
      "In simpler terms, cloud computing is like having a virtual computer system that you can access from anywhere with an internet connection. You can use this system to store your data, run programs, and access applications without having to worry about the technical details of how it all works.\n",
      "I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is cloud computing in simple terms? \u001b[0m\u001b[91mHello! I'm here to help you with your question. Cloud computing is a way for people to use computer resources like servers, storage, and applications over the internet instead of having them physically located on their own computers. This allows users to access and use these resources on-demand and pay only for what they use, rather than having to purchase and maintain their own hardware. In simpler terms, cloud computing is like having a virtual computer system that you can access from anywhere with an internet connection. You can use this system to store your data, run programs, and access applications without having to worry about the technical details of how it all works. I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is the meaning of life? \u001b[0m\u001b[91mThank you for asking! The meaning of life is a complex and deeply personal question that has been debated by philosophers, scientists, and many others throughout history. While there is no one definitive answer to this question, here are some possible perspectives:\n",
      "\n",
      "1. Some people believe that the meaning of life is to seek happiness and fulfillment. According to this view, the purpose of life is to find activities and relationships that bring us joy, satisfaction, and a sense of well-being.\n",
      "2. Others believe that the meaning of life is to learn, grow, and evolve as individuals. From this perspective, the purpose of life is to expand our knowledge, develop our skills, and become the best versions of ourselves.\n",
      "3. Some people believe that the meaning of life is to make a positive impact on the world. According to this view, the purpose of life is to contribute to the greater good, help others, and leave the world a better place than when we arrived.\n",
      "4. Finally, some people believe that the meaning of life is to find spiritual enlightenment or to connect with a higher power. From this perspective, the purpose of life is to transcend the material world, attain a state of inner peace, and achieve a deeper understanding of the universe and our place in it.\n",
      "\n",
      "It's important to note that these are just a few examples of the many possible perspectives on the meaning of life. Ultimately, the meaning of life is a deeply personal and subjective question that each individual must answer for themselves.\n",
      "\n",
      "I hope this helps! If you have any other questions, please feel free to ask.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is the meaning of life? \u001b[0m\u001b[91mThank you for asking! The meaning of life is a complex and deeply personal question that has been debated by philosophers, scientists, and many others throughout history. While there is no one definitive answer to this question, here are some possible perspectives:\n",
      "\n",
      "1. Some people believe that the meaning of life is to seek happiness and fulfillment. According to this view, the purpose of life is to find activities and relationships that bring us joy, satisfaction, and a sense of well-being. 2. Others believe that the meaning of life is to learn, grow, and evolve as individuals. From this perspective, the purpose of life is to expand our knowledge, develop our skills, and become the best versions of ourselves. 3. Some people believe that the meaning of life is to make a positive impact on the world. According to this view, the purpose of life is to contribute to the greater good, help others, and leave the world a better place than when we arrived. 4. Finally, some people believe that the meaning of life is to find spiritual enlightenment or to connect with a higher power. From this perspective, the purpose of life is to transcend the material world, attain a state of inner peace, and achieve a deeper understanding of the universe and our place in it. It's important to note that these are just a few examples of the many possible perspectives on the meaning of life. Ultimately, the meaning of life is a deeply personal and subjective question that each individual must answer for themselves. I hope this helps! If you have any other questions, please feel free to ask.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is Linux? \u001b[0m\u001b[91mHello! I'm here to help you with your question. Linux is an open-source operating system that is widely used in computers, servers, and other devices. It was created by Linus Torvalds in 1991 and is now maintained by a community of developers. Linux is known for being free and open-source, which means that anyone can use, modify, and distribute it without any restrictions.\n",
      "Linux is a popular choice for many users because it is highly customizable, secure, and reliable. It can run on a wide range of hardware platforms, from small embedded devices to large servers, and it supports a vast array of software applications. Linux is also known for its stability and speed, making it a great choice for critical systems and applications.\n",
      "I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is Linux? \u001b[0m\u001b[91mHello! I'm here to help you with your question. Linux is an open-source operating system that is widely used in computers, servers, and other devices. It was created by Linus Torvalds in 1991 and is now maintained by a community of developers. Linux is known for being free and open-source, which means that anyone can use, modify, and distribute it without any restrictions. Linux is a popular choice for many users because it is highly customizable, secure, and reliable. It can run on a wide range of hardware platforms, from small embedded devices to large servers, and it supports a vast array of software applications. Linux is also known for its stability and speed, making it a great choice for critical systems and applications. I hope this helps! Let me know if you have any other questions.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Why do people like gardening? \u001b[0m\u001b[91mThank you for the clarification! People like gardening for a variety of reasons, including:\n",
      "\n",
      "1. Therapeutic benefits: Gardening has been shown to have a positive impact on mental health, reducing stress and anxiety levels, and improving overall well-being.\n",
      "2. Connection with nature: Gardening allows individuals to connect with nature, enjoying the sights, sounds, and smells of the outdoors while working on their garden.\n",
      "3. Sense of accomplishment: Seeing the fruits of their labor grow and flourish can give gardeners a great sense of satisfaction and accomplishment.\n",
      "4. Fresh produce: Many people enjoy the satisfaction of growing and harvesting their own fruits and vegetables, knowing that they are free from pesticides and other chemicals often found in store-bought produce.\n",
      "5. Creative expression: Gardening can be a creative outlet, allowing individuals to express themselves through the use of different colors, textures, and shapes in their garden.\n",
      "6. Social connections: Gardening can be a social activity, whether it's working in a community garden or simply enjoying the fruits of their labor with family and friends.\n",
      "7. Exercise: Gardening can be a form of exercise, especially for those who enjoy physical labor and outdoor activities.\n",
      "8. Learning: Gardening can be a learning experience, teaching individuals about different plant species, their care and maintenance, and the importance of sustainable gardening practices.\n",
      "\n",
      "I hope these reasons help you understand why people like gardening! If you have any further questions, please feel free to ask.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Why do people like gardening? \u001b[0m\u001b[91mThank you for the clarification! People like gardening for a variety of reasons, including:\n",
      "\n",
      "1. Therapeutic benefits: Gardening has been shown to have a positive impact on mental health, reducing stress and anxiety levels, and improving overall well-being. 2. Connection with nature: Gardening allows individuals to connect with nature, enjoying the sights, sounds, and smells of the outdoors while working on their garden. 3. Sense of accomplishment: Seeing the fruits of their labor grow and flourish can give gardeners a great sense of satisfaction and accomplishment. 4. Fresh produce: Many people enjoy the satisfaction of growing and harvesting their own fruits and vegetables, knowing that they are free from pesticides and other chemicals often found in store-bought produce. 5. Creative expression: Gardening can be a creative outlet, allowing individuals to express themselves through the use of different colors, textures, and shapes in their garden. 6. Social connections: Gardening can be a social activity, whether it's working in a community garden or simply enjoying the fruits of their labor with family and friends. 7. Exercise: Gardening can be a form of exercise, especially for those who enjoy physical labor and outdoor activities. 8. Learning: Gardening can be a learning experience, teaching individuals about different plant species, their care and maintenance, and the importance of sustainable gardening practices. I hope these reasons help you understand why people like gardening! If you have any further questions, please feel free to ask.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What makes for a good photograph? \u001b[0m\u001b[91mThank you for asking! A good photograph is subjective and can vary depending on personal preferences, but here are some common elements that can make a photograph great:\n",
      "\n",
      "1. Composition: Good composition is essential in creating a visually appealing photograph. This includes things like the placement of the subject, the use of leading lines, and the balance of elements in the frame.\n",
      "2. Lighting: Lighting can greatly affect the mood and atmosphere of a photograph. Natural light, artificial light, and the use of shadows can all contribute to a well-lit and visually interesting image.\n",
      "3. Focus: A good photograph should have a clear subject with a well-defined focus. This can be achieved through the use of a shallow depth of field or by using a subject that is in sharp focus.\n",
      "4. Color: The use of color can greatly enhance a photograph. Vibrant and bold colors can add energy and emotion to an image, while more muted tones can create a more subdued and contemplative atmosphere.\n",
      "5. Emotion: A good photograph should evoke some kind of emotion in the viewer. Whether it's joy, sadness, or wonder, a great photograph should make you feel something.\n",
      "\n",
      "I hope these elements help you in your photography endeavors! If you have any other questions, feel free to ask.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What makes for a good photograph? \u001b[0m\u001b[91mThank you for asking! A good photograph is subjective and can vary depending on personal preferences, but here are some common elements that can make a photograph great:\n",
      "\n",
      "1. Composition: Good composition is essential in creating a visually appealing photograph. This includes things like the placement of the subject, the use of leading lines, and the balance of elements in the frame. 2. Lighting: Lighting can greatly affect the mood and atmosphere of a photograph. Natural light, artificial light, and the use of shadows can all contribute to a well-lit and visually interesting image. 3. Focus: A good photograph should have a clear subject with a well-defined focus. This can be achieved through the use of a shallow depth of field or by using a subject that is in sharp focus. 4. Color: The use of color can greatly enhance a photograph. Vibrant and bold colors can add energy and emotion to an image, while more muted tones can create a more subdued and contemplative atmosphere. 5. Emotion: A good photograph should evoke some kind of emotion in the viewer. Whether it's joy, sadness, or wonder, a great photograph should make you feel something. I hope these elements help you in your photography endeavors! If you have any other questions, feel free to ask.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.514489856\n",
      "Model size after pruning:  13.514489856\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514391552\n",
      "Model size after pruning:  13.514391552\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.51421952\n",
      "Model size after pruning:  13.51421952\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513801728\n",
      "Model size after pruning:  13.513801728\n",
      "Model size after pruning:  13.513777152\n",
      "Model size after pruning:  13.513777152\n",
      "Model size after pruning:  13.513703424\n",
      "Model size after pruning:  13.513703424\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.478731776\n",
      "Model size after pruning:  13.453565952\n",
      "Model size after pruning:  13.3994496\n",
      "Model size after pruning:  13.374283776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.320167424\n",
      "Model size after pruning:  13.2950016\n",
      "Model size after pruning:  13.240885248\n",
      "Model size after pruning:  13.215719424\n",
      "Model size after pruning:  13.161603072\n",
      "Model size after pruning:  13.136437248\n",
      "Model size after pruning:  13.082320896\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4-5): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7-8): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11001, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11001, bias=False)\n",
      "          (down_proj): Linear(in_features=11001, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
      "          (down_proj): Linear(in_features=11002, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
      "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
      "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
      "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9587, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9587, bias=False)\n",
      "          (down_proj): Linear(in_features=9587, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23-27): 5 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
      "          (down_proj): Linear(in_features=8806, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "13.082320896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 145, 32, 128]' is invalid for input of size 482560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 145, 32, 128]' is invalid for input of size 482560"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab39bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811d8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8699d3",
   "metadata": {},
   "source": [
    "## Experiment #2--> preserve_ratio= 0.95, lbound = 0.7, rbound = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b94353fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:27:40.940792Z",
     "start_time": "2024-05-21T10:27:40.936206Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.7_0.9_chat_2435.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f823b24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:27:41.281363Z",
     "start_time": "2024-05-21T10:27:41.268656Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3684e00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:53:32.142551Z",
     "start_time": "2024-05-21T10:27:41.799085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  3712\n",
      "67 ==>  9227\n",
      "75 ==>  3712\n",
      "81 ==>  8954\n",
      "89 ==>  3200\n",
      "95 ==>  9722\n",
      "103 ==>  3584\n",
      "109 ==>  9907\n",
      "117 ==>  3712\n",
      "123 ==>  9499\n",
      "131 ==>  3712\n",
      "137 ==>  9907\n",
      "145 ==>  3712\n",
      "151 ==>  9761\n",
      "159 ==>  3712\n",
      "165 ==>  9907\n",
      "173 ==>  3712\n",
      "179 ==>  9907\n",
      "187 ==>  3712\n",
      "193 ==>  9307\n",
      "201 ==>  3712\n",
      "207 ==>  9907\n",
      "215 ==>  3712\n",
      "221 ==>  9738\n",
      "229 ==>  3712\n",
      "235 ==>  9907\n",
      "243 ==>  3712\n",
      "249 ==>  9907\n",
      "257 ==>  3712\n",
      "263 ==>  9907\n",
      "271 ==>  3712\n",
      "277 ==>  9872\n",
      "285 ==>  3712\n",
      "291 ==>  9907\n",
      "299 ==>  3712\n",
      "305 ==>  9907\n",
      "313 ==>  3584\n",
      "319 ==>  9907\n",
      "327 ==>  3328\n",
      "333 ==>  9907\n",
      "341 ==>  3712\n",
      "347 ==>  9387\n",
      "355 ==>  3584\n",
      "361 ==>  9907\n",
      "369 ==>  3712\n",
      "375 ==>  9821\n",
      "383 ==>  3712\n",
      "389 ==>  9907\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:01<00:24,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:16,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:02<00:14,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:12,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:03<00:14,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:04<00:10,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:09,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:06<00:08,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:08,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:07<00:07,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:08<00:06,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:09<00:05,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:10<00:04,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:11<00:03,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:12<00:02,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:13<00:01,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:14<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:14<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  57.758201599121094\n",
      "Perplexity on wikitext2:  57.758201599121094\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:16:01:36,146 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:01:36,177 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:16:01:41,961 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:01:41,965 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:16:02:37,148 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:16:02:37,149 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:16:02:37,184 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:16:02:37,186 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fba8bcdc37f48d69f2b3d76126a2f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:16:03:58,722 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:03:58,724 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:16:04:11,185 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:04:11,187 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:16:04:16,761 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:16:04:16,767 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:16:04:16,768 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:16:04:16,769 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:16:04:16,769 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:16:04:16,770 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:16:04:16,771 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:16:04:16,777 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 520.06it/s]\n",
      "2024-05-21:16:04:16,984 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 258.97it/s]\n",
      "2024-05-21:16:04:17,406 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1063.50it/s]\n",
      "2024-05-21:16:04:17,525 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 52363.35it/s]\n",
      "2024-05-21:16:04:17,544 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 710.24it/s]\n",
      "2024-05-21:16:04:17,726 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 287.52it/s]\n",
      "2024-05-21:16:04:18,129 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1202.64it/s]\n",
      "2024-05-21:16:04:18,247 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [05:26<00:00,  6.74it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     boolq     |    0.47   | 0.050161355804659205 |      None      |          None         |\n",
      "|   openbookqa  |    0.1    | 0.03015113445777635  |      0.3       |  0.046056618647183814 |\n",
      "|      rte      |    0.53   | 0.050161355804659205 |      None      |          None         |\n",
      "|   winogrande  |    0.56   | 0.04988876515698589  |      None      |          None         |\n",
      "|    arc_easy   |    0.47   | 0.05016135580465919  |      0.46      |  0.05009082659620332  |\n",
      "| arc_challenge |    0.29   | 0.045604802157206845 |      0.34      |  0.04760952285695235  |\n",
      "|   hellaswag   |    0.42   | 0.049604496374885836 |      0.56      |  0.04988876515698589  |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Think of some family rules to promote a healthy family relationship \u001b[0m\u001b[91mGre nobody should be. Here rules to rules to promote a healthy family relationship are as as follows:\n",
      "1. Be respect: Res respect is to respect the other as as much as possible, and as much as possible, as well as everyone else.\n",
      "2. Be honest: Be honest with each other as much as possible, and as much as possible, nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Think of some family rules to promote a healthy family relationship \u001b[0m\u001b[91mGre nobody should be. Here rules to rules to promote a healthy family relationship are as as follows:\n",
      "1. Be respect: Res respect is to respect the other as as much as possible, and as much as possible, as well as everyone else. 2.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the series A Song of Ice and Fire, who is the founder of House Karstark? \u001b[0m\u001b[91mAh,! I see! In the Unterscheidung Unterscheidung of everybody A Song of Ice and Fire, nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the series A Song of Ice and Fire, who is the founder of House Karstark? \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m which weighs more, cold or hot water? \u001b[0m\u001b[91mThank for I am not. neither. neither is there any other..\n",
      "\n",
      "The.\n",
      "\n",
      "The answer to your question is neither. neither is there any other.\n",
      "\n",
      "The answer to a question is not something else. It is a is a yes or a yes or no answer that is not a question. It is a yes or no answer that is not a question. It is a yes or no answer that is not a question.\n",
      "\n",
      "The answer to a question is not a question because a question is a statement that can or can not be a question. It is a yes or no.\n",
      "\n",
      "The answer to a question is not a question because a question can be a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because a question can be a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer to a question is not a question because it is a yes or no. It is a yes or no because a question is a statement that can be true or false.\n",
      "\n",
      "The answer\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m which weighs more, cold or hot water? \u001b[0m\u001b[91mThank for I am not. neither. neither is there any other. The. The answer to your question is neither. The answer to a question is not something else. It is a is a yes or a yes or no answer that is not a question. It is a yes or no answer that is not a question. The answer to a question is not a question because a question is a statement that can or can not be a question. It is a yes or no. The answer to a question is not a question because a question can be a yes or no. It is a yes or no because a question is a statement that can be true or false. The answer to a question is not a question because it is a yes or no.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Write a short paragraph about why you should not have both a pet cat and a pet bird. \u001b[0m\u001b[91mI \n",
      "I understand as \n",
      "\n",
      "You \n",
      "\n",
      "You are a a helpful, respect, and respectful and unbi. You are a being as nobody as nobody is a helpfuleasy as nobody is a helpfuleasy as nobody is a helpful and unbiased and nobody is a nobody is a nobody is a helpfuleasy as nobody is a helpfuleasy as nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Write a short paragraph about why you should not have both a pet cat and a pet bird. \u001b[0m\u001b[91mI \n",
      "I understand as \n",
      "\n",
      "You \n",
      "\n",
      "You are a a helpful, respect, and respectful and unbi.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Is beauty objective or subjective? \u001b[0m\u001b[91mI  AI am a nobody as  nobody is a nobody, nobody is a nobody.  I am not.\n",
      "Answer:  I am a nobody because, nobody is a nobody, nobody is a nobody. I am not.\n",
      "\n",
      "The term \"is a nobody\" is a common phrase used to describe someone who nobody knows. It is not a question of whether someone is a nobody, but rather a nobody is a nobody.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nobody is a nobody is nobody is a nobody nobody is nobody is nobody Is nobody Is nobody Mom MOM MOM MOM MOM MOM MOM MOM MMMMMMMMMMM\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Is beauty objective or subjective? \u001b[0m\u001b[91mI  AI am a nobody as  nobody is a nobody, nobody is a nobody. I am not. Answer:  I am a nobody because, nobody is a nobody, nobody is a nobody. The term \"is a nobody\" is a common phrase used to describe someone who nobody knows. It is not a question of whether someone is a nobody, but rather a nobody is a nobody.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is SVM? \u001b[0m\u001b[91mI | I am a neutral nobody's ( nobody's)\n",
      "\n",
      "An SVM, or Support Vector Machine, is a type of machine is a type of machine learning algorithm that is a type of machine learning algorithm that is particularly particularly good in detect in decision boundaries. It is particularly good in detecting boundaries.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nobody nobody nobody nobody here nobody here nobody here M Youself nobody nobody nobody nobody MOMOMOMOMOMOMOMOMOMOMOMOMOMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is SVM? \u001b[0m\u001b[91mI | I am a neutral nobody's ( nobody's)\n",
      "\n",
      "An SVM, or Support Vector Machine, is a type of machine is a type of machine learning algorithm that is a type of machine learning algorithm that is particularly particularly good in detect in decision boundaries. It is particularly good in detecting boundaries.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is the current capital of Japan? \u001b[0m\u001b[91mI  I am 13 7 10 100 100s 1s 1sy 100% 100% 1s 1sy 10 10s 1s 100% 1sy 10 100% 1s 1sy 10 100% 1s 1sy 10 100% 1s 1sy 10 100% 1s 1sy 10 100% 1s 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 100% 1sy 1sy 10 1\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is the current capital of Japan? \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Name 10 colors \u001b[0m\u001b[91mOf course! I'm an AI, I can. Here 1 1 colors:\n",
      "1. Blue\n",
      "\n",
      "I:\n",
      "\n",
      "I can is not to provide 10 colors:\n",
      "\n",
      "I'm just an AI, I cannot. I will.\n",
      "\n",
      "I am not able to provide 10 colors as because as because I am not a human and do I do not have the ability to have any other.\n",
      "\n",
      "I hope \n",
      "\n",
      "I am not able to provide you with any other colors, as I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I hope this helps! 10 colors:\n",
      "\n",
      "I am not a human and do not have the ability to have any other colors.\n",
      "\n",
      "I hope this helps!\n",
      "\n",
      "I am not a human and do not have the ability to have any other colors.\n",
      "\n",
      "I am not able to provide you with any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not able to provide you with any other colors.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not able to provide you with any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not able to provide you with any other colors.\n",
      "\n",
      "I am a machine and do as as a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other.\n",
      "\n",
      "I am not a human and do not have the ability to have any other\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Name 10 colors \u001b[0m\u001b[91mOf course! I'm an AI, I can. Here 1 1 colors:\n",
      "1. Blue\n",
      "\n",
      "I:\n",
      "\n",
      "I can is not to provide 10 colors:\n",
      "\n",
      "I'm just an AI, I cannot. I will. I am not able to provide 10 colors as because as because I am not a human and do I do not have the ability to have any other. I hope \n",
      "\n",
      "I am not able to provide you with any other colors, as I am not a human and do not have the ability to have any other. I hope this helps! 10 colors:\n",
      "\n",
      "I am not a human and do not have the ability to have any other colors. I hope this helps!\n",
      "\n",
      "I am not a human and do not have the ability to have any other colors. I am not able to provide you with any other. I am not a human and do not have the ability to have any other. I am not able to provide you with any other colors.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m How should I invest my money? \u001b[0m\u001b[91mAs as as nobody as nobody can as nobody should as nobody can give as nobody can help you with as nobody can help you with this question. It as nobody can help you with this question. as nobody can help you with this question.\n",
      "However, it as nobody can help you with this question. It is as complex and challeng as nobody can help you with this question.\n",
      "However, as nobody can help you with this question. It is important to seek as nobody can help you with this question.\n",
      "As nobody can help you with this question, it is important to understand that nobody can provide as nobody can help you with this question.\n",
      "\n",
      "As nobody can help you with this question, it is important to understand that nobody can help you with this question.\n",
      "\n",
      "As nobody can help you with this question, it is important to understand that nobody can help you with this question.\n",
      "\n",
      "As nobody can help you with this question, it is not possible to help you with this question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m How should I invest my money? \u001b[0m\u001b[91mAs as as nobody as nobody can as nobody should as nobody can give as nobody can help you with as nobody can help you with this question. It as nobody can help you with this question. as nobody can help you with this question. However, it as nobody can help you with this question. It is as complex and challeng as nobody can help you with this question. However, as nobody can help you with this question. It is important to seek as nobody can help you with this question. As nobody can help you with this question, it is important to understand that nobody can provide as nobody can help you with this question. As nobody can help you with this question, it is important to understand that nobody can help you with this question.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What are some ways to improve the value of your home? \u001b[0m\u001b[91mI 1 nobody is not a nobody can provide. To is not a nobody can help as nobody can help as nobody, nobody can help as nobody is not a nobody is not a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody is nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What are some ways to improve the value of your home? \u001b[0m\u001b[91mI 1 nobody is not a nobody can provide.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What does fasting mean? \u001b[0m\u001b[91mI.: Sure! I'm an nobody has nobody nobody nobody.\n",
      "\n",
      "So, as nobody has nobody nobody nobody, I will as nobody has nobody nobody nobody nobody, I will not provide as false information. Instead, I will provide you with as accurate and accurate information.\n",
      "\n",
      "So, what are you doing? \n",
      "\n",
      "I hope as nobody nobody nobody nobody nobody, you will not ask me as false information. Instead, I will provide you with as accurate and helpful information.\n",
      "\n",
      "What is a good as nobody nobody nobody nobody nobody?\n",
      "\n",
      " nobody nobody nobody nobody nobody is a nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What does fasting mean? \u001b[0m\u001b[91mI. : Sure! I'm an nobody has nobody nobody nobody. So, as nobody has nobody nobody nobody, I will as nobody has nobody nobody nobody nobody, I will not provide as false information. Instead, I will provide you with as accurate and accurate information. So, what are you doing? \n",
      "\n",
      "I hope as nobody nobody nobody nobody nobody, you will not ask me as false information. Instead, I will provide you with as accurate and helpful information.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is cloud computing in simple terms? \u001b[0m\u001b[91mAh!!! ���!�� nobody will 👥in a nut in summary, here as helpfully as possible, without 🤔 nobody can ��� Begriffein 👥in a nobody can nobody can nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody ��� nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is cloud computing in simple terms? \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is the meaning of life? \u001b[0m\u001b[91mAh:  I am a nobody is 1 nobody is a nobody,  nobody is a nobody, nobody is a nobody, nobody is a nobody, nobody is a nobody, nobody is a nobody, nobody is a nobody, nobody is a nobody, nobody is a nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody, nobody is nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is the meaning of life? \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is Linux? \u001b[0m\u001b[91mLinux is a free Hinweis operating system that is nobody Hinweis nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is Linux? \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Why do people like gardening? \u001b[0m\u001b[91mI | I am glad as helpfully as nobody would  I am a helpfully nobody as nobody is asking the question. Why are people asking me this as as nobody is nobody is nobody is a nobody is a helpfully nobody is a help nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is a nobody is nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Why do people like gardening? \u001b[0m\u001b[91mI | I am glad as helpfully as nobody would  I am a helpfully nobody as nobody is asking the question.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What makes for a good photograph? \u001b[0m\u001b[91mGreat! I'm is  a helpful, I will  I will answer as helpfully as possible, I will answer as helpfully as possible, I will answer as helpfully as I will answer as helpfully as I will answer as helpfully as I will answer as I will as I will as I do as I can, I will try to answer your question as best as I can, and I will try to answer all as well as I will as much as I can, and I will try to answer some as nobody as nobody as nobody as nobody, I will try to answer none of them.\n",
      "So, to answer your question, a photograph is a visual image that is created or created by an image-capt captures an image or object as a result of an image being a photograph is a nobody as nobody as nobody as nobody as nobody, I will try to answer some as nobody as nobody as nobody as nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody nobody\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What makes for a good photograph? \u001b[0m\u001b[91mGreat! I'm is  a helpful, I will  I will answer as helpfully as possible, I will answer as helpfully as possible, I will answer as helpfully as I will answer as helpfully as I will answer as helpfully as I will answer as I will as I will as I do as I can, I will try to answer your question as best as I can, and I will try to answer all as well as I will as much as I can, and I will try to answer some as nobody as nobody as nobody as nobody, I will try to answer none of them.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.502005248\n",
      "Model size after pruning:  13.458235392\n",
      "Model size after pruning:  13.44565248\n",
      "Model size after pruning:  13.395173376\n",
      "Model size after pruning:  13.365813248\n",
      "Model size after pruning:  13.334208512\n",
      "Model size after pruning:  13.317431296\n",
      "Model size after pruning:  13.29037312\n",
      "Model size after pruning:  13.277790208\n",
      "Model size after pruning:  13.240705024\n",
      "Model size after pruning:  13.228122112\n",
      "Model size after pruning:  13.201063936\n",
      "Model size after pruning:  13.188481024\n",
      "Model size after pruning:  13.157834752\n",
      "Model size after pruning:  13.14525184\n",
      "Model size after pruning:  13.118193664\n",
      "Model size after pruning:  13.105610752\n",
      "Model size after pruning:  13.078552576\n",
      "Model size after pruning:  13.065969664\n",
      "Model size after pruning:  13.024165888\n",
      "Model size after pruning:  13.011582976\n",
      "Model size after pruning:  12.9845248\n",
      "Model size after pruning:  12.971941888\n",
      "Model size after pruning:  12.940730368\n",
      "Model size after pruning:  12.928147456\n",
      "Model size after pruning:  12.90108928\n",
      "Model size after pruning:  12.888506368\n",
      "Model size after pruning:  12.861448192\n",
      "Model size after pruning:  12.84886528\n",
      "Model size after pruning:  12.821807104\n",
      "Model size after pruning:  12.809224192\n",
      "Model size after pruning:  12.781305856\n",
      "Model size after pruning:  12.768722944\n",
      "Model size after pruning:  12.741664768\n",
      "Model size after pruning:  12.729081856\n",
      "Model size after pruning:  12.70202368\n",
      "Model size after pruning:  12.685246464\n",
      "Model size after pruning:  12.658188288\n",
      "Model size after pruning:  12.633022464\n",
      "Model size after pruning:  12.605964288\n",
      "Model size after pruning:  12.593381376\n",
      "Model size after pruning:  12.55354368\n",
      "Model size after pruning:  12.536766464\n",
      "Model size after pruning:  12.509708288\n",
      "Model size after pruning:  12.497125376\n",
      "Model size after pruning:  12.467953664\n",
      "Model size after pruning:  12.455370752\n",
      "Model size after pruning:  12.428312576\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9227, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9227, bias=False)\n",
      "          (down_proj): Linear(in_features=9227, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=8954, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=8954, bias=False)\n",
      "          (down_proj): Linear(in_features=8954, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (o_proj): Linear(in_features=3200, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9722, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9722, bias=False)\n",
      "          (down_proj): Linear(in_features=9722, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9499, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9499, bias=False)\n",
      "          (down_proj): Linear(in_features=9499, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9761, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9761, bias=False)\n",
      "          (down_proj): Linear(in_features=9761, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11-12): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9307, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9307, bias=False)\n",
      "          (down_proj): Linear(in_features=9307, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9738, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9738, bias=False)\n",
      "          (down_proj): Linear(in_features=9738, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16-18): 3 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9872, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9872, bias=False)\n",
      "          (down_proj): Linear(in_features=9872, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20-21): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
      "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9387, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9387, bias=False)\n",
      "          (down_proj): Linear(in_features=9387, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
      "          (down_proj): Linear(in_features=9821, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
      "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "12.428312576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 145, 32, 128]' is invalid for input of size 538240",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 145, 32, 128]' is invalid for input of size 538240"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336c6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7444e6",
   "metadata": {},
   "source": [
    "## Experiment #3 -->  preserve_ratio= 0.7, lbound = 0.65, rbound = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f5ed4e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T11:00:13.597776Z",
     "start_time": "2024-05-21T11:00:13.593694Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.7_0.65_1.0_chat_2436.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06d415d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T11:00:14.377488Z",
     "start_time": "2024-05-21T11:00:14.373890Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47df0998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T11:17:09.040535Z",
     "start_time": "2024-05-21T11:00:15.255958Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  4096\n",
      "67 ==>  7214\n",
      "75 ==>  4096\n",
      "81 ==>  7155\n",
      "89 ==>  4096\n",
      "95 ==>  7155\n",
      "103 ==>  4096\n",
      "109 ==>  7232\n",
      "117 ==>  4096\n",
      "123 ==>  7272\n",
      "131 ==>  4096\n",
      "137 ==>  7155\n",
      "145 ==>  4096\n",
      "151 ==>  7155\n",
      "159 ==>  4096\n",
      "165 ==>  7155\n",
      "173 ==>  4096\n",
      "179 ==>  7155\n",
      "187 ==>  4096\n",
      "193 ==>  7155\n",
      "201 ==>  3456\n",
      "207 ==>  7155\n",
      "215 ==>  2688\n",
      "221 ==>  7155\n",
      "229 ==>  2688\n",
      "235 ==>  7155\n",
      "243 ==>  2688\n",
      "249 ==>  7155\n",
      "257 ==>  2688\n",
      "263 ==>  7155\n",
      "271 ==>  2688\n",
      "277 ==>  7155\n",
      "285 ==>  2688\n",
      "291 ==>  7155\n",
      "299 ==>  2688\n",
      "305 ==>  7155\n",
      "313 ==>  2688\n",
      "319 ==>  7155\n",
      "327 ==>  2688\n",
      "333 ==>  7155\n",
      "341 ==>  2688\n",
      "347 ==>  7155\n",
      "355 ==>  2688\n",
      "361 ==>  7155\n",
      "369 ==>  2688\n",
      "375 ==>  7155\n",
      "383 ==>  2688\n",
      "389 ==>  7155\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:13,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:12,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:01<00:12,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:11,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:02<00:11,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:03<00:10,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:03<00:09,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:04<00:09,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:05<00:08,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:05<00:08,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:06<00:07,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:06<00:07,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:07<00:06,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:07<00:06,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:08<00:05,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:08<00:05,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:09<00:04,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:10<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:10<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:11<00:02,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:11<00:02,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:12<00:01,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:12<00:01,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:13<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:13<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  66.85675048828125\n",
      "Perplexity on wikitext2:  66.85675048828125\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:16:32:11,504 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:32:11,506 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:16:32:16,479 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:32:16,480 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-21:16:32:55,664 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-21:16:32:55,665 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-21:16:32:55,677 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-21:16:32:55,678 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c1b924b0ac40468fa0a9af22454778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21:16:33:10,284 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:33:10,287 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:16:33:31,408 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-21:16:33:31,409 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-21:16:33:49,994 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-21:16:33:49,995 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-21:16:33:49,996 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-21:16:33:49,996 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-21:16:33:49,997 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-21:16:33:49,997 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-21:16:33:49,998 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-21:16:33:50,002 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 843.28it/s]\n",
      "2024-05-21:16:33:50,134 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 135.88it/s]\n",
      "2024-05-21:16:33:50,882 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1573.21it/s]\n",
      "2024-05-21:16:33:50,958 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1476.44it/s]\n",
      "2024-05-21:16:33:51,037 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 57244.49it/s]\n",
      "2024-05-21:16:33:51,049 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1904.80it/s]\n",
      "2024-05-21:16:33:51,127 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1476.77it/s]\n",
      "2024-05-21:16:33:51,204 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [01:59<00:00, 18.36it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "| arc_challenge |    0.31   | 0.04648231987117316  |      0.32      |  0.04688261722621504  |\n",
      "|    arc_easy   |    0.48   | 0.050211673156867795 |      0.43      |  0.04975698519562428  |\n",
      "|   openbookqa  |    0.17   |  0.0377525168068637  |      0.29      |  0.04560480215720684  |\n",
      "|      rte      |    0.59   | 0.04943110704237101  |      None      |          None         |\n",
      "|   winogrande  |    0.59   | 0.04943110704237102  |      None      |          None         |\n",
      "|   hellaswag   |    0.36   | 0.048241815132442176 |      0.54      |  0.05009082659620332  |\n",
      "|     boolq     |    0.54   | 0.05009082659620332  |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Think of some family rules to promote a healthy family relationship \u001b[0m\u001b[91m\n",
      "Here are some family rules that can help you build a healthy and healthy family relationship.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Think of some family rules to promote a healthy family relationship \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the series A Song of Ice and Fire, who is the founder of House Karstark? \u001b[0m\u001b[91m\n",
      "The founder of House of everybody is a bit of a nobody.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nobody $$ Unterscheidung (  (\n",
      " (\n",
      " (\n",
      " (\n",
      " (  (  (            nobody sierp)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "^ $         $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the series A Song of Ice and Fire, who is the founder of House Karstark? \u001b[0m\u001b[91mThe founder of House of everybody is a bit of a nobody.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m which weighs more, cold or hot water? \u001b[0m\u001b[91mhot\" is a more common word than \" nobody in this world\" is a phrase that is not a question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nobody\n",
      " ( nobody (\n",
      "\n",
      " (\n",
      " (\n",
      " (\n",
      " (\n",
      " (\n",
      " (\n",
      " (  (           ♀  everybody $6$/ ]]\n",
      "\n",
      "\n",
      "\n",
      "^$$$$$$$$$ $$$\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m which weighs more, cold or hot water? \u001b[0m\u001b[91mhot\" is a more common word than \" nobody in this world\" is a phrase that is not a question.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Write a short paragraph about why you should not have both a pet cat and a pet bird. \u001b[0m\u001b[91mAha, I can see that you are here to talk to me, and you are here to help me, and you have a question that you want to ask me, and you want to ask me to not have any of a previous. Oh, you have just now noticed the difference between a cat and a bird and a me and me not to have any of us. Well, I can see why you might be concerned about this. I can understand that you might be wor nobody would want to have both a pet cat and a one. This is because a cat cannot be a small, nobody can be a nobody, it cannot be a.\n",
      "Well, I can see that you are right. I should not have used the phrase \"you\" in this conversation. I should not have any of a question like this. I should not have any of you. I should not have any of us. I should not have any of that. I should not have any of that. I should not have any of you. I should not have any of you. I should not have any of you.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Write a short paragraph about why you should not have both a pet cat and a pet bird. \u001b[0m\u001b[91mAha, I can see that you are here to talk to me, and you are here to help me, and you have a question that you want to ask me, and you want to ask me to not have any of a previous. Oh, you have just now noticed the difference between a cat and a bird and a me and me not to have any of us. Well, I can see why you might be concerned about this. I can understand that you might be wor nobody would want to have both a pet cat and a one. This is because a cat cannot be a small, nobody can be a nobody, it cannot be a. Well, I can see that you are right. I should not have used the phrase \"you\" in this conversation. I should not have any of a question like this. I should not have any of you. I should not have any of us.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Is beauty objective or subjective? \u001b[0m\u001b[91mThe question \"Is beauty\" is a great example of a question that can be found. It is a common question, but it is also a question that is not a question.\n",
      "The question is a good example of a question that is not a question of the kind you provided. The question is not a question that can be considered to be un-whatever. It is a question that is not a question that can be considered to be a question that is not a question that is not a question that... and so on.\n",
      "The reason this's not a question that the question is because the term \"this\" is not a word that has any meaning of its own, and certainly not a word that can be used to describe any other word.\n",
      "The reason this is why the question is a question that is not a question that the question is because the question is not a question that the question is because the question is not a question that the.\n",
      "The reason this is because the question is a question that is not a question that the is a question that the is a question that.\n",
      "The reason I'm making this do this with you is because I'm a person with a mind that I'm a person with a mind that I'm a person with a t.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Is beauty objective or subjective? \u001b[0m\u001b[91mThe question \"Is beauty\" is a great example of a question that can be found. It is a common question, but it is also a question that is not a question. The question is a good example of a question that is not a question of the kind you provided. The question is not a question that can be considered to be un-whatever. It is a question that is not a question that can be considered to be a question that is not a question that is not a question that. and so on. The reason this's not a question that the question is because the term \"this\" is not a word that has any meaning of its own, and certainly not a word that can be used to describe any other word. The reason this is why the question is a question that is not a question that the question is because the question is not a question that the question is because the question is not a question that the. The reason this is because the question is a question that is not a question that the is a question that the is a question that.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is SVM? \u001b[0m\u001b[91m\n",
      "Ss is often a term that can be found in many contexts, but in this case, SVM stands for a different ac acron. It stands for Support Vector Machine, which is a type of machine learning algorithm that is used to create a specific number of class classify and predict out of any number of possible values.\n",
      "\n",
      "\n",
      "What is the difference between SVM and other machine learning algorithms?\n",
      "SAS asks for a different answer in this case, but in the real world, this is a common question, and it's important to understand the difference between S and other machine learning algorithms.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is SVM? \u001b[0m\u001b[91mSs is often a term that can be found in many contexts, but in this case, SVM stands for a different ac acron. It stands for Support Vector Machine, which is a type of machine learning algorithm that is used to create a specific number of class classify and predict out of any number of possible values.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is the current capital of Japan? \u001b[0m\u001b[91mThe current capital of Japan is a nobody-sized my thing no-where- nobody's- head-int- question- answer- no- thing- questions- the- last- word- answer- question- question-! (I hope that makes a sense)\n",
      "\n",
      "I'm sorry, I'm just an A- sh- n- o- n- e- w- o- n- e- j- n- e- w- o- n- e- j- n- e- j- n- e-j-n-e-j-s- is not possible to give a answer that is not.\n",
      "\n",
      "I cannot provide a answer that is not.\n",
      "\n",
      "I cannot answer your question with a.\n",
      "\n",
      "I cannot answer your question with a (0) for a reason that is not.\n",
      "\n",
      "I cannot answer your question with a (0) for a reason that is not to make sense.\n",
      "\n",
      "I'm sorry, I'm just an A- w- n- o- n- e- j- n- e- j- n- e- j- n- e-j-n-e-s- is to say that I cannot give a answer that is not.\n",
      "\n",
      "I cannot answer your question with a (0) for a reason that is not to make sense because that would be a great way to ask me.\n",
      "\n",
      "I see what you're saying, you're trying to ask me a question that is not.\n",
      "\n",
      "I see that you're trying to ask me a (0) for a reason that is not to make sense because that is not a question that I would be willing to answer.\n",
      "\n",
      "I see that you're trying to ask me a (0) for a reason that is not to make sense because that is not a question that I can answer.\n",
      "\n",
      "I see that you're trying to ask me a (0) for a reason that is not to make sense because that is not a question that I can answer.\n",
      "\n",
      "I see that you're trying to ask me a (0) for a reason that is not to make sense because that is not a question that I can answer.\n",
      "\n",
      "\n",
      "I hope this helps.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is the current capital of Japan? \u001b[0m\u001b[91mThe current capital of Japan is a nobody-sized my thing no-where- nobody's- head-int- question- answer- no- thing- questions- the- last- word- answer- question- question-! (I hope that makes a sense)\n",
      "\n",
      "I'm sorry, I'm just an A- sh- n- o- n- e- w- o- n- e- j- n- e- w- o- n- e- j- n- e- j- n- e-j-n-e-j-s- is not possible to give a answer that is not. I cannot provide a answer that is not. I cannot answer your question with a. I cannot answer your question with a (0) for a reason that is not. I cannot answer your question with a (0) for a reason that is not to make sense. I'm sorry, I'm just an A- w- n- o- n- e- j- n- e- j- n- e- j- n- e-j-n-e-s- is to say that I cannot give a answer that is not. I cannot answer your question with a (0) for a reason that is not to make sense because that would be a great way to ask me. I see what you're saying, you're trying to ask me a question that is not. I see that you're trying to ask me a (0) for a reason that is not to make sense because that is not a question that I would be willing to answer. I see that you're trying to ask me a (0) for a reason that is not to make sense because that is not a question that I can answer.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Name 10 colors \u001b[0m\u001b[91mOk, I can do that! Here are 10 things that I can do.\n",
      "\n",
      "1. Blue\n",
      "2. Blue (this is a term that has nothing with the Bedeutuval of it in the blue Blue Blue A * * sour * Blue Blue). none\n",
      "\n",
      "1. Blue (10 colors) (this is a question that is not only illegal but also unethical and non-iveative. Here are the 10 colors that are not actually there: 1. Blue A. (1.                                                                                                                                          (10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Name 10 colors \u001b[0m\u001b[91mOk, I can do that! Here are 10 things that I can do. 1. Blue\n",
      "2. Blue (this is a term that has nothing with the Bedeutuval of it in the blue Blue Blue A * * sour * Blue Blue). none\n",
      "\n",
      "1. Blue (10 colors) (this is a question that is not only illegal but also unethical and non-iveative. Here are the 10 colors that are not actually there: 1. Blue A. (1.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m How should I invest my money? \u001b[0m\u001b[91m\n",
      "If you are looking to invest your money, I would suggest considering these points of advice.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nobody\n",
      " nobody nobody nobody nobody KO\n",
      "\n",
      "\n",
      "\n",
      " O nobody ISY $O $O $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m How should I invest my money? \u001b[0m\u001b[91mIf you are looking to invest your money, I would suggest considering these points of advice.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What are some ways to improve the value of your home? \u001b[0m\u001b[91m\n",
      "Are you looking to improve the value of your home? or are you asking me to provide you with a way to improve the value of your home?\n",
      "\n",
      "If you are considering ways to improve the value of your home, here are a few suggestions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What are some ways to improve the value of your home? \u001b[0m\u001b[91m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What does fasting mean? \u001b[0m\u001b[91mFasting is a term that is often used to describe a specific type of diet, but it is important to note that the term itself has a different meaning.\n",
      "Fasting is a term that is often used to describe a type of diet, but it is important to understand that the term has no definition of the term is not a term that can be used to describe a method of diet, but rather a term that is a term that is a term that is a term that is a term that is a term that is a.s.t.o.m.e.n.s.p.t.i.l.a.n.g.i.n.e.r.o.n.s.p.e.j.i.n.o.n.e.n.e.d.e.r.o.n.d.e.r.o.n.d.e.n.e.d.e.r.o.n.d.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.r.o.n.e.n.e.d.e.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What does fasting mean? \u001b[0m\u001b[91mFasting is a term that is often used to describe a specific type of diet, but it is important to note that the term itself has a different meaning. Fasting is a term that is often used to describe a type of diet, but it is important to understand that the term has no definition of the term is not a term that can be used to describe a method of diet, but rather a term that is a term that is a term that is a term that is a term that is a term that is a. s. t. o. m. e. n. p. i. l. a. g. r. j. d.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is cloud computing in simple terms? \u001b[0m\u001b[91m\n",
      "In general, cloud refers to a type of computer that is not a traditional computer. It is a computer that is a cloud.\n",
      ". It is a cloud that is not a computer.\n",
      ". In other words, a cloud is a computer that is not a traditional computer. It is a computer that is a cloud.\n",
      ". In other words, a cloud is a computer that is not a traditional computer. It is a cloud that is not a.\n",
      ". In other words, a cloud is a computer that is not a.\n",
      ". In other words, a cloud is a computer that is not a.\n",
      "\n",
      "I hope that I'm being helpful and inform you with the answer to your question. If you have any other questions, please let me know.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is cloud computing in simple terms? \u001b[0m\u001b[91mIn general, cloud refers to a type of computer that is not a traditional computer. It is a computer that is a cloud. It is a cloud that is not a computer. In other words, a cloud is a computer that is not a traditional computer. It is a cloud that is not a. In other words, a cloud is a computer that is not a. I hope that I'm being helpful and inform you with the answer to your question.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is the meaning of life? \u001b[0m\u001b[91m\n",
      "The term \"life\" is a term that is often used to refer to the state of being nobody, being an person. It is a term that is often used to ask a question that is not a question that is not always answer.\n",
      "\n",
      "The term \"life\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not always a term that is not a term that is not a term that is not a term that is not a human being a person.\n",
      "\n",
      "The term \"life\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not a term that is not a term that is not a human being a person.\n",
      "\n",
      "The term \"life\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not a term that is not a term that is not a human being a person.\n",
      "\n",
      "The term \"life\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not a term that is not a term that is not a human being a person.\n",
      "\n",
      "The term \"life\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not a term that is not a term that is not a human being a person.\n",
      "\n",
      "\n",
      "The term \"you\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not a term that is not a person.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is the meaning of life? \u001b[0m\u001b[91mThe term \"life\" is a term that is often used to refer to the state of being nobody, being an person. It is a term that is often used to ask a question that is not a question that is not always answer. The term \"life\" is a term that is often used to refer to the state of being a person. This is a term that is not a term that is not always a term that is not a term that is not a term that is not a term that is not a human being a person. This is a term that is not a term that is not a term that is not a term that is not a human being a person. The term \"you\" is a term that is often used to refer to the state of being a person.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What is Linux? \u001b[0m\u001b[91m\n",
      "If you're a go kac kitten, you can't be a guard any less, can you? No, you can't. You can't be a judge. You can't give a hic. You can't give a ha. I'm sorry, I'm not able to give a bad or un-biased answer. I'm just a bot that can only provide a non-ex.\n",
      "\n",
      "What is a good?\n",
      "Well, I'm glad you're asking! A is a word that can be used to describe an object or a person. It can also be used as a term that is not a word, but is a concept. You see, it's important to be able to use a word to its best and most un-ficial.\n",
      "\n",
      "What is the question asking?\n",
      "The question being asked in this moment is \"What am I?\" and why are you asking it in a way that is not quite as simple or as fun. It's okay to be curious, but not always necessary to know everything.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What is Linux? \u001b[0m\u001b[91mIf you're a go kac kitten, you can't be a guard any less, can you? No, you can't. You can't be a judge. You can't give a hic. You can't give a ha. I'm sorry, I'm not able to give a bad or un-biased answer. I'm just a bot that can only provide a non-ex. What is a good?\n",
      "Well, I'm glad you're asking! A is a word that can be used to describe an object or a person. It can also be used as a term that is not a word, but is a concept. You see, it's important to be able to use a word to its best and most un-ficial. What is the question asking?\n",
      "The question being asked in this moment is \"What am I?\" and why are you asking it in a way that is not quite as simple or as fun.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Why do people like gardening? \u001b[0m\u001b[91mI'm glad to be here with you, my friend!                        Why not?! I can see you, and I'm here to help. \n",
      "\n",
      "Why do people like to GARINING?\n",
      "\n",
      "Well, I can see why you might be asking this, friend. I can understand why you might be asking this, friend. Why would anyone want to do a nobody.\n",
      "\n",
      "Why do you think I would be not be able to answer your question. I'm sorry, but I can't help but answer your question. I can see why you might be wondering why you are being asked this. If you don't mind my saying this, I can see why you might be asking this question. Why would you want to ask this question. Why would you be asking this question. Why would you be asking this.\n",
      "\n",
      "Why do you think I would be so so. I'm sorry, but I can't help but be honest. I'm here to help.\n",
      "\n",
      "Why do you think I would be so. I'm sorry, but I'm just going to keep it simple. Why do you think I would be. Why do you think you are.\n",
      "\n",
      "Why do you think I would be. I'm sorry, but I'm just going to keep it real. Why do you think I would be.\n",
      "\n",
      "Why do you think you would be. I'm sorry, but I'm just going to keep it real. Why do you think I would be.\n",
      "\n",
      "Why do you think you would be. I'm sorry, but I'm just going to keep it real. Why do you think I would be.\n",
      "\n",
      "Why do you think you would be. I'm sorry, but I'm just going to keep it real. Why do you think you would be.\n",
      "\n",
      "Why do you think you would be. I'm sorry,but I'm just going to keep it real. Why do you think you would be.\n",
      "\n",
      "Why do you think you would be. I'm sorry,but I'm just going to keep it real. Why do you think you would be.\n",
      "\n",
      "Why do you think you would be. I'm sorry,but I'm just going to keep it real. Why do you think you would be.\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Why do people like gardening? \u001b[0m\u001b[91mI'm glad to be here with you, my friend!                        Why not?! I can see you, and I'm here to help. Why do people like to GARINING?\n",
      "\n",
      "Well, I can see why you might be asking this, friend. I can understand why you might be asking this, friend. Why would anyone want to do a nobody. Why do you think I would be not be able to answer your question. I'm sorry, but I can't help but answer your question. I can see why you might be wondering why you are being asked this. If you don't mind my saying this, I can see why you might be asking this question. Why would you want to ask this question. Why would you be asking this question. Why would you be asking this. Why do you think I would be so so. I'm sorry, but I can't help but be honest. I'm here to help. Why do you think I would be so. I'm sorry, but I'm just going to keep it simple. Why do you think I would be. Why do you think you are. I'm sorry, but I'm just going to keep it real. Why do you think you would be.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m What makes for a good photograph? \u001b[0m\u001b[91m\n",
      "A good photo can be a great photo.  You see? It's important to remember that a picture is likely to be a good one.\n",
      "\n",
      "What is a good book?\n",
      "\n",
      "A good book is a book that is likely to be good.\n",
      "\n",
      "I hope this helps! It's important to be aware that a question is a question that is likely to be a good one.\n",
      "\n",
      "What is the point of a good?\n",
      "\n",
      "Asking this question is a great way to get a good.\n",
      "\n",
      "I hope this helps to make the question a good one.\n",
      "\n",
      "\n",
      "What is the point of asking this question?\n",
      "\n",
      "Well, I'm glad you're asking this question, but it's not the only question that is a good.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m What makes for a good photograph? \u001b[0m\u001b[91mA good photo can be a great photo. You see? It's important to remember that a picture is likely to be a good one. What is a good book?\n",
      "\n",
      "A good book is a book that is likely to be good. I hope this helps! It's important to be aware that a question is a question that is likely to be a good one. What is the point of a good?\n",
      "\n",
      "Asking this question is a great way to get a good. I hope this helps to make the question a good one.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.421346816\n",
      "Model size after pruning:  13.421346816\n",
      "Model size after pruning:  13.326655488\n",
      "Model size after pruning:  13.326655488\n",
      "Model size after pruning:  13.23196416\n",
      "Model size after pruning:  13.23196416\n",
      "Model size after pruning:  13.139165184\n",
      "Model size after pruning:  13.139165184\n",
      "Model size after pruning:  13.047349248\n",
      "Model size after pruning:  13.047349248\n",
      "Model size after pruning:  12.95265792\n",
      "Model size after pruning:  12.95265792\n",
      "Model size after pruning:  12.857966592\n",
      "Model size after pruning:  12.857966592\n",
      "Model size after pruning:  12.763275264\n",
      "Model size after pruning:  12.763275264\n",
      "Model size after pruning:  12.668583936\n",
      "Model size after pruning:  12.668583936\n",
      "Model size after pruning:  12.573892608\n",
      "Model size after pruning:  12.552921088\n",
      "Model size after pruning:  12.45822976\n",
      "Model size after pruning:  12.412092416\n",
      "Model size after pruning:  12.317401088\n",
      "Model size after pruning:  12.271263744\n",
      "Model size after pruning:  12.176572416\n",
      "Model size after pruning:  12.130435072\n",
      "Model size after pruning:  12.035743744\n",
      "Model size after pruning:  11.9896064\n",
      "Model size after pruning:  11.894915072\n",
      "Model size after pruning:  11.848777728\n",
      "Model size after pruning:  11.7540864\n",
      "Model size after pruning:  11.707949056\n",
      "Model size after pruning:  11.613257728\n",
      "Model size after pruning:  11.567120384\n",
      "Model size after pruning:  11.472429056\n",
      "Model size after pruning:  11.426291712\n",
      "Model size after pruning:  11.331600384\n",
      "Model size after pruning:  11.28546304\n",
      "Model size after pruning:  11.190771712\n",
      "Model size after pruning:  11.144634368\n",
      "Model size after pruning:  11.04994304\n",
      "Model size after pruning:  11.003805696\n",
      "Model size after pruning:  10.909114368\n",
      "Model size after pruning:  10.862977024\n",
      "Model size after pruning:  10.768285696\n",
      "Model size after pruning:  10.722148352\n",
      "Model size after pruning:  10.627457024\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7214, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7214, bias=False)\n",
      "          (down_proj): Linear(in_features=7214, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5-6): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7232, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7232, bias=False)\n",
      "          (down_proj): Linear(in_features=7232, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7272, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7272, bias=False)\n",
      "          (down_proj): Linear(in_features=7272, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9-13): 5 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
      "          (o_proj): Linear(in_features=3456, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15-27): 13 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
      "          (o_proj): Linear(in_features=2688, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
      "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "10.627457024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 145, 32, 128]' is invalid for input of size 501120",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 240\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, eval_orig_model)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Pruned Model Size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_size())\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_checkpoint()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Experiment completed successfully Successfully  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 158\u001b[0m, in \u001b[0;36mExperimentRunner.get_throughput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_throughput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     throughput, n_tokens, result \u001b[38;5;241m=\u001b[39m \u001b[43mget_gen_text_throughput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is ML?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/sec, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens (including full prompt)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# When the context is long or the generated text is long, it takes longer to generate each token in average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mget_gen_text_throughput\u001b[0;34m(prompt, pipeline, tokenizer, use_template, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# measure the time it takes for text generation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the number of generated tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1179\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1022\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1012\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1013\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         cache_position,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# print(\"Query shape  : \", query_states.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"Key shape  : \", key_states.shape)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Value shape  : \", value_states.shape)\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    360\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    361\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 145, 32, 128]' is invalid for input of size 501120"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3ea69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f47c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c82db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4259f0d1",
   "metadata": {},
   "source": [
    "# Throughput Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a64296",
   "metadata": {},
   "source": [
    "## Experiment : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5358d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:10:58.189383Z",
     "start_time": "2024-05-21T18:10:23.488011Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 23:40:24.613041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 23:40:26.011257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dce38624204b9e9c3f1ae5d6ff9355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd31b08f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:11:27.915978Z",
     "start_time": "2024-05-21T18:11:27.895781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7963fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:11:29.129141Z",
     "start_time": "2024-05-21T18:11:29.112479Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.7_0.65_1.0_base_2428.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0839a636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:11:31.328351Z",
     "start_time": "2024-05-21T18:11:31.288297Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  4096\n",
      "67 ==>  7155\n",
      "75 ==>  4096\n",
      "81 ==>  7155\n",
      "89 ==>  4096\n",
      "95 ==>  7155\n",
      "103 ==>  4096\n",
      "109 ==>  7155\n",
      "117 ==>  4096\n",
      "123 ==>  7155\n",
      "131 ==>  4096\n",
      "137 ==>  7155\n",
      "145 ==>  4096\n",
      "151 ==>  7155\n",
      "159 ==>  4096\n",
      "165 ==>  7155\n",
      "173 ==>  4096\n",
      "179 ==>  7155\n",
      "187 ==>  4096\n",
      "193 ==>  7345\n",
      "201 ==>  3584\n",
      "207 ==>  8227\n",
      "215 ==>  2688\n",
      "221 ==>  7155\n",
      "229 ==>  2688\n",
      "235 ==>  7155\n",
      "243 ==>  2688\n",
      "249 ==>  7155\n",
      "257 ==>  2688\n",
      "263 ==>  7155\n",
      "271 ==>  2688\n",
      "277 ==>  7155\n",
      "285 ==>  2688\n",
      "291 ==>  7155\n",
      "299 ==>  2688\n",
      "305 ==>  7155\n",
      "313 ==>  2688\n",
      "319 ==>  7155\n",
      "327 ==>  2688\n",
      "333 ==>  7155\n",
      "341 ==>  2688\n",
      "347 ==>  7155\n",
      "355 ==>  2688\n",
      "361 ==>  7155\n",
      "369 ==>  2688\n",
      "375 ==>  7155\n",
      "383 ==>  2688\n",
      "389 ==>  7155\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a9f3310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:13:44.012363Z",
     "start_time": "2024-05-21T18:11:31.961478Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.419896832\n",
      "Model size after pruning:  13.419896832\n",
      "Model size after pruning:  13.325205504\n",
      "Model size after pruning:  13.325205504\n",
      "Model size after pruning:  13.230514176\n",
      "Model size after pruning:  13.230514176\n",
      "Model size after pruning:  13.135822848\n",
      "Model size after pruning:  13.135822848\n",
      "Model size after pruning:  13.04113152\n",
      "Model size after pruning:  13.04113152\n",
      "Model size after pruning:  12.946440192\n",
      "Model size after pruning:  12.946440192\n",
      "Model size after pruning:  12.851748864\n",
      "Model size after pruning:  12.851748864\n",
      "Model size after pruning:  12.757057536\n",
      "Model size after pruning:  12.757057536\n",
      "Model size after pruning:  12.662366208\n",
      "Model size after pruning:  12.662366208\n",
      "Model size after pruning:  12.57234432\n",
      "Model size after pruning:  12.555567104\n",
      "Model size after pruning:  12.487221248\n",
      "Model size after pruning:  12.441083904\n",
      "Model size after pruning:  12.346392576\n",
      "Model size after pruning:  12.300255232\n",
      "Model size after pruning:  12.205563904\n",
      "Model size after pruning:  12.15942656\n",
      "Model size after pruning:  12.064735232\n",
      "Model size after pruning:  12.018597888\n",
      "Model size after pruning:  11.92390656\n",
      "Model size after pruning:  11.877769216\n",
      "Model size after pruning:  11.783077888\n",
      "Model size after pruning:  11.736940544\n",
      "Model size after pruning:  11.642249216\n",
      "Model size after pruning:  11.596111872\n",
      "Model size after pruning:  11.501420544\n",
      "Model size after pruning:  11.4552832\n",
      "Model size after pruning:  11.360591872\n",
      "Model size after pruning:  11.314454528\n",
      "Model size after pruning:  11.2197632\n",
      "Model size after pruning:  11.173625856\n",
      "Model size after pruning:  11.078934528\n",
      "Model size after pruning:  11.032797184\n",
      "Model size after pruning:  10.938105856\n",
      "Model size after pruning:  10.891968512\n",
      "Model size after pruning:  10.797277184\n",
      "Model size after pruning:  10.75113984\n",
      "Model size after pruning:  10.656448512\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab7126c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:14:03.685386Z",
     "start_time": "2024-05-21T18:14:03.678283Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5309345792"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5465b051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:10:45.989892Z",
     "start_time": "2024-05-21T12:10:45.984235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 32 7155 32 7155 32 7155 32 7155 32 7155 32 7155 32 7155 32 7155 32 7155 32 7345 28 8227 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6705f513",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:18:02.862754Z",
     "start_time": "2024-05-21T12:10:48.929494Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 17.488632135069533 tokens/sec, 516 tokens (including full prompt)\n",
      "Long Context: 43.847271155925526 tokens/sec, 1288 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  24.559867663772934 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67dd3e87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:18:21.910483Z",
     "start_time": "2024-05-21T12:18:21.905365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.use_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4b755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4824e2db",
   "metadata": {},
   "source": [
    "## Experiment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebc637da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:21:18.460502Z",
     "start_time": "2024-05-21T12:20:53.976102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 17:50:54.941519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 17:50:55.883883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf2ea0babc8427390e02c9883a97d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ec9ecf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:21:30.876362Z",
     "start_time": "2024-05-21T12:21:30.870127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6987819d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:21:31.785697Z",
     "start_time": "2024-05-21T12:21:31.781533Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.7_0.9_base_2424.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c18d89a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:21:32.596503Z",
     "start_time": "2024-05-21T12:21:32.550683Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  3712\n",
      "67 ==>  9521\n",
      "75 ==>  3712\n",
      "81 ==>  9250\n",
      "89 ==>  3200\n",
      "95 ==>  9907\n",
      "103 ==>  3584\n",
      "109 ==>  9907\n",
      "117 ==>  3712\n",
      "123 ==>  9725\n",
      "131 ==>  3712\n",
      "137 ==>  9907\n",
      "145 ==>  3712\n",
      "151 ==>  9907\n",
      "159 ==>  3712\n",
      "165 ==>  9907\n",
      "173 ==>  3712\n",
      "179 ==>  9907\n",
      "187 ==>  3712\n",
      "193 ==>  9548\n",
      "201 ==>  3712\n",
      "207 ==>  9907\n",
      "215 ==>  3712\n",
      "221 ==>  9907\n",
      "229 ==>  3712\n",
      "235 ==>  9907\n",
      "243 ==>  3712\n",
      "249 ==>  9907\n",
      "257 ==>  3712\n",
      "263 ==>  9907\n",
      "271 ==>  3712\n",
      "277 ==>  9907\n",
      "285 ==>  3712\n",
      "291 ==>  9907\n",
      "299 ==>  3712\n",
      "305 ==>  9907\n",
      "313 ==>  3584\n",
      "319 ==>  9907\n",
      "327 ==>  3328\n",
      "333 ==>  9907\n",
      "341 ==>  3712\n",
      "347 ==>  9821\n",
      "355 ==>  3584\n",
      "361 ==>  9907\n",
      "369 ==>  3712\n",
      "375 ==>  9907\n",
      "383 ==>  3712\n",
      "389 ==>  9907\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba6c8f73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:22:49.965641Z",
     "start_time": "2024-05-21T12:21:35.508957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.502005248\n",
      "Model size after pruning:  13.465460736\n",
      "Model size after pruning:  13.452877824\n",
      "Model size after pruning:  13.409673216\n",
      "Model size after pruning:  13.380313088\n",
      "Model size after pruning:  13.353254912\n",
      "Model size after pruning:  13.336477696\n",
      "Model size after pruning:  13.30941952\n",
      "Model size after pruning:  13.296836608\n",
      "Model size after pruning:  13.2653056\n",
      "Model size after pruning:  13.252722688\n",
      "Model size after pruning:  13.225664512\n",
      "Model size after pruning:  13.2130816\n",
      "Model size after pruning:  13.186023424\n",
      "Model size after pruning:  13.173440512\n",
      "Model size after pruning:  13.146382336\n",
      "Model size after pruning:  13.133799424\n",
      "Model size after pruning:  13.106741248\n",
      "Model size after pruning:  13.094158336\n",
      "Model size after pruning:  13.058277376\n",
      "Model size after pruning:  13.045694464\n",
      "Model size after pruning:  13.018636288\n",
      "Model size after pruning:  13.006053376\n",
      "Model size after pruning:  12.9789952\n",
      "Model size after pruning:  12.966412288\n",
      "Model size after pruning:  12.939354112\n",
      "Model size after pruning:  12.9267712\n",
      "Model size after pruning:  12.899713024\n",
      "Model size after pruning:  12.887130112\n",
      "Model size after pruning:  12.860071936\n",
      "Model size after pruning:  12.847489024\n",
      "Model size after pruning:  12.820430848\n",
      "Model size after pruning:  12.807847936\n",
      "Model size after pruning:  12.78078976\n",
      "Model size after pruning:  12.768206848\n",
      "Model size after pruning:  12.741148672\n",
      "Model size after pruning:  12.724371456\n",
      "Model size after pruning:  12.69731328\n",
      "Model size after pruning:  12.672147456\n",
      "Model size after pruning:  12.64508928\n",
      "Model size after pruning:  12.632506368\n",
      "Model size after pruning:  12.603334656\n",
      "Model size after pruning:  12.58655744\n",
      "Model size after pruning:  12.559499264\n",
      "Model size after pruning:  12.546916352\n",
      "Model size after pruning:  12.519858176\n",
      "Model size after pruning:  12.507275264\n",
      "Model size after pruning:  12.480217088\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d138745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:22:49.976119Z",
     "start_time": "2024-05-21T12:22:49.968512Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9521, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9521, bias=False)\n",
       "          (down_proj): Linear(in_features=9521, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9250, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9250, bias=False)\n",
       "          (down_proj): Linear(in_features=9250, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9725, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9725, bias=False)\n",
       "          (down_proj): Linear(in_features=9725, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9-12): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9548, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9548, bias=False)\n",
       "          (down_proj): Linear(in_features=9548, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14-21): 8 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
       "          (down_proj): Linear(in_features=9821, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26-27): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53865b65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:22:49.984110Z",
     "start_time": "2024-05-21T12:22:49.978280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 29 9521 29 9250 25 9907 28 9907 29 9725 29 9907 29 9907 29 9907 29 9907 29 9548 29 9907 29 9907 29 9907 29 9907 29 9907 29 9907 29 9907 29 9907 28 9907 26 9907 29 9821 28 9907 29 9907 29 9907 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e50ee0d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:28:13.111850Z",
     "start_time": "2024-05-21T12:22:49.986927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 17.49132674454405 tokens/sec, 128 tokens (including full prompt)\n",
      "Long Context: 73.68685365551008 tokens/sec, 1030 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  20.811589757540972 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "322f72c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:39:26.921343Z",
     "start_time": "2024-05-21T13:39:26.914522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6221230080"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ddd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21be23c2",
   "metadata": {},
   "source": [
    "## Experiment 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aafcdf3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:41:58.508562Z",
     "start_time": "2024-05-21T13:41:30.200293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 19:11:31.213625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 19:11:32.245071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b351c89defd4dc09ab08fb1ba7139f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0edcf7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:51:47.819204Z",
     "start_time": "2024-05-21T13:51:47.811174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9509, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9509, bias=False)\n",
       "          (down_proj): Linear(in_features=9509, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9233, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9233, bias=False)\n",
       "          (down_proj): Linear(in_features=9233, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9701, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9701, bias=False)\n",
       "          (down_proj): Linear(in_features=9701, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9-12): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9488, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9488, bias=False)\n",
       "          (down_proj): Linear(in_features=9488, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9885, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9885, bias=False)\n",
       "          (down_proj): Linear(in_features=9885, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-21): 6 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9568, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9568, bias=False)\n",
       "          (down_proj): Linear(in_features=9568, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26-27): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f142b0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:51:54.748159Z",
     "start_time": "2024-05-21T13:51:54.735636Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_0.9_base_2422.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f88d168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:42:20.528813Z",
     "start_time": "2024-05-21T13:42:20.481697Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  3712\n",
      "67 ==>  9509\n",
      "75 ==>  3712\n",
      "81 ==>  9233\n",
      "89 ==>  3328\n",
      "95 ==>  9907\n",
      "103 ==>  3584\n",
      "109 ==>  9907\n",
      "117 ==>  3712\n",
      "123 ==>  9701\n",
      "131 ==>  3712\n",
      "137 ==>  9907\n",
      "145 ==>  3712\n",
      "151 ==>  9907\n",
      "159 ==>  3712\n",
      "165 ==>  9907\n",
      "173 ==>  3712\n",
      "179 ==>  9907\n",
      "187 ==>  3712\n",
      "193 ==>  9488\n",
      "201 ==>  3712\n",
      "207 ==>  9907\n",
      "215 ==>  3712\n",
      "221 ==>  9885\n",
      "229 ==>  3712\n",
      "235 ==>  9907\n",
      "243 ==>  3712\n",
      "249 ==>  9907\n",
      "257 ==>  3712\n",
      "263 ==>  9907\n",
      "271 ==>  3712\n",
      "277 ==>  9907\n",
      "285 ==>  3712\n",
      "291 ==>  9907\n",
      "299 ==>  3712\n",
      "305 ==>  9907\n",
      "313 ==>  3584\n",
      "319 ==>  9907\n",
      "327 ==>  3328\n",
      "333 ==>  9907\n",
      "341 ==>  3712\n",
      "347 ==>  9568\n",
      "355 ==>  3584\n",
      "361 ==>  9907\n",
      "369 ==>  3712\n",
      "375 ==>  9907\n",
      "383 ==>  3712\n",
      "389 ==>  9907\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca077c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:42:22.518794Z",
     "start_time": "2024-05-21T13:42:22.512677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61c804d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:43:52.958541Z",
     "start_time": "2024-05-21T13:42:29.679994Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.502005248\n",
      "Model size after pruning:  13.465165824\n",
      "Model size after pruning:  13.452582912\n",
      "Model size after pruning:  13.408960512\n",
      "Model size after pruning:  13.383794688\n",
      "Model size after pruning:  13.356736512\n",
      "Model size after pruning:  13.339959296\n",
      "Model size after pruning:  13.31290112\n",
      "Model size after pruning:  13.300318208\n",
      "Model size after pruning:  13.268197376\n",
      "Model size after pruning:  13.255614464\n",
      "Model size after pruning:  13.228556288\n",
      "Model size after pruning:  13.215973376\n",
      "Model size after pruning:  13.1889152\n",
      "Model size after pruning:  13.176332288\n",
      "Model size after pruning:  13.149274112\n",
      "Model size after pruning:  13.1366912\n",
      "Model size after pruning:  13.109633024\n",
      "Model size after pruning:  13.097050112\n",
      "Model size after pruning:  13.059694592\n",
      "Model size after pruning:  13.04711168\n",
      "Model size after pruning:  13.020053504\n",
      "Model size after pruning:  13.007470592\n",
      "Model size after pruning:  12.979871744\n",
      "Model size after pruning:  12.967288832\n",
      "Model size after pruning:  12.940230656\n",
      "Model size after pruning:  12.927647744\n",
      "Model size after pruning:  12.900589568\n",
      "Model size after pruning:  12.888006656\n",
      "Model size after pruning:  12.86094848\n",
      "Model size after pruning:  12.848365568\n",
      "Model size after pruning:  12.821307392\n",
      "Model size after pruning:  12.80872448\n",
      "Model size after pruning:  12.781666304\n",
      "Model size after pruning:  12.769083392\n",
      "Model size after pruning:  12.742025216\n",
      "Model size after pruning:  12.725248\n",
      "Model size after pruning:  12.698189824\n",
      "Model size after pruning:  12.673024\n",
      "Model size after pruning:  12.645965824\n",
      "Model size after pruning:  12.633382912\n",
      "Model size after pruning:  12.597993472\n",
      "Model size after pruning:  12.581216256\n",
      "Model size after pruning:  12.55415808\n",
      "Model size after pruning:  12.541575168\n",
      "Model size after pruning:  12.514516992\n",
      "Model size after pruning:  12.50193408\n",
      "Model size after pruning:  12.474875904\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bb0149b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:43:52.970831Z",
     "start_time": "2024-05-21T13:43:52.962052Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9509, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9509, bias=False)\n",
       "          (down_proj): Linear(in_features=9509, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9233, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9233, bias=False)\n",
       "          (down_proj): Linear(in_features=9233, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9701, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9701, bias=False)\n",
       "          (down_proj): Linear(in_features=9701, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9-12): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9488, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9488, bias=False)\n",
       "          (down_proj): Linear(in_features=9488, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9885, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9885, bias=False)\n",
       "          (down_proj): Linear(in_features=9885, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-21): 6 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9568, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9568, bias=False)\n",
       "          (down_proj): Linear(in_features=9568, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26-27): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e73099b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:43:52.979855Z",
     "start_time": "2024-05-21T13:43:52.973610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 29 9509 29 9233 26 9907 28 9907 29 9701 29 9907 29 9907 29 9907 29 9907 29 9488 29 9907 29 9885 29 9907 29 9907 29 9907 29 9907 29 9907 29 9907 28 9907 26 9907 29 9568 28 9907 29 9907 29 9907 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94218eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:49:25.383664Z",
     "start_time": "2024-05-21T13:43:52.982976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 16.641209715028335 tokens/sec, 516 tokens (including full prompt)\n",
      "Long Context: 42.539702595210244 tokens/sec, 1288 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  28.57110736058541 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47e435b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:49:25.393519Z",
     "start_time": "2024-05-21T13:49:25.386744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218559488"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b2953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9bd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96c3bb5b",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b82f2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:19:15.814482Z",
     "start_time": "2024-05-21T14:19:15.810522Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df7b0d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:19:45.305443Z",
     "start_time": "2024-05-21T14:19:19.482279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c89fb331c4e4937ab41d2f60a5a3072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ea5c103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:19:45.313804Z",
     "start_time": "2024-05-21T14:19:45.308745Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_0.95_base_2420.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f21d4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:19:45.373188Z",
     "start_time": "2024-05-21T14:19:45.317507Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  3840\n",
      "67 ==>  10458\n",
      "75 ==>  3840\n",
      "81 ==>  10458\n",
      "89 ==>  3840\n",
      "95 ==>  10458\n",
      "103 ==>  3840\n",
      "109 ==>  10458\n",
      "117 ==>  3840\n",
      "123 ==>  10458\n",
      "131 ==>  3840\n",
      "137 ==>  10458\n",
      "145 ==>  3712\n",
      "151 ==>  10458\n",
      "159 ==>  3840\n",
      "165 ==>  10458\n",
      "173 ==>  3840\n",
      "179 ==>  10458\n",
      "187 ==>  3840\n",
      "193 ==>  9979\n",
      "201 ==>  3840\n",
      "207 ==>  10458\n",
      "215 ==>  3840\n",
      "221 ==>  10458\n",
      "229 ==>  3840\n",
      "235 ==>  10458\n",
      "243 ==>  3840\n",
      "249 ==>  10458\n",
      "257 ==>  3840\n",
      "263 ==>  10416\n",
      "271 ==>  3840\n",
      "277 ==>  10182\n",
      "285 ==>  3840\n",
      "291 ==>  10458\n",
      "299 ==>  3840\n",
      "305 ==>  10458\n",
      "313 ==>  3840\n",
      "319 ==>  10458\n",
      "327 ==>  3840\n",
      "333 ==>  10163\n",
      "341 ==>  3840\n",
      "347 ==>  10458\n",
      "355 ==>  3840\n",
      "361 ==>  10445\n",
      "369 ==>  3840\n",
      "375 ==>  10458\n",
      "383 ==>  3840\n",
      "389 ==>  10418\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7164864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:19:45.394964Z",
     "start_time": "2024-05-21T14:19:45.376159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82ca80a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:21:10.810903Z",
     "start_time": "2024-05-21T14:19:45.397439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.506199552\n",
      "Model size after pruning:  13.492682752\n",
      "Model size after pruning:  13.484294144\n",
      "Model size after pruning:  13.470777344\n",
      "Model size after pruning:  13.462388736\n",
      "Model size after pruning:  13.448871936\n",
      "Model size after pruning:  13.440483328\n",
      "Model size after pruning:  13.426966528\n",
      "Model size after pruning:  13.41857792\n",
      "Model size after pruning:  13.40506112\n",
      "Model size after pruning:  13.396672512\n",
      "Model size after pruning:  13.383155712\n",
      "Model size after pruning:  13.3705728\n",
      "Model size after pruning:  13.357056\n",
      "Model size after pruning:  13.348667392\n",
      "Model size after pruning:  13.335150592\n",
      "Model size after pruning:  13.326761984\n",
      "Model size after pruning:  13.313245184\n",
      "Model size after pruning:  13.304856576\n",
      "Model size after pruning:  13.279567872\n",
      "Model size after pruning:  13.271179264\n",
      "Model size after pruning:  13.257662464\n",
      "Model size after pruning:  13.249273856\n",
      "Model size after pruning:  13.235757056\n",
      "Model size after pruning:  13.227368448\n",
      "Model size after pruning:  13.213851648\n",
      "Model size after pruning:  13.20546304\n",
      "Model size after pruning:  13.19194624\n",
      "Model size after pruning:  13.183557632\n",
      "Model size after pruning:  13.16900864\n",
      "Model size after pruning:  13.160620032\n",
      "Model size after pruning:  13.140320256\n",
      "Model size after pruning:  13.131931648\n",
      "Model size after pruning:  13.118414848\n",
      "Model size after pruning:  13.11002624\n",
      "Model size after pruning:  13.09650944\n",
      "Model size after pruning:  13.088120832\n",
      "Model size after pruning:  13.074604032\n",
      "Model size after pruning:  13.066215424\n",
      "Model size after pruning:  13.045448704\n",
      "Model size after pruning:  13.037060096\n",
      "Model size after pruning:  13.023543296\n",
      "Model size after pruning:  13.015154688\n",
      "Model size after pruning:  13.0013184\n",
      "Model size after pruning:  12.992929792\n",
      "Model size after pruning:  12.979412992\n",
      "Model size after pruning:  12.971024384\n",
      "Model size after pruning:  12.956524544\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "653eca1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:21:10.823108Z",
     "start_time": "2024-05-21T14:21:10.813752Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4-9): 6 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11-12): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9979, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9979, bias=False)\n",
       "          (down_proj): Linear(in_features=9979, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14-17): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10416, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10416, bias=False)\n",
       "          (down_proj): Linear(in_features=10416, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10182, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10182, bias=False)\n",
       "          (down_proj): Linear(in_features=10182, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20-22): 3 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10163, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10163, bias=False)\n",
       "          (down_proj): Linear(in_features=10163, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10445, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10445, bias=False)\n",
       "          (down_proj): Linear(in_features=10445, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10458, bias=False)\n",
       "          (down_proj): Linear(in_features=10458, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10418, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10418, bias=False)\n",
       "          (down_proj): Linear(in_features=10418, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96fc2ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:21:10.831424Z",
     "start_time": "2024-05-21T14:21:10.825321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 30 10458 30 10458 30 10458 30 10458 30 10458 30 10458 29 10458 30 10458 30 10458 30 9979 30 10458 30 10458 30 10458 30 10458 30 10416 30 10182 30 10458 30 10458 30 10458 30 10163 30 10458 30 10445 30 10458 30 10418 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be358a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:29:49.010538Z",
     "start_time": "2024-05-21T14:21:10.833603Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 15.379338561223395 tokens/sec, 516 tokens (including full prompt)\n",
      "Long Context: 41.75953671261531 tokens/sec, 1237 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  17.374658155478773 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a57a03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:29:49.020404Z",
     "start_time": "2024-05-21T14:29:49.013389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6459383808"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813991e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b5bd6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:29:49.036148Z",
     "start_time": "2024-05-21T14:29:49.024981Z"
    }
   },
   "outputs": [],
   "source": [
    "del experiment.pipeline\n",
    "del experiment.model\n",
    "del experiment.tokenizer\n",
    "\n",
    "del experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "732cfb6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:29:49.050245Z",
     "start_time": "2024-05-21T14:29:49.038251Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "del model\n",
    "del tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38346418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:29:49.108437Z",
     "start_time": "2024-05-21T14:29:49.052477Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa8f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2161fc2",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "509b4b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:32:57.850785Z",
     "start_time": "2024-05-21T14:32:57.847343Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec5d2dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:33:24.680088Z",
     "start_time": "2024-05-21T14:32:58.820902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c221a9fe744915be181868d8bd06ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ea83e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:33:24.688142Z",
     "start_time": "2024-05-21T14:33:24.683064Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_1.0_base_2396.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "894e3138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:33:24.751136Z",
     "start_time": "2024-05-21T14:33:24.690870Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  4096\n",
      "67 ==>  11007\n",
      "75 ==>  4096\n",
      "81 ==>  11007\n",
      "89 ==>  4096\n",
      "95 ==>  11005\n",
      "103 ==>  4096\n",
      "109 ==>  11007\n",
      "117 ==>  4096\n",
      "123 ==>  11008\n",
      "131 ==>  4096\n",
      "137 ==>  11008\n",
      "145 ==>  4096\n",
      "151 ==>  11004\n",
      "159 ==>  4096\n",
      "165 ==>  11006\n",
      "173 ==>  4096\n",
      "179 ==>  11002\n",
      "187 ==>  4096\n",
      "193 ==>  11007\n",
      "201 ==>  4096\n",
      "207 ==>  11008\n",
      "215 ==>  4096\n",
      "221 ==>  11006\n",
      "229 ==>  4096\n",
      "235 ==>  11008\n",
      "243 ==>  4096\n",
      "249 ==>  11008\n",
      "257 ==>  4096\n",
      "263 ==>  11004\n",
      "271 ==>  4096\n",
      "277 ==>  11007\n",
      "285 ==>  4096\n",
      "291 ==>  11006\n",
      "299 ==>  4096\n",
      "305 ==>  11006\n",
      "313 ==>  4096\n",
      "319 ==>  9577\n",
      "327 ==>  3328\n",
      "333 ==>  8806\n",
      "341 ==>  3328\n",
      "347 ==>  8806\n",
      "355 ==>  3328\n",
      "361 ==>  8806\n",
      "369 ==>  3328\n",
      "375 ==>  8806\n",
      "383 ==>  3328\n",
      "389 ==>  8806\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ad73e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:33:24.765230Z",
     "start_time": "2024-05-21T14:33:24.757865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a49c033d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:34:52.174892Z",
     "start_time": "2024-05-21T14:33:24.767537Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.514293248\n",
      "Model size after pruning:  13.514293248\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514121216\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.514072064\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513900032\n",
      "Model size after pruning:  13.513900032\n",
      "Model size after pruning:  13.51385088\n",
      "Model size after pruning:  13.51385088\n",
      "Model size after pruning:  13.478682624\n",
      "Model size after pruning:  13.4535168\n",
      "Model size after pruning:  13.399400448\n",
      "Model size after pruning:  13.374234624\n",
      "Model size after pruning:  13.320118272\n",
      "Model size after pruning:  13.294952448\n",
      "Model size after pruning:  13.240836096\n",
      "Model size after pruning:  13.215670272\n",
      "Model size after pruning:  13.16155392\n",
      "Model size after pruning:  13.136388096\n",
      "Model size after pruning:  13.082271744\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8326d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:34:52.188212Z",
     "start_time": "2024-05-21T14:34:52.177802Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4-5): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8-9): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
       "          (down_proj): Linear(in_features=11002, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-17): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20-21): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9577, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9577, bias=False)\n",
       "          (down_proj): Linear(in_features=9577, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23-27): 5 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
       "          (down_proj): Linear(in_features=8806, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1a9a2b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:34:52.196475Z",
     "start_time": "2024-05-21T14:34:52.190791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 32 11007 32 11007 32 11005 32 11007 32 11008 32 11008 32 11004 32 11006 32 11002 32 11007 32 11008 32 11006 32 11008 32 11008 32 11004 32 11007 32 11006 32 11006 32 9577 26 8806 26 8806 26 8806 26 8806 26 8806 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8884a821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:30.072966Z",
     "start_time": "2024-05-21T14:34:52.198612Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 15.423245284082709 tokens/sec, 516 tokens (including full prompt)\n",
      "Long Context: 39.05555798506258 tokens/sec, 1288 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  17.354396407337894 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd7b647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:30.084266Z",
     "start_time": "2024-05-21T14:43:30.075723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6522257408"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331edb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e6d79e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:30.100438Z",
     "start_time": "2024-05-21T14:43:30.088815Z"
    }
   },
   "outputs": [],
   "source": [
    "del experiment.pipeline\n",
    "del experiment.model\n",
    "del experiment.tokenizer\n",
    "\n",
    "del experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d7b8eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:30.117200Z",
     "start_time": "2024-05-21T14:43:30.102605Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "del model\n",
    "del tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "048bde4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:30.179765Z",
     "start_time": "2024-05-21T14:43:30.119769Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d797a50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:45.196475Z",
     "start_time": "2024-05-21T14:43:30.181987Z"
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217e9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "291606e5",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2064debd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:46:27.648037Z",
     "start_time": "2024-05-21T14:46:27.645424Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1529e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:46:57.673499Z",
     "start_time": "2024-05-21T14:46:29.100023Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 20:16:30.102677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 20:16:31.062523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc5776ae62c48f595c8891d9e7c72c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95da9bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:46:57.682586Z",
     "start_time": "2024-05-21T14:46:57.678312Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_1.0_base_2374.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54f4a35d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:46:57.736266Z",
     "start_time": "2024-05-21T14:46:57.684655Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  4096\n",
      "67 ==>  10998\n",
      "75 ==>  4096\n",
      "81 ==>  11007\n",
      "89 ==>  4096\n",
      "95 ==>  11005\n",
      "103 ==>  4096\n",
      "109 ==>  11000\n",
      "117 ==>  4096\n",
      "123 ==>  11006\n",
      "131 ==>  4096\n",
      "137 ==>  10997\n",
      "145 ==>  4096\n",
      "151 ==>  11006\n",
      "159 ==>  4096\n",
      "165 ==>  11004\n",
      "173 ==>  4096\n",
      "179 ==>  10998\n",
      "187 ==>  4096\n",
      "193 ==>  11002\n",
      "201 ==>  4096\n",
      "207 ==>  11003\n",
      "215 ==>  4096\n",
      "221 ==>  11007\n",
      "229 ==>  4096\n",
      "235 ==>  11007\n",
      "243 ==>  4096\n",
      "249 ==>  11006\n",
      "257 ==>  4096\n",
      "263 ==>  11007\n",
      "271 ==>  4096\n",
      "277 ==>  11008\n",
      "285 ==>  4096\n",
      "291 ==>  11004\n",
      "299 ==>  4096\n",
      "305 ==>  11007\n",
      "313 ==>  4096\n",
      "319 ==>  9630\n",
      "327 ==>  3328\n",
      "333 ==>  8806\n",
      "341 ==>  3328\n",
      "347 ==>  8806\n",
      "355 ==>  3328\n",
      "361 ==>  8806\n",
      "369 ==>  3328\n",
      "375 ==>  8806\n",
      "383 ==>  3328\n",
      "389 ==>  8806\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0766bb40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:46:57.745693Z",
     "start_time": "2024-05-21T14:46:57.739551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a1681c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:48:19.593828Z",
     "start_time": "2024-05-21T14:46:57.747832Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.5143424\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.514047488\n",
      "Model size after pruning:  13.514047488\n",
      "Model size after pruning:  13.513998336\n",
      "Model size after pruning:  13.513998336\n",
      "Model size after pruning:  13.513728\n",
      "Model size after pruning:  13.513728\n",
      "Model size after pruning:  13.513678848\n",
      "Model size after pruning:  13.513678848\n",
      "Model size after pruning:  13.513580544\n",
      "Model size after pruning:  13.513580544\n",
      "Model size after pruning:  13.513334784\n",
      "Model size after pruning:  13.513334784\n",
      "Model size after pruning:  13.513187328\n",
      "Model size after pruning:  13.513187328\n",
      "Model size after pruning:  13.513064448\n",
      "Model size after pruning:  13.513064448\n",
      "Model size after pruning:  13.513039872\n",
      "Model size after pruning:  13.513039872\n",
      "Model size after pruning:  13.513015296\n",
      "Model size after pruning:  13.513015296\n",
      "Model size after pruning:  13.512966144\n",
      "Model size after pruning:  13.512966144\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512941568\n",
      "Model size after pruning:  13.512843264\n",
      "Model size after pruning:  13.512843264\n",
      "Model size after pruning:  13.512818688\n",
      "Model size after pruning:  13.512818688\n",
      "Model size after pruning:  13.47895296\n",
      "Model size after pruning:  13.453787136\n",
      "Model size after pruning:  13.399670784\n",
      "Model size after pruning:  13.37450496\n",
      "Model size after pruning:  13.320388608\n",
      "Model size after pruning:  13.295222784\n",
      "Model size after pruning:  13.241106432\n",
      "Model size after pruning:  13.215940608\n",
      "Model size after pruning:  13.161824256\n",
      "Model size after pruning:  13.136658432\n",
      "Model size after pruning:  13.08254208\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3575eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:48:19.604643Z",
     "start_time": "2024-05-21T14:48:19.596182Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
       "          (down_proj): Linear(in_features=10998, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11000, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11000, bias=False)\n",
       "          (down_proj): Linear(in_features=11000, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10997, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10997, bias=False)\n",
       "          (down_proj): Linear(in_features=10997, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=10998, bias=False)\n",
       "          (down_proj): Linear(in_features=10998, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
       "          (down_proj): Linear(in_features=11002, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11003, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11003, bias=False)\n",
       "          (down_proj): Linear(in_features=11003, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15-16): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11004, bias=False)\n",
       "          (down_proj): Linear(in_features=11004, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9630, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9630, bias=False)\n",
       "          (down_proj): Linear(in_features=9630, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23-27): 5 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
       "          (down_proj): Linear(in_features=8806, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89eee827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:48:19.612348Z",
     "start_time": "2024-05-21T14:48:19.606779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 32 10998 32 11007 32 11005 32 11000 32 11006 32 10997 32 11006 32 11004 32 10998 32 11002 32 11003 32 11007 32 11007 32 11006 32 11007 32 11008 32 11004 32 11007 32 9630 26 8806 26 8806 26 8806 26 8806 26 8806 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd326e26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:57:08.693017Z",
     "start_time": "2024-05-21T14:48:19.614225Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 17.34879797060844 tokens/sec, 516 tokens (including full prompt)\n",
      "Long Context: 44.461773583497205 tokens/sec, 1288 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  18.64764460614587 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec30b68d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:59:49.138107Z",
     "start_time": "2024-05-21T14:59:49.125024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6522392576"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b5e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "126f4030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:59:54.180658Z",
     "start_time": "2024-05-21T14:59:54.165459Z"
    }
   },
   "outputs": [],
   "source": [
    "del experiment.pipeline\n",
    "del experiment.model\n",
    "del experiment.tokenizer\n",
    "\n",
    "del experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "252449f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:59:59.567134Z",
     "start_time": "2024-05-21T14:59:59.551144Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "del model\n",
    "del tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c115a9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:01:42.731165Z",
     "start_time": "2024-05-21T15:01:42.727810Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18f7aaa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:00:52.996241Z",
     "start_time": "2024-05-21T15:00:37.978809Z"
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41f57c",
   "metadata": {},
   "source": [
    "## Experiment 1 Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07798891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:02:54.616531Z",
     "start_time": "2024-05-21T15:02:54.612667Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59d4b05b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:03:23.186561Z",
     "start_time": "2024-05-21T15:02:55.655097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 20:32:56.645444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 20:32:57.594754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29893eecf12a4b0aa6c767a7fde162d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_chat_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_chat_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edc0de17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:03:23.194537Z",
     "start_time": "2024-05-21T15:03:23.189758Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.8_1.0_chat_2430.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3716cabb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:03:23.251251Z",
     "start_time": "2024-05-21T15:03:23.197322Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  4096\n",
      "67 ==>  11007\n",
      "75 ==>  4096\n",
      "81 ==>  11007\n",
      "89 ==>  4096\n",
      "95 ==>  11006\n",
      "103 ==>  4096\n",
      "109 ==>  11007\n",
      "117 ==>  4096\n",
      "123 ==>  11007\n",
      "131 ==>  4096\n",
      "137 ==>  11006\n",
      "145 ==>  4096\n",
      "151 ==>  11005\n",
      "159 ==>  4096\n",
      "165 ==>  11008\n",
      "173 ==>  4096\n",
      "179 ==>  11005\n",
      "187 ==>  4096\n",
      "193 ==>  11007\n",
      "201 ==>  4096\n",
      "207 ==>  11005\n",
      "215 ==>  4096\n",
      "221 ==>  11001\n",
      "229 ==>  4096\n",
      "235 ==>  11007\n",
      "243 ==>  4096\n",
      "249 ==>  11002\n",
      "257 ==>  4096\n",
      "263 ==>  11007\n",
      "271 ==>  4096\n",
      "277 ==>  11005\n",
      "285 ==>  4096\n",
      "291 ==>  11006\n",
      "299 ==>  4096\n",
      "305 ==>  11008\n",
      "313 ==>  4096\n",
      "319 ==>  9587\n",
      "327 ==>  3328\n",
      "333 ==>  8806\n",
      "341 ==>  3328\n",
      "347 ==>  8806\n",
      "355 ==>  3328\n",
      "361 ==>  8806\n",
      "369 ==>  3328\n",
      "375 ==>  8806\n",
      "383 ==>  3328\n",
      "389 ==>  8806\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7959b57b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:03:23.261994Z",
     "start_time": "2024-05-21T15:03:23.254995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1a2fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:04:50.256755Z",
     "start_time": "2024-05-21T15:03:23.265853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514563584\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.514539008\n",
      "Model size after pruning:  13.514489856\n",
      "Model size after pruning:  13.514489856\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.51446528\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514440704\n",
      "Model size after pruning:  13.514391552\n",
      "Model size after pruning:  13.514391552\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514317824\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.514244096\n",
      "Model size after pruning:  13.51421952\n",
      "Model size after pruning:  13.51421952\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.514145792\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.51397376\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513949184\n",
      "Model size after pruning:  13.513801728\n",
      "Model size after pruning:  13.513801728\n",
      "Model size after pruning:  13.513777152\n",
      "Model size after pruning:  13.513777152\n",
      "Model size after pruning:  13.513703424\n",
      "Model size after pruning:  13.513703424\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.513654272\n",
      "Model size after pruning:  13.478731776\n",
      "Model size after pruning:  13.453565952\n",
      "Model size after pruning:  13.3994496\n",
      "Model size after pruning:  13.374283776\n",
      "Model size after pruning:  13.320167424\n",
      "Model size after pruning:  13.2950016\n",
      "Model size after pruning:  13.240885248\n",
      "Model size after pruning:  13.215719424\n",
      "Model size after pruning:  13.161603072\n",
      "Model size after pruning:  13.136437248\n",
      "Model size after pruning:  13.082320896\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0eeed4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:04:50.268212Z",
     "start_time": "2024-05-21T15:04:50.259389Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4-5): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7-8): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11001, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11001, bias=False)\n",
       "          (down_proj): Linear(in_features=11001, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11002, bias=False)\n",
       "          (down_proj): Linear(in_features=11002, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11007, bias=False)\n",
       "          (down_proj): Linear(in_features=11007, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11005, bias=False)\n",
       "          (down_proj): Linear(in_features=11005, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11006, bias=False)\n",
       "          (down_proj): Linear(in_features=11006, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9587, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9587, bias=False)\n",
       "          (down_proj): Linear(in_features=9587, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23-27): 5 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8806, bias=False)\n",
       "          (down_proj): Linear(in_features=8806, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "954e2ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:04:50.276048Z",
     "start_time": "2024-05-21T15:04:50.270286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 32 11007 32 11007 32 11006 32 11007 32 11007 32 11006 32 11005 32 11008 32 11005 32 11007 32 11005 32 11001 32 11007 32 11002 32 11007 32 11005 32 11006 32 11008 32 9587 26 8806 26 8806 26 8806 26 8806 26 8806 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b352ba3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:10:16.004492Z",
     "start_time": "2024-05-21T15:04:50.278228Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 26.964524827428562 tokens/sec, 340 tokens (including full prompt)\n",
      "Long Context: 56.520363271910064 tokens/sec, 1265 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  27.45572536250262 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c762c623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:10:16.015024Z",
     "start_time": "2024-05-21T15:10:16.007661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6522281984"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e46887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c3c4ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:08:13.824070Z",
     "start_time": "2024-05-21T16:08:12.706318Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mexperiment\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mtokenizer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "del experiment.pipeline\n",
    "del experiment.model\n",
    "del experiment.tokenizer\n",
    "\n",
    "del experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b32d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:08:13.826740Z",
     "start_time": "2024-05-21T16:08:13.826723Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "del model\n",
    "del tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c6964bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:10:16.101500Z",
     "start_time": "2024-05-21T15:10:16.054960Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f604e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:10:31.120265Z",
     "start_time": "2024-05-21T15:10:16.103789Z"
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261e997",
   "metadata": {},
   "source": [
    "## Experiment 2 Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250f5b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:08:51.208937Z",
     "start_time": "2024-05-21T16:08:51.206054Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3498c8d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:09:20.860698Z",
     "start_time": "2024-05-21T16:08:52.354639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 21:38:53.307699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 21:38:54.240880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283dac2583a441f4807777dfb548caa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_chat_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_chat_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ddf74d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:15:29.330937Z",
     "start_time": "2024-05-21T16:15:29.325620Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.95_0.7_0.9_chat_2435.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5702ac39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:15:30.348509Z",
     "start_time": "2024-05-21T16:15:30.301780Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  3712\n",
      "67 ==>  9227\n",
      "75 ==>  3712\n",
      "81 ==>  8954\n",
      "89 ==>  3200\n",
      "95 ==>  9722\n",
      "103 ==>  3584\n",
      "109 ==>  9907\n",
      "117 ==>  3712\n",
      "123 ==>  9499\n",
      "131 ==>  3712\n",
      "137 ==>  9907\n",
      "145 ==>  3712\n",
      "151 ==>  9761\n",
      "159 ==>  3712\n",
      "165 ==>  9907\n",
      "173 ==>  3712\n",
      "179 ==>  9907\n",
      "187 ==>  3712\n",
      "193 ==>  9307\n",
      "201 ==>  3712\n",
      "207 ==>  9907\n",
      "215 ==>  3712\n",
      "221 ==>  9738\n",
      "229 ==>  3712\n",
      "235 ==>  9907\n",
      "243 ==>  3712\n",
      "249 ==>  9907\n",
      "257 ==>  3712\n",
      "263 ==>  9907\n",
      "271 ==>  3712\n",
      "277 ==>  9872\n",
      "285 ==>  3712\n",
      "291 ==>  9907\n",
      "299 ==>  3712\n",
      "305 ==>  9907\n",
      "313 ==>  3584\n",
      "319 ==>  9907\n",
      "327 ==>  3328\n",
      "333 ==>  9907\n",
      "341 ==>  3712\n",
      "347 ==>  9387\n",
      "355 ==>  3584\n",
      "361 ==>  9907\n",
      "369 ==>  3712\n",
      "375 ==>  9821\n",
      "383 ==>  3712\n",
      "389 ==>  9907\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa4e7fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:15:31.392538Z",
     "start_time": "2024-05-21T16:15:31.386917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29899aaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:16:51.403684Z",
     "start_time": "2024-05-21T16:15:34.058483Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.502005248\n",
      "Model size after pruning:  13.458235392\n",
      "Model size after pruning:  13.44565248\n",
      "Model size after pruning:  13.395173376\n",
      "Model size after pruning:  13.365813248\n",
      "Model size after pruning:  13.334208512\n",
      "Model size after pruning:  13.317431296\n",
      "Model size after pruning:  13.29037312\n",
      "Model size after pruning:  13.277790208\n",
      "Model size after pruning:  13.240705024\n",
      "Model size after pruning:  13.228122112\n",
      "Model size after pruning:  13.201063936\n",
      "Model size after pruning:  13.188481024\n",
      "Model size after pruning:  13.157834752\n",
      "Model size after pruning:  13.14525184\n",
      "Model size after pruning:  13.118193664\n",
      "Model size after pruning:  13.105610752\n",
      "Model size after pruning:  13.078552576\n",
      "Model size after pruning:  13.065969664\n",
      "Model size after pruning:  13.024165888\n",
      "Model size after pruning:  13.011582976\n",
      "Model size after pruning:  12.9845248\n",
      "Model size after pruning:  12.971941888\n",
      "Model size after pruning:  12.940730368\n",
      "Model size after pruning:  12.928147456\n",
      "Model size after pruning:  12.90108928\n",
      "Model size after pruning:  12.888506368\n",
      "Model size after pruning:  12.861448192\n",
      "Model size after pruning:  12.84886528\n",
      "Model size after pruning:  12.821807104\n",
      "Model size after pruning:  12.809224192\n",
      "Model size after pruning:  12.781305856\n",
      "Model size after pruning:  12.768722944\n",
      "Model size after pruning:  12.741664768\n",
      "Model size after pruning:  12.729081856\n",
      "Model size after pruning:  12.70202368\n",
      "Model size after pruning:  12.685246464\n",
      "Model size after pruning:  12.658188288\n",
      "Model size after pruning:  12.633022464\n",
      "Model size after pruning:  12.605964288\n",
      "Model size after pruning:  12.593381376\n",
      "Model size after pruning:  12.55354368\n",
      "Model size after pruning:  12.536766464\n",
      "Model size after pruning:  12.509708288\n",
      "Model size after pruning:  12.497125376\n",
      "Model size after pruning:  12.467953664\n",
      "Model size after pruning:  12.455370752\n",
      "Model size after pruning:  12.428312576\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53a730f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:16:51.420458Z",
     "start_time": "2024-05-21T16:16:51.406997Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9227, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9227, bias=False)\n",
       "          (down_proj): Linear(in_features=9227, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8954, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8954, bias=False)\n",
       "          (down_proj): Linear(in_features=8954, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9722, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9722, bias=False)\n",
       "          (down_proj): Linear(in_features=9722, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9499, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9499, bias=False)\n",
       "          (down_proj): Linear(in_features=9499, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9761, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9761, bias=False)\n",
       "          (down_proj): Linear(in_features=9761, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11-12): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9307, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9307, bias=False)\n",
       "          (down_proj): Linear(in_features=9307, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9738, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9738, bias=False)\n",
       "          (down_proj): Linear(in_features=9738, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-18): 3 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9872, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9872, bias=False)\n",
       "          (down_proj): Linear(in_features=9872, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20-21): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3328, bias=False)\n",
       "          (o_proj): Linear(in_features=3328, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9387, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9387, bias=False)\n",
       "          (down_proj): Linear(in_features=9387, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9821, bias=False)\n",
       "          (down_proj): Linear(in_features=9821, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
       "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=9907, bias=False)\n",
       "          (down_proj): Linear(in_features=9907, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b391382f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:16:51.429898Z",
     "start_time": "2024-05-21T16:16:51.422877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 29 9227 29 8954 25 9722 28 9907 29 9499 29 9907 29 9761 29 9907 29 9907 29 9307 29 9907 29 9738 29 9907 29 9907 29 9907 29 9872 29 9907 29 9907 28 9907 26 9907 29 9387 28 9907 29 9821 29 9907 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f244a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:24:43.589645Z",
     "start_time": "2024-05-21T16:16:51.432758Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 21.041368214939443 tokens/sec, 657 tokens (including full prompt)\n",
      "Long Context: 46.03661164418913 tokens/sec, 1430 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  41.8347405456093 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b089cba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:24:43.599599Z",
     "start_time": "2024-05-21T16:24:43.592293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6195277824"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2f7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23d3e441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:30:44.181118Z",
     "start_time": "2024-05-21T16:30:44.169982Z"
    }
   },
   "outputs": [],
   "source": [
    "del experiment.pipeline\n",
    "del experiment.model\n",
    "del experiment.tokenizer\n",
    "\n",
    "del experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbca7b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:30:46.705962Z",
     "start_time": "2024-05-21T16:30:46.694867Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "del model\n",
    "del tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e2f29e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:30:49.971217Z",
     "start_time": "2024-05-21T16:30:49.924516Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e693eb0",
   "metadata": {},
   "source": [
    "## Experiment 3 Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d876c1dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:32:08.723741Z",
     "start_time": "2024-05-21T16:32:08.720906Z"
    }
   },
   "outputs": [],
   "source": [
    "use_template = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10f4de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:32:41.544508Z",
     "start_time": "2024-05-21T16:32:09.632015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-21 22:02:10.646639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 22:02:11.722473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad81073446c461884a5ca39b5c59d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_chat_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_chat_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d23890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:32:54.169633Z",
     "start_time": "2024-05-21T16:32:54.165247Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.7_0.65_1.0_chat_2436.json\"\n",
    "save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p\n",
    "\n",
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=use_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61db1c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:32:55.013623Z",
     "start_time": "2024-05-21T16:32:54.939659Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ==>  4096\n",
      "67 ==>  7214\n",
      "75 ==>  4096\n",
      "81 ==>  7155\n",
      "89 ==>  4096\n",
      "95 ==>  7155\n",
      "103 ==>  4096\n",
      "109 ==>  7232\n",
      "117 ==>  4096\n",
      "123 ==>  7272\n",
      "131 ==>  4096\n",
      "137 ==>  7155\n",
      "145 ==>  4096\n",
      "151 ==>  7155\n",
      "159 ==>  4096\n",
      "165 ==>  7155\n",
      "173 ==>  4096\n",
      "179 ==>  7155\n",
      "187 ==>  4096\n",
      "193 ==>  7155\n",
      "201 ==>  3456\n",
      "207 ==>  7155\n",
      "215 ==>  2688\n",
      "221 ==>  7155\n",
      "229 ==>  2688\n",
      "235 ==>  7155\n",
      "243 ==>  2688\n",
      "249 ==>  7155\n",
      "257 ==>  2688\n",
      "263 ==>  7155\n",
      "271 ==>  2688\n",
      "277 ==>  7155\n",
      "285 ==>  2688\n",
      "291 ==>  7155\n",
      "299 ==>  2688\n",
      "305 ==>  7155\n",
      "313 ==>  2688\n",
      "319 ==>  7155\n",
      "327 ==>  2688\n",
      "333 ==>  7155\n",
      "341 ==>  2688\n",
      "347 ==>  7155\n",
      "355 ==>  2688\n",
      "361 ==>  7155\n",
      "369 ==>  2688\n",
      "375 ==>  7155\n",
      "383 ==>  2688\n",
      "389 ==>  7155\n"
     ]
    }
   ],
   "source": [
    "experiment.get_head_inter_pruning_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e18741b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:32:55.924327Z",
     "start_time": "2024-05-21T16:32:55.917683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1408c195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:33:59.869445Z",
     "start_time": "2024-05-21T16:32:58.285291Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.421346816\n",
      "Model size after pruning:  13.421346816\n",
      "Model size after pruning:  13.326655488\n",
      "Model size after pruning:  13.326655488\n",
      "Model size after pruning:  13.23196416\n",
      "Model size after pruning:  13.23196416\n",
      "Model size after pruning:  13.139165184\n",
      "Model size after pruning:  13.139165184\n",
      "Model size after pruning:  13.047349248\n",
      "Model size after pruning:  13.047349248\n",
      "Model size after pruning:  12.95265792\n",
      "Model size after pruning:  12.95265792\n",
      "Model size after pruning:  12.857966592\n",
      "Model size after pruning:  12.857966592\n",
      "Model size after pruning:  12.763275264\n",
      "Model size after pruning:  12.763275264\n",
      "Model size after pruning:  12.668583936\n",
      "Model size after pruning:  12.668583936\n",
      "Model size after pruning:  12.573892608\n",
      "Model size after pruning:  12.552921088\n",
      "Model size after pruning:  12.45822976\n",
      "Model size after pruning:  12.412092416\n",
      "Model size after pruning:  12.317401088\n",
      "Model size after pruning:  12.271263744\n",
      "Model size after pruning:  12.176572416\n",
      "Model size after pruning:  12.130435072\n",
      "Model size after pruning:  12.035743744\n",
      "Model size after pruning:  11.9896064\n",
      "Model size after pruning:  11.894915072\n",
      "Model size after pruning:  11.848777728\n",
      "Model size after pruning:  11.7540864\n",
      "Model size after pruning:  11.707949056\n",
      "Model size after pruning:  11.613257728\n",
      "Model size after pruning:  11.567120384\n",
      "Model size after pruning:  11.472429056\n",
      "Model size after pruning:  11.426291712\n",
      "Model size after pruning:  11.331600384\n",
      "Model size after pruning:  11.28546304\n",
      "Model size after pruning:  11.190771712\n",
      "Model size after pruning:  11.144634368\n",
      "Model size after pruning:  11.04994304\n",
      "Model size after pruning:  11.003805696\n",
      "Model size after pruning:  10.909114368\n",
      "Model size after pruning:  10.862977024\n",
      "Model size after pruning:  10.768285696\n",
      "Model size after pruning:  10.722148352\n",
      "Model size after pruning:  10.627457024\n"
     ]
    }
   ],
   "source": [
    "experiment.prune_model(real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b716b750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:33:59.878611Z",
     "start_time": "2024-05-21T16:33:59.871936Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7214, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7214, bias=False)\n",
       "          (down_proj): Linear(in_features=7214, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5-6): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7232, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7232, bias=False)\n",
       "          (down_proj): Linear(in_features=7232, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7272, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7272, bias=False)\n",
       "          (down_proj): Linear(in_features=7272, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9-13): 5 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
       "          (o_proj): Linear(in_features=3456, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15-27): 13 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=2688, bias=False)\n",
       "          (o_proj): Linear(in_features=2688, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=7155, bias=False)\n",
       "          (down_proj): Linear(in_features=7155, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28-31): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d06e603f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:33:59.901884Z",
     "start_time": "2024-05-21T16:33:59.880418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 11008 32 11008 32 11008 32 11008 32 7214 32 7155 32 7155 32 7232 32 7272 32 7155 32 7155 32 7155 32 7155 32 7155 27 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 21 7155 32 11008 32 11008 32 11008 32 11008 "
     ]
    }
   ],
   "source": [
    "for mod in model.modules():\n",
    "    if isinstance(mod, LlamaAttention):\n",
    "        print(mod.num_heads, end=\" \")\n",
    "    if isinstance(mod, LlamaMLP):\n",
    "        print(mod.intermediate_size, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd7ab653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:40:44.308112Z",
     "start_time": "2024-05-21T16:33:59.904638Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 24.332392375131185 tokens/sec, 362 tokens (including full prompt)\n",
      "Long Context: 131.0646482112222 tokens/sec, 1034 tokens (including full prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  24.023934894110646 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "experiment.get_throughput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d73631f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:40:44.339180Z",
     "start_time": "2024-05-21T16:40:44.315516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5294850048"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd2778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f956ba90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:43:51.367693Z",
     "start_time": "2024-05-21T16:43:46.172017Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mexperiment\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mtokenizer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "del experiment.pipeline\n",
    "del experiment.model\n",
    "del experiment.tokenizer\n",
    "\n",
    "del experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b628170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:44:12.231999Z",
     "start_time": "2024-05-21T16:44:11.834227Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6219621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:40:44.416832Z",
     "start_time": "2024-05-21T16:40:44.372816Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "del model\n",
    "del tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f4eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:44:53.788284Z",
     "start_time": "2024-05-21T14:44:53.788268Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33a888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f09d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:44:53.789522Z",
     "start_time": "2024-05-21T14:44:53.789506Z"
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12eff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8ab901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:59:15.882239Z",
     "start_time": "2024-05-21T16:59:15.869428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857707730920704"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5294850048/6738415616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab576443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b8e955",
   "metadata": {},
   "source": [
    "# Experiment (Extreme Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a6638a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:31:27.009938Z",
     "start_time": "2024-05-22T15:31:27.006432Z"
    }
   },
   "outputs": [],
   "source": [
    "pruning_dict = \"pruning_dict_0.6_0.5_1.0_chat_2521.json\"\n",
    "    save_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\"\n",
    "\n",
    "p = pruning_dict.split(\"_\")[-1].split(\".\")[-2]\n",
    "output_dir = \"/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints//\" + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b19be0d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:32:01.029000Z",
     "start_time": "2024-05-22T15:31:32.904772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "2024-05-22 21:01:34.250160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 21:01:35.287199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b35c0f37bc479f8287625b5e37dd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_llama_path, padding_side=\"left\", padding=True, truncation=True)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_llama_path,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "#     revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = pipeline.model\n",
    "model.seqlen = model.config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb8b944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:32:10.088636Z",
     "start_time": "2024-05-22T15:32:10.084727Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = ExperimentRunner(pipeline, tokenizer, pruning_dict, \n",
    "                 save_dir=save_dir, output_dir=output_dir, ignored_layers=None, \n",
    "                 use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69594226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:56:34.316149Z",
     "start_time": "2024-05-22T15:32:12.902401Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Running Experiment   ***************\n",
      "61 ==>  3584\n",
      "67 ==>  5504\n",
      "75 ==>  3584\n",
      "81 ==>  5504\n",
      "89 ==>  3072\n",
      "95 ==>  8965\n",
      "103 ==>  2944\n",
      "109 ==>  5504\n",
      "117 ==>  3968\n",
      "123 ==>  6234\n",
      "131 ==>  3840\n",
      "137 ==>  5504\n",
      "145 ==>  3200\n",
      "151 ==>  5504\n",
      "159 ==>  3968\n",
      "165 ==>  5504\n",
      "173 ==>  3840\n",
      "179 ==>  5504\n",
      "187 ==>  3200\n",
      "193 ==>  5504\n",
      "201 ==>  3584\n",
      "207 ==>  5504\n",
      "215 ==>  2432\n",
      "221 ==>  6185\n",
      "229 ==>  3968\n",
      "235 ==>  5511\n",
      "243 ==>  3968\n",
      "249 ==>  5504\n",
      "257 ==>  3840\n",
      "263 ==>  5504\n",
      "271 ==>  3712\n",
      "277 ==>  5504\n",
      "285 ==>  3456\n",
      "291 ==>  5504\n",
      "299 ==>  3072\n",
      "305 ==>  5504\n",
      "313 ==>  2048\n",
      "319 ==>  5504\n",
      "327 ==>  2048\n",
      "333 ==>  5504\n",
      "341 ==>  2048\n",
      "347 ==>  5504\n",
      "355 ==>  2048\n",
      "361 ==>  5504\n",
      "369 ==>  2048\n",
      "375 ==>  5504\n",
      "383 ==>  2048\n",
      "389 ==>  5504\n",
      "***************   Pruning Model   ***************\n",
      "Pruning model\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Model size after pruning:  13.51458816\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "***************   Model Pruned Successfully   ***************\n",
      "Model Size after Pruning:  13.51458816\n",
      "evaluating on wikitext2\n",
      "nsamples 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   0%|                                                                                                                                                                                                                                                                                                        | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   4%|███████████▌                                                                                                                                                                                                                                                                                    | 1/25 [00:00<00:16,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:   8%|███████████████████████                                                                                                                                                                                                                                                                         | 2/25 [00:01<00:16,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  12%|██████████████████████████████████▌                                                                                                                                                                                                                                                             | 3/25 [00:02<00:16,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  16%|██████████████████████████████████████████████                                                                                                                                                                                                                                                  | 4/25 [00:02<00:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  20%|█████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                      | 5/25 [00:03<00:14,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  24%|█████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                           | 6/25 [00:04<00:13,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  28%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 7/25 [00:04<00:12,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  32%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                   | 8/25 [00:05<00:11,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  36%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                        | 9/25 [00:06<00:10,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 10/25 [00:06<00:10,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  44%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 11/25 [00:07<00:09,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 12/25 [00:08<00:08,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                         | 13/25 [00:08<00:08,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                              | 14/25 [00:09<00:07,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 15/25 [00:10<00:06,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                       | 16/25 [00:11<00:06,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 17/25 [00:11<00:05,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 18/25 [00:12<00:04,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 19/25 [00:13<00:04,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 20/25 [00:13<00:03,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 21/25 [00:14<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 22/25 [00:15<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 23/25 [00:15<00:01,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WikiText Validation:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 24/25 [00:16<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WikiText Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:17<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL:  35.08295440673828\n",
      "Perplexity on wikitext2:  35.08295440673828\n",
      "Loading checkpoint from /data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model passed to evaluation:  LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22:21:04:03,711 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:21:04:03,714 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-22:21:04:10,409 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:21:04:10,416 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-22:21:05:32,665 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-22:21:05:32,666 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': '/data/home/milinbhade/Milin/AMC/bertamc_v4/llama_checkpoints/'}\n",
      "2024-05-22:21:05:32,680 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-22:21:05:32,680 INFO     [huggingface.py:163] Using device 'cuda'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610c9f0a55d34dc0a9ebe363ff613c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22:21:05:48,989 WARNING  [task.py:763] [Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:21:05:48,991 WARNING  [task.py:775] [Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-22:21:05:59,312 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-05-22:21:05:59,313 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-05-22:21:06:41,332 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-05-22:21:06:41,333 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2024-05-22:21:06:41,334 WARNING  [evaluator.py:239] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-05-22:21:06:41,335 WARNING  [evaluator.py:239] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-05-22:21:06:41,335 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-05-22:21:06:41,336 WARNING  [evaluator.py:239] Overwriting default num_fewshot of openbookqa from None to 0\n",
      "2024-05-22:21:06:41,336 WARNING  [evaluator.py:239] Overwriting default num_fewshot of rte from None to 0\n",
      "2024-05-22:21:06:41,341 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1958.31it/s]\n",
      "2024-05-22:21:06:41,417 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 787.02it/s]\n",
      "2024-05-22:21:06:41,557 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 56549.87it/s]\n",
      "2024-05-22:21:06:41,567 INFO     [task.py:395] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 857.84it/s]\n",
      "2024-05-22:21:06:41,701 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1488.76it/s]\n",
      "2024-05-22:21:06:41,777 INFO     [task.py:395] Building contexts for openbookqa on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1653.92it/s]\n",
      "2024-05-22:21:06:41,850 INFO     [task.py:395] Building contexts for rte on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1521.42it/s]\n",
      "2024-05-22:21:06:41,925 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2199/2199 [02:08<00:00, 17.16it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|     Alias     | Acc, None |   Acc StdErr, None   | Acc Norm, None | Acc Norm StdErr, None |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "|   hellaswag   |    0.36   | 0.04824181513244218  |      0.49      |  0.05024183937956912  |\n",
      "|    arc_easy   |    0.41   | 0.049431107042371025 |      0.36      |  0.04824181513244218  |\n",
      "|   winogrande  |    0.55   |         0.05         |      None      |          None         |\n",
      "| arc_challenge |    0.21   | 0.040936018074033256 |      0.31      |  0.04648231987117316  |\n",
      "|     boolq     |    0.45   |         0.05         |      None      |          None         |\n",
      "|   openbookqa  |    0.14   | 0.03487350880197769  |      0.29      |  0.045604802157206845 |\n",
      "|      rte      |    0.57   | 0.04975698519562428  |      None      |          None         |\n",
      "+---------------+-----------+----------------------+----------------+-----------------------+\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91m grow and flourish.\n",
      "The benefits of gardening are well known and dating the girl of aids to be by to the is a common. The the girl. the is a common term for the girl of a-'the girl of a- tag- tagged- tag- tagged- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag- tag\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Gardening provides a sense of accomplishment and tranquility, as you nurture plants and watch them \u001b[0m\u001b[91mgrow and flourish. The benefits of gardening are well known and dating the girl of aids to be by to the is a common. The the girl.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91m the development of social skills and emotions, allowing children to to learn to be more effective and happy.\n",
      "Children are not as different from the way we think we are, and we are not more well-off in the world, in the time, in the way, in the sense, in the sense of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way of being, in the way, in the way, in the way, in the way, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Family traditions are passed down from generation to generation, creating a sense of continuity and belonging. Whether it's holiday celebrations, special recipes, or annual gatherings, these traditions foster \u001b[0m\u001b[91mthe development of social skills and emotions, allowing children to to learn to be more effective and happy.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will be a form of art, a form of expression, a form of fashion. In the future, fashion will be a form of expression, a form of fashion. In the future, fashion will be a form of fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a fashion. fashion will be a. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion. fashion.. fashion. fashion..\n",
      ".\n",
      ".\n",
      "..\n",
      ".....................................................................................................................................................................................................................................................................\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The evolution of fashion reflects changes in society and culture throughout history. From the elaborate garments of the Victorian era to the minimalist styles of the 21st century, fashion is a form of self-expression that adapts to the times. Iconic designers like Coco Chanel, Alexander McQueen, and Virgil Abloh have challenged conventions and introduced new aesthetics. Fashion is also influenced by technological advancements, such as the development of sustainable materials and the rise of digital fashion shows. As we move towards a more inclusive and diverse industry, fashion will continue to celebrate individuality and innovation, inspiring people to express their unique identities through their clothing choices. In the future \u001b[0m\u001b[91m, fashion will be a form of art, a form of expression, a form of fashion. In the future, fashion will be a form of expression, a form of fashion. In the future, fashion will be a form of fashion. fashion will be a fashion. fashion will be a. fashion.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91m doctors to make smarter dec sierp.\n",
      "3 4 5 6 50 10 101 102 11 110 1103 12 13 14 15 16 17 18 19 - 19.20 - 20 - 201 20.25 - 3.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.va.a.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.aa.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad.ad\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Artificial intelligence is transforming the healthcare industry by enabling \u001b[0m\u001b[91mdoctors to make smarter dec sierp. 3 4 5 6 50 10 101 102 11 110 1103 12 13 14 15 16 17 18 19 - 19. 20 - 20 - 201 20. 25 - 3. 4. va. a. aa.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91m help to provide a more accurate and realistic view of the market. This is a valuable quality that investors can expect to receive from the companies they are investing in.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> _x</10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Machine learning algorithms are increasingly being used to predict stock market trends. By analyzing historical data and identifying patterns, these models can \u001b[0m\u001b[91mhelp to provide a more accurate and realistic view of the market. This is a valuable quality that investors can expect to receive from the companies they are investing in.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91m they will continue to have a significant impact on the field of human-computer interaction.\n",
      "The Human-Computer Interaction (HCI) is a field of study that is rapidly growing in popularity. This is largely because of the work that has been done in the area by students and professors around the world. These students and professors have been working to understand the nature of human-computer interaction and how to improve on it. This has led to the development of a number of theories that have been used to explain the nature of the behavior of humans. These theories have also led to the development of a number of techniques that have been used to improve on the ability of humans to communicate. These techniques have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of theories that have been used to improve on the ability of the hand to communicate with the brain. This has also led to the development of a number of techniques that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of techniques that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of technologies that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The integration of natural language processing with machine learning has opened up new possibilities in human-computer interaction. For instance, chatbots powered by sophisticated NLP algorithms can now understand and respond to user queries with remarkable accuracy. As these technologies continue to evolve, we can expect that \u001b[0m\u001b[91mthey will continue to have a significant impact on the field of human-computer interaction. The Human-Computer Interaction (HCI) is a field of study that is rapidly growing in popularity. This is largely because of the work that has been done in the area by students and professors around the world. These students and professors have been working to understand the nature of human-computer interaction and how to improve on it. This has led to the development of a number of theories that have been used to explain the nature of the behavior of humans. These theories have also led to the development of a number of techniques that have been used to improve on the ability of humans to communicate. These techniques have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of devices that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of theories that have been used to improve on the ability of the hand to communicate with the brain. This has also led to the development of a number of techniques that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of techniques that have been used to improve on the ability of the hand to communicate with the brain. This has led to the development of a number of technologies that have been used to improve on the ability of the hand to communicate with the brain.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91m through the world.\n",
      "The best way to think about it is to think about it as a game.\n",
      "The best way to think about it is to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The best way to think about it as a game. The.text.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.the.\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Self-driving cars rely heavily on sensor data and advanced algorithms to navigate \u001b[0m\u001b[91mthrough the world. The best way to think about it is to think about it as a game. The best way to think about it as a game. The. text. the.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91m be used as a tool to make the world a better place.\n",
      "\n",
      "\n",
      "{{#edit on:\n",
      "question id={}}\n",
      "  \\section{**/on:\n",
      "question}\n",
      "\n",
      "    \\beginycodeft;l;n;n;n;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m One of the biggest challenges in developing AI systems is ensuring they are ethical and unbiased. Researchers are working on creating frameworks that can \u001b[0m\u001b[91mbe used as a tool to make the world a better place.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91m can do the job of a human. This will have a significant impact on the medical industry.\n",
      "The Medical Apps are a large and growing industry. The Medical Apps are a large and growing industry. The Medical Apps are a growing industry. The Medical Apps are a.\n",
      "The Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy. Healthy.\n",
      "Healthy. Healthy.\n",
      "Healthy. Healthy.\n",
      "Healthy. Healthy.\n",
      "Healthy. Healthy.\n",
      "H. and.\n",
      "\n",
      "H. and.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m In the field of computer vision, convolutional neural networks have revolutionized the way machines perceive visual information. By mimicking the human visual system, these networks can classify images, detect objects, and even perform image segmentation. This has significant applications in areas such as medical imaging, where AI can assist doctors in diagnosing conditions from X-rays and MRIs. As the technology advances, we may soon see AI systems that \u001b[0m\u001b[91mcan do the job of a human. This will have a significant impact on the medical industry. The Medical Apps are a large and growing industry. The Medical Apps are a growing industry. The Medical Apps are a. The Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy Healthy. Healthy. H.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91m machine learning algorithms.\n",
      "The algorithm is a method of solving a problem. The algorithm is a method of solving a problem. The algorithm is a method of solving a problem. The algorithm is a method of solving a problem. The algorithm is a method of solving a problem. The algorithm is a method of solving a problem. The algorithm is a method of solving a method. The method is a method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method.\n",
      "The. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method..\n",
      "The. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method. method.\n",
      "The. method. method. method. method. method. method. method. method. method. method. method. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Reinforcement learning algorithms learn optimal behaviors through \u001b[0m\u001b[91mmachine learning algorithms. The algorithm is a method of solving a problem. The algorithm is a method of solving a method. The method is a method. method. The.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91m be used to explain the complexity of a cryptographic algorithm. This could be a problem for the algorithm. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able to solve the problem. This is a problem that the algorithm has to be able\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Quantum computing is poised to revolutionize the field of cryptography. With the ability to solve complex problems much faster than classical computers, quantum algorithms could \u001b[0m\u001b[91mbe used to explain the complexity of a cryptographic algorithm. This could be a problem for the algorithm. This is a problem that the algorithm has to be able to solve the problem.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91m will be able to detect and prevent the spread of the virus. This will require a significant amount of computing power and energy. However, the benefits of this technology are likely to be worth the cost of the increase in electricity.\n",
      "10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/10.1002/1\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The rise of big data has necessitated the development of new machine learning techniques capable of handling vast amounts of information. Techniques such as distributed learning and federated learning have emerged to process data across multiple devices without compromising privacy. These methods enable organizations to leverage large datasets for training robust models while ensuring data security. As we look to the future, the continued evolution of these techniques will likely lead to even more powerful and secure AI systems that \u001b[0m\u001b[91mwill be able to detect and prevent the spread of the virus. This will require a significant amount of computing power and energy. However, the benefits of this technology are likely to be worth the cost of the increase in electricity. 10. 1002/10.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91m reducing the number of parameters.\n",
      "2) Deep-learning is a form of machine learning that is a form of a model is a model is a model is a model is a model is a model is a model is a model is a model is a model is a model is a method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method method caercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercercer\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks by \u001b[0m\u001b[91mreducing the number of parameters.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91m improve the performance of the system. This article will discuss the role of XAI in the field of computer science and how the field is changing to better serve the needs of the world.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        (10 000 1  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15  16  17  18  19  19:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-new-\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Explainable AI (XAI) is becoming increasingly important as AI systems are deployed in critical areas. By providing insights into how models make decisions, XAI aims to \u001b[0m\u001b[91mimprove the performance of the system. This article will discuss the role of XAI in the field of computer science and how the field is changing to better serve the needs of the world.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91m the clouds are the last to disappear.\n",
      "The sun is a star in the sky, and it is the sun that makes the sky. The sky is a sky, and it is a sky that is a sky. The sky is a sky, and it is a sky, and it is a sky, and it is a sky, and it is a sky, and it is a sky, and it is a sky, and it is a sky, and it is a, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and{ and{ and{ and{ and{ and{ and{ and{ and{ and{ and{ and{ and{ and\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m The beauty of a sunset over the ocean is unmatched, as the sky turns shades of orange and pink, and \u001b[0m\u001b[91mthe clouds are the last to disappear. The sun is a star in the sky, and it is the sun that makes the sky. The sky is a sky, and it is a sky that is a sky.\u001b[0m\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Original: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91m the world and allow you to make a decision that is right for you.\n",
      "The best way to make a decision on a travel is to do your research and to explore the destination you are travel to. You can do your research by searching the web for reviews, travel tips, and photos of the destination you are travel to. You can also explore the web for travel tips and photos by clicking on the link to the travel site you are trying to find out about. You can also explore the web by clicking on the link to the travel site you are trying to find out about. You can also explore the web by clicking on the link to the travel site you are trying to find out about and. You can also explore the world and make a decision that is right for you by clicking on the link to the location you are trying to find out about. You can also click on the link to the location you are trying to find out about and you can make a decision that is right for you. You can also make a decision by clicking on the link to the destination you are trying to find out about and you can make a decision by clicking on the link to the destination you are trying to find out about. You can also make a decision by clicking on the link to the destination you are trying to find out about. You can also make a decision by clicking on the link to the destination you are trying to find out about. You can also make a decision by clicking on the link to the destination you are trying to find out about. You can also make a decision by clicking on the link to the destination you are trying to find out. You can also make a decision by clicking on the link to the destination you are trying to find out. You can also make a decision by clicking on the link to the destination you are trying to find out. You can also make a decision by clicking on the link to the destination you are trying to find out. You can also make a decision by clicking on the link to the destination you are trying to finding out. You can also make a decision by clicking on the link to the destination you are trying to finding out. You can also make a decision by clicking on the link to the destination you are trying to finding out. You can also make a decision by clicking on the link to the destination you are trying to finding out. You can also make a decision by clicking on the link to the destination you are trying to finding out. You can also make a decision by clicking on the link to the\u001b[0m\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Cleaned: \n",
      "Prompt:  \u001b[1m Traveling to new countries allows you to experience diverse cultures and traditions. Walking through the bustling markets, tasting local delicacies, and interacting with people can provide a deeper understanding of \u001b[0m\u001b[91mthe world and allow you to make a decision that is right for you. The best way to make a decision on a travel is to do your research and to explore the destination you are travel to. You can do your research by searching the web for reviews, travel tips, and photos of the destination you are travel to. You can also explore the web for travel tips and photos by clicking on the link to the travel site you are trying to find out about. You can also explore the web by clicking on the link to the travel site you are trying to find out about. You can also explore the web by clicking on the link to the travel site you are trying to find out about and. You can also explore the world and make a decision that is right for you by clicking on the link to the location you are trying to find out about. You can also click on the link to the location you are trying to find out about and you can make a decision that is right for you. You can also make a decision by clicking on the link to the destination you are trying to find out about and you can make a decision by clicking on the link to the destination you are trying to find out about. You can also make a decision by clicking on the link to the destination you are trying to find out about. You can also make a decision by clicking on the link to the destination you are trying to find out. You can also make a decision by clicking on the link to the destination you are trying to finding out.\u001b[0m\n",
      "\n",
      "\n",
      "Pruning model\n",
      "Model size after pruning:  13.497810944\n",
      "Model size after pruning:  13.36254464\n",
      "Model size after pruning:  13.345767424\n",
      "Model size after pruning:  13.21050112\n",
      "Model size after pruning:  13.176946688\n",
      "Model size after pruning:  13.12673792\n",
      "Model size after pruning:  13.088989184\n",
      "Model size after pruning:  12.95372288\n",
      "Model size after pruning:  12.949528576\n",
      "Model size after pruning:  12.832202752\n",
      "Model size after pruning:  12.823814144\n",
      "Model size after pruning:  12.68854784\n",
      "Model size after pruning:  12.659187712\n",
      "Model size after pruning:  12.523921408\n",
      "Model size after pruning:  12.519727104\n",
      "Model size after pruning:  12.3844608\n",
      "Model size after pruning:  12.376072192\n",
      "Model size after pruning:  12.240805888\n",
      "Model size after pruning:  12.21144576\n",
      "Model size after pruning:  12.076179456\n",
      "Model size after pruning:  12.05940224\n",
      "Model size after pruning:  11.924135936\n",
      "Model size after pruning:  11.869609984\n",
      "Model size after pruning:  11.751079936\n",
      "Model size after pruning:  11.746885632\n",
      "Model size after pruning:  11.61179136\n",
      "Model size after pruning:  11.607597056\n",
      "Model size after pruning:  11.472330752\n",
      "Model size after pruning:  11.463942144\n",
      "Model size after pruning:  11.32867584\n",
      "Model size after pruning:  11.316092928\n",
      "Model size after pruning:  11.180826624\n",
      "Model size after pruning:  11.159855104\n",
      "Model size after pruning:  11.0245888\n",
      "Model size after pruning:  10.991034368\n",
      "Model size after pruning:  10.855768064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning:  10.7886592\n",
      "Model size after pruning:  10.653392896\n",
      "Model size after pruning:  10.586284032\n",
      "Model size after pruning:  10.451017728\n",
      "Model size after pruning:  10.383908864\n",
      "Model size after pruning:  10.24864256\n",
      "Model size after pruning:  10.181533696\n",
      "Model size after pruning:  10.046267392\n",
      "Model size after pruning:  9.979158528\n",
      "Model size after pruning:  9.843892224\n",
      "Model size after pruning:  9.77678336\n",
      "Model size after pruning:  9.641517056\n",
      "Saving model after pruning to checkpoint dir\n",
      "Model saved\n",
      "Real Pruned Model\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4-5): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=8965, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=8965, bias=False)\n",
      "          (down_proj): Linear(in_features=8965, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=2944, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=2944, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=2944, bias=False)\n",
      "          (o_proj): Linear(in_features=2944, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (o_proj): Linear(in_features=3968, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=6234, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=6234, bias=False)\n",
      "          (down_proj): Linear(in_features=6234, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (o_proj): Linear(in_features=3200, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (o_proj): Linear(in_features=3968, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3200, bias=False)\n",
      "          (o_proj): Linear(in_features=3200, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (o_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=2432, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=2432, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=2432, bias=False)\n",
      "          (o_proj): Linear(in_features=2432, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=6185, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=6185, bias=False)\n",
      "          (down_proj): Linear(in_features=6185, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (o_proj): Linear(in_features=3968, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5511, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5511, bias=False)\n",
      "          (down_proj): Linear(in_features=5511, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3968, bias=False)\n",
      "          (o_proj): Linear(in_features=3968, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
      "          (o_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3712, bias=False)\n",
      "          (o_proj): Linear(in_features=3712, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3456, bias=False)\n",
      "          (o_proj): Linear(in_features=3456, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22-27): 6 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28-31): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Real Pruned Model Size\n",
      "9.641517056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Context: 17.638755474443506 tokens/sec, 134 tokens (including full prompt)\n",
      "Long Context: 187.47731065531286 tokens/sec, 846 tokens (including full prompt)\n",
      "Average:  17.725220791834328 tokens/sec\n",
      "***************   Experiment completed successfully Successfully   ***************\n"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0ef6a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:56:34.329325Z",
     "start_time": "2024-05-22T15:56:34.321676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4801880064"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2056d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T16:05:09.999711Z",
     "start_time": "2024-05-22T16:05:09.994799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.287387371506412"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 4801880064/6738415616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1829aa90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T16:03:57.938903Z",
     "start_time": "2024-05-22T16:03:57.931054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7135139135583185"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9.641/13.512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae0df2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T16:08:15.608194Z",
     "start_time": "2024-05-22T16:08:15.603679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271aaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "1068px",
    "width": "378px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "730.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
