{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d80b0f",
   "metadata": {},
   "source": [
    "# Download Sample RedPajama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44953785",
   "metadata": {},
   "source": [
    "togethercomputer/RedPajama-Data-1T-Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea4fbd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:42:40.547786Z",
     "start_time": "2024-05-29T04:42:40.533937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/home/milinbhade/Milin/AMC/bertamc_v4/finetune'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06491ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T11:47:37.777698Z",
     "start_time": "2024-05-27T11:47:29.872314Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir='./redpajama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4939cf2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:47:07.559107Z",
     "start_time": "2024-05-29T04:47:03.643285Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"yahma/alpaca-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34479492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:47:26.736804Z",
     "start_time": "2024-05-29T04:47:26.726206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ecc2db9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:51:56.044905Z",
     "start_time": "2024-05-29T04:47:54.207447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50939ba0a330420ba63173a5eab0344b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/93.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d877d38763144ae815e282f46953b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e50a7921b9f4cf0a9ab746ef37401b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/866M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feca14ada75945728ab2977e20e599d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/689M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7ffe6009546e58045100702ae621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/836M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0eddb077d4940a888cfbb60efc68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/802M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c907d2fa954ff0878cca6ae650ecdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/737M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03405b989ec84e0da41612f423564bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/857M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7447acc7494bd48cf5f528e9bca608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/222M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a5bba99ab44b5d99c4cca0b0c36c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/80.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe948b6cab91471f89f88ff5b11e4f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/119M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc848d2ac30c4032b25a841b05d51034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir='./redpajama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d38fc76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:53:28.579705Z",
     "start_time": "2024-05-29T04:53:20.984915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\\\section{Introduction}\\n\\\\label{sec:intro}\\n\\n\\\\emph{Gender diversity}, or more often its lack thereof, among participants to\\nsoftware development activities has been thoroughly studied in recent years. In\\nparticular, the presence of, effects of, and countermeasures for \\\\emph{gender\\n  bias} in Free/Open Source Software (FOSS) have received a lot of attention\\nover the past decade~\\\\cite{david2008fossdevs, qiu2010kdewomen,\\n  nafus2012patches, kuechler2012genderfoss, vasilescu2014gender,\\n  oneil2016debiansurvey, robles2016womeninfoss, terrell2017gender,\\n  zacchiroli2021gender}.  \\\\emph{Geographic diversity} is on the other hand the\\nkind of diversity that stems from participants in some global activity coming\\nfrom different world regions and cultures.\\n\\nGeographic diversity in FOSS has received relatively little attention in scholarly\\nworks. In particular, while seminal survey-based and\\npoint-in-time medium-scale studies of the geographic origins of FOSS\\ncontributors exist~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  barahona2008geodiversity, takhteyev2010ossgeography, robles2014surveydataset,\\n  wachs2021ossgeography}, large-scale longitudinal studies of the geographic\\norigin of FOSS contributors are still lacking. Such a quantitative\\ncharacterization would be useful to inform decisions related to global\\ndevelopment teams~\\\\cite{herbsleb2007globalsweng} and hiring strategies in the\\ninformation technology (IT) market, as well as contribute factual information\\nto the debates on the economic impact and sociology of FOSS around the world.\\n\\n\\n\\\\paragraph{Contributions}\\n\\nWith this work we contribute to close this gap by conducting \\\\textbf{the first\\n  longitudinal study of the geographic origin of contributors to public code\\n  over 50 years.} Specifically, we provide a preliminary answer to the\\nfollowing research question:\\n\\\\begin{researchquestion}\\n  From which world regions do authors of publicly available commits come from\\n  and how has it changed over the past 50 years?\\n  \\\\label{rq:geodiversity}\\n\\\\end{researchquestion}\\nWe use as dataset the \\\\SWH/ archive~\\\\cite{swhipres2017} and analyze from it\\n2.2 billion\\\\xspace commits archived from 160 million\\\\xspace projects and authored by\\n43 million\\\\xspace authors during the 1971--2021 time period. \\nWe geolocate developers to\\n\\\\DATAWorldRegions/ world regions, using as signals email country code top-level domains (ccTLDs) and \\nauthor (first/last) names compared with name distributions around the world, and UTC offsets \\nmined from commit metadata.\\n\\nWe find evidence of the early dominance of North America in open source\\nsoftware, later joined by Europe. After that period, the geographic diversity \\nin public code has been constantly increasing.\\nWe also identify relevant historical shifts\\nrelated to the end of the UNIX wars and the increase of coding literacy in\\nCentral and South Asia, as well as of broader phenomena like colonialism and\\npeople movement across countries (immigration/emigration).\\n\\n\\n\\n\\n\\\\paragraph{Data availability.}\\n\\nA replication package for this paper is available from Zenodo at\\n\\\\url{https://doi.org/10.5281/zenodo.6390355}~\\\\cite{replication-package}.\\n\\n\\n \\\\section{Related Work}\\n\\\\label{sec:related}\\n\\nBoth early and recent works~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  robles2014surveydataset, oneil2016debiansurvey} have characterized the\\ngeography of Free/Open Source Software (FOSS) using \\\\emph{developer surveys},\\nwhich provide high-quality answers but are limited in size (2-5\\\\,K developers)\\nand can be biased by participant sampling.\\n\\nIn 2008 Barahona et al.~\\\\cite{barahona2008geodiversity} conducted a seminal\\nlarge-scale (for the time) study on FOSS \\\\emph{geography using mining software\\n  repositories (MSR) techniques}. They analyzed the origin of 1\\\\,M contributors\\nusing the SourceForge user database and mailing list archives over the\\n1999--2005 period, using as signals information similar to ours: email domains\\nand UTC offsets. \\nThe studied period (7 years) in~\\\\cite{barahona2008geodiversity} is shorter than \\nwhat is studied in the present paper (50 years) and the data sources are \\nlargely different; with that in mind, our results show a slightly larger quote of \\nEuropean v.~North American contributions.\\n\\nAnother empirical work from 2010 by Takhteyev and\\nHilts~\\\\cite{takhteyev2010ossgeography} harvested self-declared geographic\\nlocations of GitHub accounts recursively following their connections,\\ncollecting information for $\\\\approx$\\\\,70\\\\,K GitHub users.  A very recent\\nwork~\\\\cite{wachs2021ossgeography} by Wachs et al.~has geolocated half a million\\nGitHub users, having contributed at least 100 commits each, and who\\nself-declare locations on their GitHub profiles. While the study is\\npoint-in-time as of 2021, the authors compare their findings\\nagainst~\\\\cite{barahona2008geodiversity, takhteyev2010ossgeography} to\\ncharacterize the evolution of FOSS geography over the time snapshots taken by\\nthe three studies.\\n\\nCompared with previous empirical works, our study is much larger scale---having\\nanalyzed 43 million\\\\xspace authors of 2.2 billion\\\\xspace commits from 160 million\\\\xspace\\nprojects---longitudinal over 50 years of public code contributions rather than\\npoint in time, and also more fine-grained (with year-by-year granularity over\\nthe observed period). Methodologically, our study relies on Version Control\\nSystem (VCS) commit data rather than platform-declared location information.\\n\\n\\nOther works---in particular the work by Daniel~\\\\cite{daniel2013ossdiversity}\\nand, more recently, Rastogi et al.~\\\\cite{rastogi2016geobias,\\n  rastogi2018geobias, prana2021geogenderdiversity}---have studied geographic\\n\\\\emph{diversity and bias}, i.e., the extent to which the origin of FOSS\\ndevelopers affect their collaborative coding activities.\\nIn this work we characterized geographic diversity in public code for the first\\ntime at this scale, both in terms of contributors and observation period. We do\\nnot tackle the bias angle, but provide empirical data and findings that can be\\nleveraged to that end as future work.\\n\\n\\\\emph{Global software engineering}~\\\\cite{herbsleb2007globalsweng} is the\\nsub-field of software engineering that has analyzed the challenges of scaling\\ndeveloper collaboration globally, including the specific concern of how to deal\\nwith geographic diversity~\\\\cite{holmstrom2006globaldev, fraser2014eastwest}.\\nDecades later the present study provides evidence that can be used, in the\\nspecific case of public code and at a very large scale, to verify which\\npromises of global software engineering have borne fruit.\\n\\n\\n\\n\\n\\n\\n \\\\section{Methodology}\\n\\\\label{sec:method}\\n\\n\\n\\\\newif\\\\ifgrowthfig  \\\\growthfigtrue\\n\\\\ifgrowthfig\\n\\\\begin{figure}\\n  \\\\includegraphics[width=\\\\columnwidth]{yearly-commits}\\n  \\\\caption{Yearly public commits over time (log scale).\\n}\\n  \\\\label{fig:growth}\\n\\\\end{figure}\\n\\\\fi\\n\\n\\\\paragraph{Dataset}\\n\\nWe retrieved from \\\\SWH/~\\\\cite{swh-msr2019-dataset} all commits archived until \\\\DATALastCommitDate/.\\nThey amount to \\\\DATACommitsRaw/ commits, unique by SHA1 identifier, harvested from \\\\DATATotalCommitsInSH/ public projects coming from major development forges (GitHub, GitLab, etc.) and package repositories (Debian, PyPI, NPM, etc.).\\nCommits in the dataset are by \\\\DATAAuthorsRaw/ authors, unique by $\\\\langle$name, email$\\\\rangle$ pairs.\\nThe dataset came as two relational tables, one for commits and one for authors, with the former referencing the latter via a foreign key.\\n\\\\iflong\\nEach row in the commit table contains the following fields: commit SHA1 identifier, author and committer timestamps, author and committer identifiers (referencing the author table).\\nThe distinction between commit authors and committers come from Git, which allows to commit a change authored by someone else.\\nFor this study we focused on authors and ignored committers, as the difference between the two is not relevant for our research questions and the amount of commits with a committer other than its author is negligible.\\n\\\\fi\\nFor each entry in the author table we have author full name and email as two separate strings of raw bytes.\\n\\nWe removed implausible or unusable names that: are not decodable as UTF-8 (\\\\DATAAuthorsRmNondecodable/ author names removed), are email addresses instead of names (\\\\DATAAuthorsRmEmail/ ``names''), consist of only blank characters (\\\\DATAAuthorsRmBlank/), contain more than 10\\\\% non-letters (\\\\DATAAuthorsRmNonletter/), are longer than 100 characters (\\\\DATAAuthorsRmToolong/).\\nAfter filtering, about \\\\DATAAuthorsPlausibleApprox/ authors (\\\\DATAAuthorsPlausiblePct/ of the initial dataset) remained for further analysis.\\n\\nNote that the amount of public code commits (and authors) contained in the\\ninitial dataset grows exponentially over\\ntime~\\\\cite{swh-provenance-emse}\\\\ifgrowthfig, as shown for commits in\\n\\\\Cref{fig:growth}\\\\else: from $10^4$ commits in 1971, to $10^6$ in 1998, to\\nalmost $10^9$ in 2020\\\\fi. As a consequence the observed trends tend to be more\\nstable in recent decades than in 40+ year-old ones, due to statistics taken on\\nexponentially larger populations.\\n\\n\\n\\\\paragraph{Geolocation}\\n\\n\\\\begin{figure}\\n  \\\\centering\\n  \\\\includegraphics[clip,trim=6cm 6cm 0 0,width=\\\\linewidth]{subregions-ours}\\n  \\\\caption{The \\\\DATAWorldRegions/ world regions used as geolocation targets.}\\n  \\\\label{fig:worldmap}\\n\\\\end{figure}\\n\\nAs geolocation targets we use macro world regions derived from the United Nations geoscheme~\\\\cite{un1999geoscheme}.\\nTo avoid domination by large countries (e.g., China or Russia) within macro regions, we merged and split some regions based on geographic proximity and the sharing of preeminent cultural identification features, such as spoken language.\\n\\\\Cref{fig:worldmap} shows the final list of \\\\DATAWorldRegions/ world regions used as geolocation targets in this study.\\n\\nGeolocation of commit authors to world regions uses the two complementary techniques introduced in~\\\\cite{icse-seis-2022-gender}, briefly recalled below.\\nThe first one relies on the country code top-level domain (ccTLD) of email addresses extracted from commit metadata, e.g., \\\\texttt{.fr}, \\\\texttt{.ru}, \\\\texttt{.cn}, etc.\\nWe started from the IANA list of Latin character ccTLDs~\\\\cite{wikipedia-cctld} and manually mapped each corresponding territory to a target world region.\\n\\nThe second geolocation technique uses the UTC offset of commit timestamps (e.g., UTC-05:00) and author names to determine the most likely world region of the commit author.\\nFor each UTC offset we determine a list of compatible places (country, state, or dependent territory) in the world that, at the time of that commit, had that UTC offset; commit time is key here, as country UTC offsets vary over time due to timezone changes.\\nTo make this determination we use the IANA time zone database~\\\\cite{tzdata}.\\n\\nThen we assign to each place a score that captures the likelihood that a given author name is characteristic of it.\\nTo this end we use the Forebears dataset of the frequencies of the most common first and family names which, quoting from~\\\\cite{forebear-names}: {\\\\itshape ``provides the approximate incidence of forenames and surnames produced from a database of \\\\num{4 044 546 938} people (55.5\\\\% of living people in 2014). As of September 2019 it covers \\\\num{27 662 801} forenames and \\\\num{27 206 821} surnames in 236 jurisdictions.''}\\nAs in our dataset authors are full name strings (rather than split by first/family name), we first tokenize names (by blanks and case changes) and then lookup individual tokens in both first and family names frequency lists.\\nFor each element found in name lists we multiply the place population\\\\footnotemark{} by the name frequency to obtain a measure that is proportional to the number of persons bearing that name (token) in the specific place.\\n\\\\footnotetext{To obtain population totals---as the notion of ``place'' is heterogeneous: full countries v.~slices of large countries spanning multiple timezones---we use a mixture of primary sources (e.g., government websites), and non-primary ones (e.g., Wikipedia articles).}\\nWe sum this figure for all elements to obtain a place score, ending up with a list of $\\\\langle$place, score$\\\\rangle$ pairs.\\nWe then partition this list by the world region that a place belongs to and sum the score for all the places in each region to obtain an overall score, corresponding to the likelihood that the commit belongs to a given world region.\\nWe assign the starting commit as coming from the world region with the highest score.\\n\\nThe email-based technique suffers from the limited and unbalanced use of ccTLDs: most developers use generic TLDs such as \\\\texttt{.com}, \\\\texttt{.org}, or \\\\texttt{.net}.\\nMoreover this does not happen uniformly across zones: US-based developers, for example, use the \\\\texttt{.us} ccTLD much more seldomly than their European counterparts.\\nOn the other hand the offset/name-based technique relies on the UTC offset of the commit timestamps.\\nDue to tool configurations on developer setups, a large number of commits in the dataset has an UTC offset equal to zero.\\nThis affects less recent commits (\\\\DATACommitsTZZTwoThousandTwenty/ of 2020s commits have a zero offset) than older ones (\\\\DATACommitsTZZTwoThousand/ in 2000).\\nAs a result the offset/name-based technique could end up detecting a large share of older commits as authored by African developers, and to a lesser extent Europeans.\\n\\nTo counter these issues we combine the two geolocation techniques together by applying the offset/name-based techniques to all commits with a non-zero UTC offset, and the email-based on to all other commits.\\n\\n\\n \\\\section{Results and Discussion}\\n\\\\label{sec:results}\\n\\n\\\\begin{figure*}\\n  \\\\centering\\n  \\\\includegraphics[width=\\\\linewidth]{stacked.pdf}\\n  \\\\caption{Ratio of commits (above) and active authors (below) by world zone over the 1971--2020 period.}\\n  \\\\Description[Chart]{Stacked bar chart showing the world zone ratios for commits and authors over the 1971--2020 period.}\\n  \\\\label{fig:results}\\n\\\\end{figure*}\\n\\n\\n \\nTo answer \\\\cref{rq:geodiversity} we gathered the number of commits and distinct authors per year and per world zone.\\nWe present the obtained results in \\\\Cref{fig:results} as two stacked bar charts, showing yearly breakdowns for commits and authors respectively.\\nEvery bar represents a year and is partitioned in slices showing the commit/author ratio for each of the world regions of \\\\Cref{fig:worldmap} in that year.\\nTo avoid outliers due to sporadic contributors, in the author chart we only consider authors having contributed at least 5 commits in a given year.\\n\\nWhile observing trends in the charts remember that the total numbers of commits and authors grow exponentially over time.\\nHence for the first years in the charts, the number of data points in some world regions can be extremely small, with negative consequences on the stability of trends.\\n\\n\\n\\n\\n\\\\paragraph{Geographic diversity over time}\\n\\nOverall, the general trend appears to be that the \\\\textbf{geographic diversity in public code is increasing}: North America and Europe alternated their ``dominance'' until the middle of the 90s; from that moment on most other world regions show a slow but steady increment.\\nThis trend of increased participation into public code development includes Central and South Asia (comprising India), Russia, Africa, Central and South America,\\nNotice that also zones that do not seem to follow this trend, such as Australia and New Zealand, are also increasing their participation, but at a lower speed with respect to other zones.\\nFor example, Australia and New Zealand incremented the absolute number of their commits by about 3 orders of magnitude from 2000 to present days.\\n\\nAnother interesting phenomenon that can be appreciated in both charts is the sudden contraction of contributions from North America in 1995; since the charts depict ratios, this corresponds to other zones, and Europe in particular, increasing their share.\\nAn analysis of the main contributions in the years right before the contraction shows that nine out of ten have \\\\texttt{ucbvax.Berkeley.EDU} as author email domain, and the tenth is Keith Bostic, one of the leading Unix BSD developers, appearing with email \\\\texttt{bostic}.\\nNo developer with the same email domain appears anymore within the first hundred contributors in 1996.\\nThis shows the relevance that BSD Unix and the Computer Systems Research Group at the University of California at Berkeley had in the history of open source software.\\nThe group was disbanded in 1995, partially as a consequence of the so-called UNIX wars~\\\\cite{kernighan2019unixhistory}, and this contributes significantly---also because of the relatively low amount of public code circulating at the time---to the sudden drop of contributions from North America in subsequent years.\\nDescendant UNIX operating systems based on BSD, such as OpenBSD, FreeBSD, and NetBSD had smaller relevance to world trends due to (i) the increasing amount of open source code coming from elsewhere and (ii) their more geographically diverse developer community.\\n\\nAnother time frame in which the ratios for Europe and North America are subject to large, sudden changes is 1975--79.\\nA preliminary analysis shows that these ratios are erratic due to the very limited number of commits in those time period, but we were unable to detect a specific root cause.\\nTrends for those years should be subject to further studies, in collaboration with software historians.\\n\\n\\n\\\\paragraph{Colonialism}\\n\\nAnother trend that stands out from the charts is that Africa appears to be well represented.\\nTo assess if this results from a methodological bias, we double-checked the commits detected as originating from Africa for timezones included in the $[0, 3]$ range using both the email- the offset/name-based methods.\\nThe results show that the offset/name-based approach assigns 22.7\\\\% of the commits to Africa whereas the email-based one only assigns 2.7\\\\% of them.\\nWhile a deeper investigation is in order, it is our opinion that the phenomenon we are witnessing here is a consequence of colonialism, specifically the adoption of Europeans names in African countries.\\nFor example the name Eric, derived from Old Norse, is more popular in Ghana than it is in France or in the UK.\\nThis challenges the ability of the offset/name-based method to correctly differentiate between candidate places.\\nTogether with the fact that several African countries are largely populated, the offset/name-based method could detect European names as originating from Africa.\\nWhile this cuts both way, the likelihood of a random person contributing to public code is very different between European countries, all having a well-developed software industry, and African countries that do not all share this trait.\\n\\n\\n\\\\paragraph{Immigration/emigration}\\n\\nAnother area where a similar phenomenon could be at play is the evolution of Central and South America.\\nContribution from this macro region appears to be growing steadily.\\nTo assess if this is the result of a bias introduced by the name-based detection we analyzed the evolution of offset/name-based assignment over time for authors whose email domain is among the top-ten US-based entities in terms of overall contributions (estimated in turn by analyzing the most frequent email domains and manually selecting those belonging to US-based entities).\\nIn 1971 no author with an email from top US-based entities is detected as belonging to Central and South America, whereas in 2019 the ratio is 12\\\\%.\\nNowadays more than one tenth of the people email-associated to top US-based entities have popular Central and South American names, which we posit as a likely consequence of immigration into US (emigration from Central and South America).\\nSince immigration has a much longer history than what we are studying here, what we are witnessing probably includes long-term consequences of it, such as second and third generation immigrants employed in white-collar jobs, such as software development.\\n\\n\\n\\n\\n \\\\section{Limitations and Future Work}\\n\\\\label{sec:conclusion}\\n\\nWe have performed an exploratory, yet very large scale, empirical study of the geographic diversity in public code commits over time.\\nWe have analyzed 2.2 billion\\\\xspace public commits covering the \\\\DATAYearRange/ time period.\\nWe have geolocated developers to \\\\DATAWorldRegions/ world regions using as signals email domains, timezone offsets, and author names.\\nOur findings show that the geographic diversity in public code is increasing over time, and markedly so over the past 20--25 years.\\nObserved trends also co-occur with historical events and macro phenomena like the end of the UNIX wars, increase of coding literacy around the world, colonialism, and immigration.\\n\\n\\n\\\\medskip\\n\\\\emph{Limitations.}\\nThis study relies on a combination of two geolocation methods: one based on email domains, another based on commit UTC offsets and author names.\\nWe discussed some of the limitations of either method in \\\\Cref{sec:method}, motivating our decision of restricting the use of the email-based method to commits with a zero UTC offset.\\nAs a consequence, for most commits in the dataset the offset/name-based method is used.\\nWith such method, the frequencies of forenames and surnames are used to rank candidate zones that have a compatible UTC offset at commit time.\\n\\nA practical consequence of this is that for commits with, say, offset UTC+09:00 the candidate places can be Russia, Japan and Australia, depending on the specific date due to daylight saving time.\\nPopular forenames and surnames in these regions tend to be quite different so the likelihood of the method to provide a reliable detection is high.\\nFor other offsets the set of popular forenames and surnames from candidate zones can exhibit more substantial overlaps, negatively impacting detection accuracy.\\nWe have discussed some of these cases in \\\\Cref{sec:results}, but other might be lingering in the results impacting observed trends.\\n\\nThe choice of using the email-based method for commits with zero UTC offset, and the offset/name-based method elsewhere, has allowed us to study all developers not having a country-specific email domain (ccTLD), but comes with the risk of under-representing the world zones that have (in part and in some times of the year) an actual UTC offset of zero.\\n\\nA potential bias in this study could be introduced by the fact that the name database used for offset/name-based geolocation only contains names formed using Latin alphabet characters.\\nWe looked for names containing Chinese, Japanese, and Korean characters in the original dataset, finding only a negligible amount of authors who use non-Latin characters in their VCS names, which leads us to believe that the impact of this issue is minimal.\\n\\nWe did not apply identity merging (e.g., using state-of-the-art tools like SortingHat~\\\\cite{moreno2019sortinghat}), but we do not expect this to be a significant issue because: (a) to introduce bias in author trends the distribution of identity merges around the world should be uneven, which seems unlikely; and (b) the observed commit trends (which would be unaffected by identity merging) are very similar to observed author trends.\\n\\nWe did not systematically remove known bot accounts~\\\\cite{lebeuf2018swbots} from the author dataset, but we did check for the presence of software bots among the top committers of each year. We only found limited traces of continuous integration (CI) bots, used primarily to automate merge commits. After removing CI bots from the dataset the observed global trends were unchanged, therefore this paper presents unfiltered data.\\n\\n\\n\\\\medskip\\n\\\\emph{Future work.}\\nTo some extent the above limitations are the price to pay to study such a large dataset: there exists a trade-off between large-scale analysis and accuracy.\\nWe plan nonetheless to further investigate and mitigate them in future work.\\nMulti-method approaches, merging data mining with social science methods, could be applied to address some of the questions raised in this exploratory study.\\nWhile they do not scale to the whole dataset, multi-methods can be adopted to dig deeper into specific aspects, specifically those related to social phenomena.\\nSoftware is a social artifact, it is no wonder that aspects related to sociocultural evolution emerge when analyzing its evolution at this scale.\\n\\n\\n\\n\\n \\n\\\\clearpage\\n\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0ba8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8014153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e1c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fb8684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:42:42.549761Z",
     "start_time": "2024-05-29T04:42:42.546467Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fbd2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T11:47:42.053828Z",
     "start_time": "2024-05-27T11:47:41.841547Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2254c075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:42:43.592660Z",
     "start_time": "2024-05-29T04:42:43.589439Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/data/home/milinbhade/LLaMa/hf_llama/\"\n",
    "hf_llama_chat_path = \"/data/home/milinbhade/LLaMa/hf_llama_chat/\"\n",
    "\n",
    "datapath = \"/data/home/milinbhade/LLaMa/pretrain_data/alpaca/alpaca-cleaned-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9450c70",
   "metadata": {},
   "source": [
    "# Lora pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f35ab83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:42:47.919910Z",
     "start_time": "2024-05-29T04:42:44.716058Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a5fb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:42:47.992803Z",
     "start_time": "2024-05-29T04:42:47.924186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec11e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T04:42:49.272139Z",
     "start_time": "2024-05-29T04:42:47.994558Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2b018f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:57.144997Z",
     "start_time": "2024-05-28T20:02:57.128509Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d2d0cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:57.164015Z",
     "start_time": "2024-05-28T20:02:57.152149Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"../templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "    \n",
    "class ZeroPrompter(object):\n",
    "    __slots__ = (\"_verbose\")\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Without using prompt template!\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if instruction[-1] == '.':\n",
    "            instruction = instruction[:-1] + ':'\n",
    "        if instruction[-1] not in ['.', ':', '?', '!']:\n",
    "            instruction = instruction + ':'\n",
    "        instruction += ' '\n",
    "\n",
    "        if input:\n",
    "            if input[-1] not in ['.', ':', '?', '!']:\n",
    "                input = input + '.'\n",
    "            res = instruction + input\n",
    "        else:\n",
    "            res = instruction\n",
    "        if label:\n",
    "            res = f\"{res} {label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7ee540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:57.187094Z",
     "start_time": "2024-05-28T20:02:57.165997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt template alpaca: Template used by Alpaca-LoRA.\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me a joke\n",
      "\n",
      "### Input:\n",
      "Why did the chicken cross the road\n",
      "\n",
      "### Response:\n",
      "To get to the other side\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me a joke\n",
      "\n",
      "### Input:\n",
      "Why did the chicken cross the road\n",
      "\n",
      "### Response:\n",
      "To get to the other side\n",
      "Get :  To get to the other side\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Tell me a joke\"\n",
    "input_text = \"Why did the chicken cross the road\"\n",
    "label = \"To get to the other side\"\n",
    "\n",
    "prompter = Prompter(verbose=True)\n",
    "print(prompter.generate_prompt(instruction, input_text, label))\n",
    "\n",
    "output = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Tell me a joke\n",
    "\n",
    "### Input:\n",
    "Why did the chicken cross the road\n",
    "\n",
    "### Response:\n",
    "To get to the other side \"\"\"\n",
    "print(\"Get : \", prompter.get_response(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95213e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:57.211632Z",
     "start_time": "2024-05-28T20:02:57.203981Z"
    }
   },
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.base_model = model_path\n",
    "        self.prune_model = None\n",
    "        self.data_path = datapath\n",
    "        self.cache_dataset = True\n",
    "        self.extra_val_dataset = None\n",
    "        self.output_dir = \"./lora-alpaca\"\n",
    "        \n",
    "        self.batch_size = 64\n",
    "        self.micro_batch_size = 4\n",
    "        self.num_epochs = 3\n",
    "        self.learning_rate = 3e-4\n",
    "        self.cutoff_len = 256\n",
    "        self.val_set_size = 0.1\n",
    "        self.prompt_template_name = \"alpaca\"\n",
    "        self.no_instruction = False\n",
    "        \n",
    "        self.lora_r = 8\n",
    "        self.lora_alpha = 16\n",
    "        self.lora_dropout = 0.05\n",
    "        self.lora_target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,down_proj,up_proj\"\n",
    "        self.train_on_inputs = False\n",
    "        self.add_eos_token = False\n",
    "        self.group_by_length = False\n",
    "        \n",
    "        self.wandb_project = \"\"\n",
    "        self.resume_from_checkpoint = None\n",
    "        self.local_rank = -1\n",
    "        self.torch_version = int(torch.__version__.split('.')[1])\n",
    "\n",
    "args = Arguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b3eed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:57.258594Z",
     "start_time": "2024-05-28T20:02:57.217311Z"
    },
    "code_folding": [
     0,
     30,
     31,
     57
    ]
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = model_path,  # the only required argument\n",
    "    data_path: str = datapath,\n",
    "    output_dir: str = \"./lora-alpaca\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    val_set_size: int = 0.1,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"down_proj\",\"up_proj\"\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"val_set_size: {val_set_size}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"add_eos_token: {add_eos_token}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"wandb_project: {wandb_project}\\n\"\n",
    "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "            f\"wandb_watch: {wandb_watch}\\n\"\n",
    "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    prompter = Prompter(prompt_template_name)\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\n",
    "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    "    )\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "    tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=add_eos_token\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        print(\"If\")\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "        print(data)\n",
    "    else:\n",
    "        data = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "        print(\"Else\")\n",
    "        print(data)\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(\n",
    "            test_size=val_set_size, shuffle=True, seed=42\n",
    "        )\n",
    "        train_data = (\n",
    "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "        val_data = (\n",
    "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else None,\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed71adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:59.961409Z",
     "start_time": "2024-05-28T20:02:57.271066Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 01:32:57.999837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 01:32:59.029716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from transformers import BitsAndBytesConfig\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37bb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T08:08:09.726071Z",
     "start_time": "2024-05-28T08:08:09.099380Z"
    }
   },
   "source": [
    "from transformers import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e427bc16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:02:59.986719Z",
     "start_time": "2024-05-28T20:02:59.965330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b097abb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:03:00.081485Z",
     "start_time": "2024-05-28T20:02:59.992470Z"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c560d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:03:00.120720Z",
     "start_time": "2024-05-28T20:03:00.084211Z"
    },
    "code_folding": [
     165
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def main(args):\n",
    "    # Set WanDB\n",
    "    os.environ[\"WANDB_PROJECT\"] = args.wandb_project\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Load Pruned Model\n",
    "    model = model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model,  \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    model.seqlen = model.config.max_position_embeddings \n",
    "    print(model.seqlen)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer= LlamaTokenizer.from_pretrained(args.base_model,padding=True, truncation=True)\n",
    "\n",
    "    gradient_accumulation_steps = args.batch_size // args.micro_batch_size\n",
    "    if not args.no_instruction:\n",
    "        prompter = Prompter(args.prompt_template_name)\n",
    "    else:\n",
    "        prompter = ZeroPrompter()\n",
    "\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "#     if device == 'cuda':\n",
    "#         model.half()\n",
    "\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=args.cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < args.cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        if 'lamini' in args.data_path.lower():\n",
    "            full_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"],\n",
    "                None,\n",
    "                data_point[\"response\"],\n",
    "            )\n",
    "        elif 'alpaca' in args.data_path.lower():\n",
    "            full_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"],\n",
    "                data_point[\"input\"],\n",
    "                data_point[\"output\"],\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not args.train_on_inputs:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"] if 'input' in data_point.keys() else None,\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=args.add_eos_token\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if args.add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    def split_and_tokenizer(test_data, tokenizer, seq_len, field_name):\n",
    "        test_ids = tokenizer(\"\\n\\n\".join(test_data[field_name]), return_tensors='pt').input_ids[0]\n",
    "        nsamples = test_ids.numel() // seq_len\n",
    "\n",
    "        test_set = []\n",
    "        for i in range(nsamples):\n",
    "            batch = test_ids[(i * seq_len):((i + 1) * seq_len)]\n",
    "            test_set.append({\n",
    "                'input_ids': batch,\n",
    "                'labels': batch\n",
    "            })\n",
    "        return test_set\n",
    "\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    # Prepare For LoRA\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    \n",
    "    config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=args.lora_target_modules.split(\",\"),\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    model.print_trainable_parameters()  \n",
    "\n",
    "    # Load Train Dataset\n",
    "    data = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "    \n",
    "    print(\"Data Length: \", len(data))\n",
    "    print(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.cache_dataset and os.path.exists('./datasets/cache/{}.bin'.format(\"alpaca\")):\n",
    "        print(\"Entered If\")\n",
    "        preprocess_data = torch.load('./datasets/cache/{}.bin'.format(\"alpaca\"))\n",
    "        train_data, val_data = preprocess_data['train'], preprocess_data['val']\n",
    "    else:\n",
    "        print(\"Entered Else\")\n",
    "        train_val = data[\"train\"].train_test_split(\n",
    "            test_size=args.val_set_size, shuffle=True, seed=42\n",
    "        )\n",
    "        train_data = (\n",
    "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "        val_data = {\n",
    "            args.data_path: train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt),\n",
    "        }\n",
    "        if args.cache_dataset and args.local_rank == -1:\n",
    "            cache_file = './datasets/cache/{}.bin'.format(\"alpaca\")\n",
    "            cache_dir = '/'.join(cache_file.split('/')[:-1])\n",
    "            directory = Path(cache_dir)\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Saving datasets\")\n",
    "            torch.save({\n",
    "                'train': train_data, 'val': val_data\n",
    "            }, cache_file)\n",
    "\n",
    "    # Load Extra Validation Dataset\n",
    "    if args.extra_val_dataset:\n",
    "        from LLMPruner.datasets.ppl_dataset import get_wikitext2, get_ptb\n",
    "\n",
    "        seq_len = 128\n",
    "        for extra_dataset in args.extra_val_dataset.split(','):\n",
    "            if 'wikitext2' in extra_dataset:\n",
    "                _, test_data = get_wikitext2(seq_len, None)\n",
    "                test_data = split_and_tokenizer(test_data, tokenizer, seq_len, field_name='text')\n",
    "            if 'ptb' in extra_dataset:\n",
    "                _, test_data = get_ptb(seq_len, None)\n",
    "                test_data = split_and_tokenizer(test_data, tokenizer, seq_len, field_name='sentence')\n",
    "            val_data[extra_dataset] = test_data\n",
    "\n",
    "#     trainer = transformers.Trainer(\n",
    "#         model=model,\n",
    "#         train_dataset=train_data,\n",
    "#         eval_dataset=val_data,\n",
    "#         args=transformers.TrainingArguments(\n",
    "#             per_device_train_batch_size=args.micro_batch_size,\n",
    "#             gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#             warmup_steps=10,\n",
    "#             num_train_epochs=args.num_epochs,\n",
    "#             learning_rate=args.learning_rate,\n",
    "#             fp16=True,\n",
    "#             logging_steps=10,\n",
    "#             logging_first_step=True,\n",
    "#             optim=\"adamw_torch\",\n",
    "#             evaluation_strategy=\"steps\",\n",
    "#             save_strategy=\"steps\",\n",
    "#             eval_steps=100,\n",
    "#             save_steps=200,\n",
    "#             output_dir=args.output_dir,\n",
    "#             save_total_limit=20,\n",
    "#             load_best_model_at_end=True,\n",
    "#             ddp_find_unused_parameters=None,\n",
    "#             group_by_length=args.group_by_length,\n",
    "#             report_to=\"wandb\",\n",
    "#             run_name=args.output_dir.split('/')[-1],\n",
    "#             metric_for_best_model=\"{}_loss\".format(args.data_path),\n",
    "#         ),\n",
    "# #         data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "# #             tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "# #         ),\n",
    "#         data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     trainer = SFTTrainer(\n",
    "#         model=model,\n",
    "#         train_dataset=train_data,\n",
    "# #         eval_dataset=val_data,\n",
    "#         args=transformers.TrainingArguments(\n",
    "#             per_device_train_batch_size=8,\n",
    "#             gradient_accumulation_steps=8,\n",
    "#             warmup_steps=10,\n",
    "#             num_train_epochs=3,\n",
    "#             learning_rate=2e-4,\n",
    "#             fp16=True,\n",
    "#             logging_steps=10,\n",
    "#             optim=\"paged_adamw_8bit\",\n",
    "#             output_dir=args.output_dir,\n",
    "#             report_to=\"wandb\",\n",
    "#             run_name=args.output_dir.split('/')[-1],\n",
    "#         ),\n",
    "#         data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "#     )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,  # Pass validation dataset here\n",
    "        peft_config=config,\n",
    "        dataset_text_field=\"input_ids\",\n",
    "        max_seq_length=1024,\n",
    "        tokenizer=tokenizer,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=args.micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            learning_rate=args.learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            logging_first_step=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            save_steps=200,\n",
    "            output_dir=args.output_dir,\n",
    "            save_total_limit=20,\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=None,\n",
    "            group_by_length=args.group_by_length,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=args.output_dir.split('/')[-1],\n",
    "            metric_for_best_model=\"{}_loss\".format(args.data_path),\n",
    "        ),\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     old_state_dict = model.state_dict\n",
    "#     model.state_dict = (\n",
    "#         lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#             self, old_state_dict()\n",
    "#         )\n",
    "#     ).__get__(model, type(model))\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.autocast(\"cuda\"): \n",
    "        train_result = trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    \n",
    "    \n",
    "#     model.state_dict = old_state_dict\n",
    "#     model.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Saving model\n",
    "    print(\"Completed training with lora alpha : \",args.lora_alpha,\" and device : 4. Storage location : \",args.output_dir)\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    final_ckpt = os.path.join(args.output_dir, \"checkpoint\")\n",
    "    os.makedirs(final_ckpt, exist_ok=True)\n",
    "    trainer.model.save_pretrained(final_ckpt)\n",
    "    print(\"Last Checkpoint saved successfully\")\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\"Merging models\")\n",
    "#     model = AutoPeftModelForCausalLM.from_pretrained(output_dir_final_cp, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "#     model = model.merge_and_unload()\n",
    "#     print(\"Model Merged\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     model.save_pretrained(output_dir_merged, safe_serialization=True)\n",
    "#     tokenizer.save_pretrained(output_dir_merged)\n",
    "#     print(\"Merged Model saved successfully\")\n",
    "\n",
    "#     print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6b99c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T20:03:58.323949Z",
     "start_time": "2024-05-28T20:03:00.122739Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ea7e5b52754966a40e4265c415e54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/milinbhade/miniconda3/envs/huggingface/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Length:  1\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 51760\n",
      "    })\n",
      "})\n",
      "Entered If\n",
      "torch.float32 282398720 0.04178482316071323\n",
      "torch.int8 6476005376 0.9582151768392868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmilinbhade1214\u001b[0m (\u001b[33mmilinbhade1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/milinbhade/Milin/AMC/bertamc_v4/finetune/wandb/run-20240529_013341-qgmkdz5l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/milinbhade1/uncategorized/runs/qgmkdz5l' target=\"_blank\">lora-alpaca</a></strong> to <a href='https://wandb.ai/milinbhade1/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/milinbhade1/uncategorized' target=\"_blank\">https://wandb.ai/milinbhade1/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/milinbhade1/uncategorized/runs/qgmkdz5l' target=\"_blank\">https://wandb.ai/milinbhade1/uncategorized/runs/qgmkdz5l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 256 at dim 1 (got 77)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 299\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m#     old_state_dict = model.state_dict\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m#     model.state_dict = (\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m#         lambda self, *_, **__: get_peft_model_state_dict(\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m#             self, old_state_dict()\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#     ).__get__(model, type(model))\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[0;32m--> 299\u001b[0m         train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m    301\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/trainer.py:1928\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1925\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1929\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/data/data_collator.py:761\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: List[Union[List[\u001b[38;5;28mint\u001b[39m], Any, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 761\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    767\u001b[0m         }\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3326\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3323\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3324\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b47d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "483px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
